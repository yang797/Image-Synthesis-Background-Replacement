{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_seed import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Triple_U_Net\n",
    "from model_train import train_triple_u_net\n",
    "from dataset_loader import PairedImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8f47e3609f1f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Batch [0/107], D_loss: 0.7122, G_loss: 6.7082\n",
      "Epoch [1/200], Batch [10/107], D_loss: 0.7068, G_loss: 3.8083\n",
      "Epoch [1/200], Batch [20/107], D_loss: 0.7103, G_loss: 2.8279\n",
      "Epoch [1/200], Batch [30/107], D_loss: 0.7066, G_loss: 3.2513\n",
      "Epoch [1/200], Batch [40/107], D_loss: 0.8748, G_loss: 3.3726\n",
      "Epoch [1/200], Batch [50/107], D_loss: 0.6816, G_loss: 2.8807\n",
      "Epoch [1/200], Batch [60/107], D_loss: 0.5987, G_loss: 3.0845\n",
      "Epoch [1/200], Batch [70/107], D_loss: 0.5167, G_loss: 3.0988\n",
      "Epoch [1/200], Batch [80/107], D_loss: 0.6517, G_loss: 2.5526\n",
      "Epoch [1/200], Batch [90/107], D_loss: 0.5781, G_loss: 2.7005\n",
      "Epoch [1/200], Batch [100/107], D_loss: 0.6362, G_loss: 3.0430\n",
      "Epoch [1/200], D_loss: 0.7113, G_GAN_loss: 0.8442, G_L1_loss: 2.1827, G_perc_loss: 0.0471, G_total_loss: 3.0740, \n",
      "Save best! epoch [1]!\n",
      "Epoch [2/200], Batch [0/107], D_loss: 0.6164, G_loss: 3.1101\n",
      "Epoch [2/200], Batch [10/107], D_loss: 0.4637, G_loss: 3.1062\n",
      "Epoch [2/200], Batch [20/107], D_loss: 0.7964, G_loss: 3.3197\n",
      "Epoch [2/200], Batch [30/107], D_loss: 0.5303, G_loss: 2.8748\n",
      "Epoch [2/200], Batch [40/107], D_loss: 0.6154, G_loss: 3.1495\n",
      "Epoch [2/200], Batch [50/107], D_loss: 0.3172, G_loss: 3.1690\n",
      "Epoch [2/200], Batch [60/107], D_loss: 0.4843, G_loss: 2.3478\n",
      "Epoch [2/200], Batch [70/107], D_loss: 0.2264, G_loss: 3.2603\n",
      "Epoch [2/200], Batch [80/107], D_loss: 0.4638, G_loss: 3.1944\n",
      "Epoch [2/200], Batch [90/107], D_loss: 0.2951, G_loss: 3.2283\n",
      "Epoch [2/200], Batch [100/107], D_loss: 0.2882, G_loss: 4.6618\n",
      "Epoch [2/200], D_loss: 0.4392, G_GAN_loss: 1.5196, G_L1_loss: 1.7762, G_perc_loss: 0.0448, G_total_loss: 3.3406, \n",
      "Epoch [3/200], Batch [0/107], D_loss: 0.2244, G_loss: 3.2567\n",
      "Epoch [3/200], Batch [10/107], D_loss: 1.1166, G_loss: 4.1559\n",
      "Epoch [3/200], Batch [20/107], D_loss: 0.4376, G_loss: 4.0395\n",
      "Epoch [3/200], Batch [30/107], D_loss: 0.1766, G_loss: 3.0501\n",
      "Epoch [3/200], Batch [40/107], D_loss: 0.8487, G_loss: 4.1581\n",
      "Epoch [3/200], Batch [50/107], D_loss: 0.2503, G_loss: 3.6340\n",
      "Epoch [3/200], Batch [60/107], D_loss: 0.4672, G_loss: 3.5060\n",
      "Epoch [3/200], Batch [70/107], D_loss: 0.1453, G_loss: 3.4064\n",
      "Epoch [3/200], Batch [80/107], D_loss: 0.5207, G_loss: 4.3272\n",
      "Epoch [3/200], Batch [90/107], D_loss: 0.1490, G_loss: 3.9048\n",
      "Epoch [3/200], Batch [100/107], D_loss: 0.1537, G_loss: 4.7165\n",
      "Epoch [3/200], D_loss: 0.3575, G_GAN_loss: 2.1066, G_L1_loss: 1.5960, G_perc_loss: 0.0441, G_total_loss: 3.7467, \n",
      "Epoch [4/200], Batch [0/107], D_loss: 0.2566, G_loss: 2.2036\n",
      "Epoch [4/200], Batch [10/107], D_loss: 0.1760, G_loss: 3.4751\n",
      "Epoch [4/200], Batch [20/107], D_loss: 0.2990, G_loss: 5.2417\n",
      "Epoch [4/200], Batch [30/107], D_loss: 1.0042, G_loss: 3.4960\n",
      "Epoch [4/200], Batch [40/107], D_loss: 0.4314, G_loss: 2.8319\n",
      "Epoch [4/200], Batch [50/107], D_loss: 0.3085, G_loss: 3.2537\n",
      "Epoch [4/200], Batch [60/107], D_loss: 0.1197, G_loss: 4.3422\n",
      "Epoch [4/200], Batch [70/107], D_loss: 0.1314, G_loss: 5.4599\n",
      "Epoch [4/200], Batch [80/107], D_loss: 0.4191, G_loss: 3.8014\n",
      "Epoch [4/200], Batch [90/107], D_loss: 0.2898, G_loss: 3.2604\n",
      "Epoch [4/200], Batch [100/107], D_loss: 0.3086, G_loss: 4.3296\n",
      "Epoch [4/200], D_loss: 0.3179, G_GAN_loss: 2.2030, G_L1_loss: 1.5254, G_perc_loss: 0.0437, G_total_loss: 3.7721, \n",
      "Epoch [5/200], Batch [0/107], D_loss: 0.0478, G_loss: 3.9699\n",
      "Epoch [5/200], Batch [10/107], D_loss: 0.1288, G_loss: 3.9591\n",
      "Epoch [5/200], Batch [20/107], D_loss: 0.1213, G_loss: 5.5913\n",
      "Epoch [5/200], Batch [30/107], D_loss: 0.1945, G_loss: 3.9973\n",
      "Epoch [5/200], Batch [40/107], D_loss: 1.4738, G_loss: 4.5060\n",
      "Epoch [5/200], Batch [50/107], D_loss: 0.1671, G_loss: 3.7307\n",
      "Epoch [5/200], Batch [60/107], D_loss: 0.5161, G_loss: 2.0997\n",
      "Epoch [5/200], Batch [70/107], D_loss: 0.2697, G_loss: 4.7805\n",
      "Epoch [5/200], Batch [80/107], D_loss: 0.9710, G_loss: 6.1077\n",
      "Epoch [5/200], Batch [90/107], D_loss: 0.5941, G_loss: 2.2519\n",
      "Epoch [5/200], Batch [100/107], D_loss: 0.4949, G_loss: 2.4192\n",
      "Epoch [5/200], D_loss: 0.4419, G_GAN_loss: 2.1395, G_L1_loss: 1.4856, G_perc_loss: 0.0435, G_total_loss: 3.6686, \n",
      "Epoch [6/200], Batch [0/107], D_loss: 0.6626, G_loss: 2.5635\n",
      "Epoch [6/200], Batch [10/107], D_loss: 0.5885, G_loss: 2.8514\n",
      "Epoch [6/200], Batch [20/107], D_loss: 0.4296, G_loss: 2.7925\n",
      "Epoch [6/200], Batch [30/107], D_loss: 0.3601, G_loss: 2.8555\n",
      "Epoch [6/200], Batch [40/107], D_loss: 0.3056, G_loss: 3.2376\n",
      "Epoch [6/200], Batch [50/107], D_loss: 0.1945, G_loss: 2.9654\n",
      "Epoch [6/200], Batch [60/107], D_loss: 0.2546, G_loss: 2.6906\n",
      "Epoch [6/200], Batch [70/107], D_loss: 0.4824, G_loss: 4.1748\n",
      "Epoch [6/200], Batch [80/107], D_loss: 0.3038, G_loss: 3.3461\n",
      "Epoch [6/200], Batch [90/107], D_loss: 0.4822, G_loss: 3.7208\n",
      "Epoch [6/200], Batch [100/107], D_loss: 0.2048, G_loss: 2.5369\n",
      "Epoch [6/200], D_loss: 0.3847, G_GAN_loss: 1.7433, G_L1_loss: 1.4666, G_perc_loss: 0.0432, G_total_loss: 3.2531, \n",
      "Epoch [7/200], Batch [0/107], D_loss: 0.4554, G_loss: 4.0868\n",
      "Epoch [7/200], Batch [10/107], D_loss: 0.2544, G_loss: 5.2425\n",
      "Epoch [7/200], Batch [20/107], D_loss: 0.2705, G_loss: 3.5288\n",
      "Epoch [7/200], Batch [30/107], D_loss: 0.4575, G_loss: 4.0469\n",
      "Epoch [7/200], Batch [40/107], D_loss: 0.1259, G_loss: 4.5031\n",
      "Epoch [7/200], Batch [50/107], D_loss: 0.1864, G_loss: 3.6862\n",
      "Epoch [7/200], Batch [60/107], D_loss: 0.2139, G_loss: 3.9752\n",
      "Epoch [7/200], Batch [70/107], D_loss: 0.6378, G_loss: 4.1526\n",
      "Epoch [7/200], Batch [80/107], D_loss: 0.3044, G_loss: 3.5769\n",
      "Epoch [7/200], Batch [90/107], D_loss: 0.4284, G_loss: 4.3551\n",
      "Epoch [7/200], Batch [100/107], D_loss: 0.5013, G_loss: 4.9158\n",
      "Epoch [7/200], D_loss: 0.3099, G_GAN_loss: 2.5071, G_L1_loss: 1.4366, G_perc_loss: 0.0430, G_total_loss: 3.9867, \n",
      "Epoch [8/200], Batch [0/107], D_loss: 0.0633, G_loss: 4.5522\n",
      "Epoch [8/200], Batch [10/107], D_loss: 0.0788, G_loss: 5.5485\n",
      "Epoch [8/200], Batch [20/107], D_loss: 0.1953, G_loss: 4.0883\n",
      "Epoch [8/200], Batch [30/107], D_loss: 0.4244, G_loss: 3.7749\n",
      "Epoch [8/200], Batch [40/107], D_loss: 0.1336, G_loss: 3.8009\n",
      "Epoch [8/200], Batch [50/107], D_loss: 0.0511, G_loss: 4.9409\n",
      "Epoch [8/200], Batch [60/107], D_loss: 0.2945, G_loss: 5.9048\n",
      "Epoch [8/200], Batch [70/107], D_loss: 1.5687, G_loss: 4.7226\n",
      "Epoch [8/200], Batch [80/107], D_loss: 0.9143, G_loss: 3.2494\n",
      "Epoch [8/200], Batch [90/107], D_loss: 0.2132, G_loss: 2.8414\n",
      "Epoch [8/200], Batch [100/107], D_loss: 0.3571, G_loss: 3.3464\n",
      "Epoch [8/200], D_loss: 0.3658, G_GAN_loss: 2.5575, G_L1_loss: 1.4287, G_perc_loss: 0.0429, G_total_loss: 4.0291, \n",
      "Epoch [9/200], Batch [0/107], D_loss: 0.2452, G_loss: 4.0255\n",
      "Epoch [9/200], Batch [10/107], D_loss: 0.5459, G_loss: 2.8588\n",
      "Epoch [9/200], Batch [20/107], D_loss: 0.2491, G_loss: 4.2933\n",
      "Epoch [9/200], Batch [30/107], D_loss: 0.2281, G_loss: 3.9557\n",
      "Epoch [9/200], Batch [40/107], D_loss: 0.3154, G_loss: 4.0135\n",
      "Epoch [9/200], Batch [50/107], D_loss: 0.3098, G_loss: 3.7647\n",
      "Epoch [9/200], Batch [60/107], D_loss: 0.2960, G_loss: 5.2169\n",
      "Epoch [9/200], Batch [70/107], D_loss: 0.0577, G_loss: 4.0728\n",
      "Epoch [9/200], Batch [80/107], D_loss: 0.1016, G_loss: 4.3015\n",
      "Epoch [9/200], Batch [90/107], D_loss: 0.1524, G_loss: 4.9179\n",
      "Epoch [9/200], Batch [100/107], D_loss: 0.0542, G_loss: 5.0726\n",
      "Epoch [9/200], D_loss: 0.2682, G_GAN_loss: 2.7060, G_L1_loss: 1.4089, G_perc_loss: 0.0429, G_total_loss: 4.1578, \n",
      "Epoch [10/200], Batch [0/107], D_loss: 0.1782, G_loss: 4.1917\n",
      "Epoch [10/200], Batch [10/107], D_loss: 0.2999, G_loss: 4.6179\n",
      "Epoch [10/200], Batch [20/107], D_loss: 0.0545, G_loss: 5.5859\n",
      "Epoch [10/200], Batch [30/107], D_loss: 0.0270, G_loss: 6.3618\n",
      "Epoch [10/200], Batch [40/107], D_loss: 0.0473, G_loss: 6.2167\n",
      "Epoch [10/200], Batch [50/107], D_loss: 0.0222, G_loss: 5.8163\n",
      "Epoch [10/200], Batch [60/107], D_loss: 0.0137, G_loss: 7.5311\n",
      "Epoch [10/200], Batch [70/107], D_loss: 0.0050, G_loss: 7.1020\n",
      "Epoch [10/200], Batch [80/107], D_loss: 0.0075, G_loss: 7.2959\n",
      "Epoch [10/200], Batch [90/107], D_loss: 0.0146, G_loss: 7.3960\n",
      "Epoch [10/200], Batch [100/107], D_loss: 0.0637, G_loss: 6.4444\n",
      "Epoch [10/200], D_loss: 0.0629, G_GAN_loss: 4.9387, G_L1_loss: 1.3976, G_perc_loss: 0.0429, G_total_loss: 6.3792, \n",
      "Epoch [11/200], Batch [0/107], D_loss: 0.0045, G_loss: 7.6810\n",
      "Epoch [11/200], Batch [10/107], D_loss: 0.0040, G_loss: 7.8511\n",
      "Epoch [11/200], Batch [20/107], D_loss: 0.0043, G_loss: 7.3524\n",
      "Epoch [11/200], Batch [30/107], D_loss: 0.0049, G_loss: 7.3409\n",
      "Epoch [11/200], Batch [40/107], D_loss: 0.0075, G_loss: 7.9921\n",
      "Epoch [11/200], Batch [50/107], D_loss: 0.0026, G_loss: 7.8244\n",
      "Epoch [11/200], Batch [60/107], D_loss: 0.0096, G_loss: 8.4288\n",
      "Epoch [11/200], Batch [70/107], D_loss: 0.0040, G_loss: 7.7811\n",
      "Epoch [11/200], Batch [80/107], D_loss: 0.0873, G_loss: 7.1672\n",
      "Epoch [11/200], Batch [90/107], D_loss: 0.0291, G_loss: 6.8586\n",
      "Epoch [11/200], Batch [100/107], D_loss: 0.0048, G_loss: 7.0169\n",
      "Epoch [11/200], D_loss: 0.0241, G_GAN_loss: 5.8465, G_L1_loss: 1.3797, G_perc_loss: 0.0428, G_total_loss: 7.2689, \n",
      "Epoch [12/200], Batch [0/107], D_loss: 0.0370, G_loss: 5.7518\n",
      "Epoch [12/200], Batch [10/107], D_loss: 0.0029, G_loss: 7.2466\n",
      "Epoch [12/200], Batch [20/107], D_loss: 0.0684, G_loss: 6.8700\n",
      "Epoch [12/200], Batch [30/107], D_loss: 0.0021, G_loss: 7.6304\n",
      "Epoch [12/200], Batch [40/107], D_loss: 0.0060, G_loss: 6.0830\n",
      "Epoch [12/200], Batch [50/107], D_loss: 0.0045, G_loss: 8.1447\n",
      "Epoch [12/200], Batch [60/107], D_loss: 0.0019, G_loss: 8.3006\n",
      "Epoch [12/200], Batch [70/107], D_loss: 0.0008, G_loss: 8.7361\n",
      "Epoch [12/200], Batch [80/107], D_loss: 0.0011, G_loss: 8.1866\n",
      "Epoch [12/200], Batch [90/107], D_loss: 0.0027, G_loss: 8.7711\n",
      "Epoch [12/200], Batch [100/107], D_loss: 0.0007, G_loss: 9.0883\n",
      "Epoch [12/200], D_loss: 0.0084, G_GAN_loss: 6.5305, G_L1_loss: 1.3761, G_perc_loss: 0.0429, G_total_loss: 7.9495, \n",
      "Epoch [13/200], Batch [0/107], D_loss: 0.0010, G_loss: 8.8847\n",
      "Epoch [13/200], Batch [10/107], D_loss: 0.0008, G_loss: 8.9729\n",
      "Epoch [13/200], Batch [20/107], D_loss: 0.0007, G_loss: 8.7760\n",
      "Epoch [13/200], Batch [30/107], D_loss: 0.0010, G_loss: 9.2421\n",
      "Epoch [13/200], Batch [40/107], D_loss: 0.0012, G_loss: 9.0680\n",
      "Epoch [13/200], Batch [50/107], D_loss: 0.0015, G_loss: 7.9303\n",
      "Epoch [13/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.4601\n",
      "Epoch [13/200], Batch [70/107], D_loss: 0.0008, G_loss: 8.7256\n",
      "Epoch [13/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.6166\n",
      "Epoch [13/200], Batch [90/107], D_loss: 0.0006, G_loss: 9.2029\n",
      "Epoch [13/200], Batch [100/107], D_loss: 0.0008, G_loss: 8.8660\n",
      "Epoch [13/200], D_loss: 0.0012, G_GAN_loss: 7.3853, G_L1_loss: 1.3493, G_perc_loss: 0.0426, G_total_loss: 8.7773, \n",
      "Epoch [14/200], Batch [0/107], D_loss: 0.0009, G_loss: 9.3808\n",
      "Epoch [14/200], Batch [10/107], D_loss: 0.0013, G_loss: 8.8845\n",
      "Epoch [14/200], Batch [20/107], D_loss: 0.0010, G_loss: 8.7392\n",
      "Epoch [14/200], Batch [30/107], D_loss: 0.0011, G_loss: 9.4570\n",
      "Epoch [14/200], Batch [40/107], D_loss: 0.0003, G_loss: 10.4416\n",
      "Epoch [14/200], Batch [50/107], D_loss: 0.0010, G_loss: 8.7812\n",
      "Epoch [14/200], Batch [60/107], D_loss: 0.0009, G_loss: 9.3735\n",
      "Epoch [14/200], Batch [70/107], D_loss: 0.0008, G_loss: 9.0199\n",
      "Epoch [14/200], Batch [80/107], D_loss: 0.0007, G_loss: 9.9409\n",
      "Epoch [14/200], Batch [90/107], D_loss: 0.0005, G_loss: 10.1848\n",
      "Epoch [14/200], Batch [100/107], D_loss: 0.0070, G_loss: 7.9125\n",
      "Epoch [14/200], D_loss: 0.0028, G_GAN_loss: 7.9817, G_L1_loss: 1.3359, G_perc_loss: 0.0427, G_total_loss: 9.3602, \n",
      "Epoch [15/200], Batch [0/107], D_loss: 0.0010, G_loss: 9.4487\n",
      "Epoch [15/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.5278\n",
      "Epoch [15/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.4782\n",
      "Epoch [15/200], D_loss: 0.0007, G_GAN_loss: 8.3440, G_L1_loss: 1.3196, G_perc_loss: 0.0425, G_total_loss: 9.7061, \n",
      "Epoch [16/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.7428\n",
      "Epoch [16/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.6574\n",
      "Epoch [16/200], Batch [20/107], D_loss: 0.0007, G_loss: 9.4010\n",
      "Epoch [16/200], Batch [30/107], D_loss: 0.0004, G_loss: 10.1269\n",
      "Epoch [16/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.3155\n",
      "Epoch [16/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.8409\n",
      "Epoch [16/200], Batch [60/107], D_loss: 0.0006, G_loss: 8.8337\n",
      "Epoch [16/200], Batch [70/107], D_loss: 0.0006, G_loss: 10.2448\n",
      "Epoch [16/200], Batch [80/107], D_loss: 0.0014, G_loss: 7.5189\n",
      "Epoch [16/200], Batch [90/107], D_loss: 0.0003, G_loss: 9.3206\n",
      "Epoch [16/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.6276\n",
      "Epoch [16/200], D_loss: 0.0007, G_GAN_loss: 8.5529, G_L1_loss: 1.3077, G_perc_loss: 0.0425, G_total_loss: 9.9031, \n",
      "Epoch [17/200], Batch [0/107], D_loss: 0.0004, G_loss: 9.5768\n",
      "Epoch [17/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.9250\n",
      "Epoch [17/200], Batch [20/107], D_loss: 0.0001, G_loss: 11.2409\n",
      "Epoch [17/200], Batch [30/107], D_loss: 0.0002, G_loss: 10.1995\n",
      "Epoch [17/200], Batch [40/107], D_loss: 0.0002, G_loss: 11.2088\n",
      "Epoch [17/200], Batch [50/107], D_loss: 0.0003, G_loss: 12.3815\n",
      "Epoch [17/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.8578\n",
      "Epoch [17/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.5587\n",
      "Epoch [17/200], Batch [80/107], D_loss: 0.0005, G_loss: 8.2735\n",
      "Epoch [17/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.6996\n",
      "Epoch [17/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.1514\n",
      "Epoch [17/200], D_loss: 0.0005, G_GAN_loss: 9.1121, G_L1_loss: 1.3384, G_perc_loss: 0.0428, G_total_loss: 10.4933, \n",
      "Epoch [18/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.0719\n",
      "Epoch [18/200], Batch [10/107], D_loss: 0.0010, G_loss: 8.2974\n",
      "Epoch [18/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.6779\n",
      "Epoch [18/200], Batch [30/107], D_loss: 0.0011, G_loss: 9.0734\n",
      "Epoch [18/200], Batch [40/107], D_loss: 0.0005, G_loss: 9.3844\n",
      "Epoch [18/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.2456\n",
      "Epoch [18/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.5846\n",
      "Epoch [18/200], Batch [70/107], D_loss: 0.0002, G_loss: 10.7913\n",
      "Epoch [18/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.0039\n",
      "Epoch [18/200], Batch [90/107], D_loss: 0.0002, G_loss: 9.4064\n",
      "Epoch [18/200], Batch [100/107], D_loss: 0.0001, G_loss: 10.6308\n",
      "Epoch [18/200], D_loss: 0.0005, G_GAN_loss: 9.1048, G_L1_loss: 1.2968, G_perc_loss: 0.0424, G_total_loss: 10.4440, \n",
      "Epoch [19/200], Batch [0/107], D_loss: 0.0002, G_loss: 11.6637\n",
      "Epoch [19/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.0282\n",
      "Epoch [19/200], Batch [20/107], D_loss: 0.0001, G_loss: 10.9057\n",
      "Epoch [19/200], Batch [30/107], D_loss: 0.0002, G_loss: 11.4090\n",
      "Epoch [19/200], Batch [40/107], D_loss: 0.0004, G_loss: 8.4064\n",
      "Epoch [19/200], Batch [50/107], D_loss: 0.0003, G_loss: 11.3976\n",
      "Epoch [19/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.3297\n",
      "Epoch [19/200], Batch [70/107], D_loss: 0.0004, G_loss: 10.7751\n",
      "Epoch [19/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.1741\n",
      "Epoch [19/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.4186\n",
      "Epoch [19/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.1470\n",
      "Epoch [19/200], D_loss: 0.0003, G_GAN_loss: 9.2910, G_L1_loss: 1.2841, G_perc_loss: 0.0424, G_total_loss: 10.6174, \n",
      "Epoch [20/200], Batch [0/107], D_loss: 0.0003, G_loss: 11.1715\n",
      "Epoch [20/200], Batch [10/107], D_loss: 0.0004, G_loss: 9.3570\n",
      "Epoch [20/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.8660\n",
      "Epoch [20/200], Batch [30/107], D_loss: 0.0004, G_loss: 9.6932\n",
      "Epoch [20/200], Batch [40/107], D_loss: 0.0005, G_loss: 9.3127\n",
      "Epoch [20/200], Batch [50/107], D_loss: 0.0065, G_loss: 9.1678\n",
      "Epoch [20/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.7103\n",
      "Epoch [20/200], Batch [70/107], D_loss: 0.0002, G_loss: 10.2979\n",
      "Epoch [20/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.1499\n",
      "Epoch [20/200], Batch [90/107], D_loss: 0.0003, G_loss: 9.3291\n",
      "Epoch [20/200], Batch [100/107], D_loss: 0.0006, G_loss: 9.5064\n",
      "Epoch [20/200], D_loss: 0.0010, G_GAN_loss: 8.9278, G_L1_loss: 1.2688, G_perc_loss: 0.0423, G_total_loss: 10.2389, \n",
      "Epoch [21/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.2618\n",
      "Epoch [21/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.4634\n",
      "Epoch [21/200], Batch [20/107], D_loss: 0.0004, G_loss: 10.9705\n",
      "Epoch [21/200], Batch [30/107], D_loss: 0.0005, G_loss: 10.7282\n",
      "Epoch [21/200], Batch [40/107], D_loss: 0.0002, G_loss: 11.0552\n",
      "Epoch [21/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.7179\n",
      "Epoch [21/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.3362\n",
      "Epoch [21/200], Batch [70/107], D_loss: 0.0002, G_loss: 10.4458\n",
      "Epoch [21/200], Batch [80/107], D_loss: 0.0001, G_loss: 10.8121\n",
      "Epoch [21/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.3182\n",
      "Epoch [21/200], Batch [100/107], D_loss: 0.0001, G_loss: 10.7411\n",
      "Epoch [21/200], D_loss: 0.0003, G_GAN_loss: 9.3812, G_L1_loss: 1.2582, G_perc_loss: 0.0423, G_total_loss: 10.6817, \n",
      "Epoch [22/200], Batch [0/107], D_loss: 0.0006, G_loss: 10.8788\n",
      "Epoch [22/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.6605\n",
      "Epoch [22/200], Batch [20/107], D_loss: 0.0002, G_loss: 9.6854\n",
      "Epoch [22/200], Batch [30/107], D_loss: 0.0001, G_loss: 12.2536\n",
      "Epoch [22/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.5818\n",
      "Epoch [22/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.5666\n",
      "Epoch [22/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.6646\n",
      "Epoch [22/200], Batch [70/107], D_loss: 0.0009, G_loss: 11.8527\n",
      "Epoch [22/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.4093\n",
      "Epoch [22/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.6352\n",
      "Epoch [22/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.0720\n",
      "Epoch [22/200], D_loss: 0.0005, G_GAN_loss: 9.6003, G_L1_loss: 1.2519, G_perc_loss: 0.0424, G_total_loss: 10.8946, \n",
      "Epoch [23/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.1286\n",
      "Epoch [23/200], Batch [10/107], D_loss: 0.0004, G_loss: 9.6164\n",
      "Epoch [23/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.0151\n",
      "Epoch [23/200], Batch [30/107], D_loss: 0.0001, G_loss: 10.7093\n",
      "Epoch [23/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.0082\n",
      "Epoch [23/200], Batch [50/107], D_loss: 0.0007, G_loss: 9.4112\n",
      "Epoch [23/200], Batch [60/107], D_loss: 0.0008, G_loss: 8.4353\n",
      "Epoch [23/200], Batch [70/107], D_loss: 0.0007, G_loss: 11.3863\n",
      "Epoch [23/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.0032\n",
      "Epoch [23/200], Batch [90/107], D_loss: 0.0001, G_loss: 9.6617\n",
      "Epoch [23/200], Batch [100/107], D_loss: 0.0001, G_loss: 10.6971\n",
      "Epoch [23/200], D_loss: 0.0004, G_GAN_loss: 9.6272, G_L1_loss: 1.2382, G_perc_loss: 0.0423, G_total_loss: 10.9076, \n",
      "Epoch [24/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.5201\n",
      "Epoch [24/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.9103\n",
      "Epoch [24/200], Batch [20/107], D_loss: 0.0007, G_loss: 12.2872\n",
      "Epoch [24/200], Batch [30/107], D_loss: 0.0018, G_loss: 8.0665\n",
      "Epoch [24/200], Batch [40/107], D_loss: 0.0002, G_loss: 11.8090\n",
      "Epoch [24/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.5430\n",
      "Epoch [24/200], Batch [60/107], D_loss: 0.0003, G_loss: 11.5306\n",
      "Epoch [24/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.8553\n",
      "Epoch [24/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.2142\n",
      "Epoch [24/200], Batch [90/107], D_loss: 0.0014, G_loss: 8.6801\n",
      "Epoch [24/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.3237\n",
      "Epoch [24/200], D_loss: 0.0003, G_GAN_loss: 9.8240, G_L1_loss: 1.2300, G_perc_loss: 0.0422, G_total_loss: 11.0962, \n",
      "Epoch [25/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.2380\n",
      "Epoch [25/200], Batch [10/107], D_loss: 0.0000, G_loss: 13.5264\n",
      "Epoch [25/200], Batch [20/107], D_loss: 0.0006, G_loss: 12.1205\n",
      "Epoch [25/200], Batch [30/107], D_loss: 0.0003, G_loss: 9.8723\n",
      "Epoch [25/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.3222\n",
      "Epoch [25/200], Batch [50/107], D_loss: 0.0001, G_loss: 11.9729\n",
      "Epoch [25/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.4370\n",
      "Epoch [25/200], Batch [70/107], D_loss: 0.0004, G_loss: 11.0745\n",
      "Epoch [25/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.0685\n",
      "Epoch [25/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.9987\n",
      "Epoch [25/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.2028\n",
      "Epoch [25/200], D_loss: 0.0003, G_GAN_loss: 10.0021, G_L1_loss: 1.3173, G_perc_loss: 0.0431, G_total_loss: 11.3625, \n",
      "Epoch [26/200], Batch [0/107], D_loss: 0.0000, G_loss: 12.2338\n",
      "Epoch [26/200], Batch [10/107], D_loss: 5.1956, G_loss: 4.5290\n",
      "Epoch [26/200], Batch [20/107], D_loss: 0.9787, G_loss: 2.3588\n",
      "Epoch [26/200], Batch [30/107], D_loss: 0.9539, G_loss: 1.9062\n",
      "Epoch [26/200], Batch [40/107], D_loss: 0.8498, G_loss: 2.0226\n",
      "Epoch [26/200], Batch [50/107], D_loss: 0.7560, G_loss: 2.2502\n",
      "Epoch [26/200], Batch [60/107], D_loss: 0.7343, G_loss: 2.0625\n",
      "Epoch [26/200], Batch [70/107], D_loss: 0.8650, G_loss: 2.0615\n",
      "Epoch [26/200], Batch [80/107], D_loss: 0.5525, G_loss: 2.2343\n",
      "Epoch [26/200], Batch [90/107], D_loss: 0.4671, G_loss: 2.2198\n",
      "Epoch [26/200], Batch [100/107], D_loss: 0.3867, G_loss: 2.5013\n",
      "Epoch [26/200], D_loss: 1.0497, G_GAN_loss: 1.7982, G_L1_loss: 1.2243, G_perc_loss: 0.0423, G_total_loss: 3.0649, \n",
      "Save best! epoch [26]!\n",
      "Epoch [27/200], Batch [0/107], D_loss: 0.3391, G_loss: 2.9705\n",
      "Epoch [27/200], Batch [10/107], D_loss: 0.3184, G_loss: 3.6280\n",
      "Epoch [27/200], Batch [20/107], D_loss: 0.2689, G_loss: 2.9577\n",
      "Epoch [27/200], Batch [30/107], D_loss: 0.6967, G_loss: 2.3176\n",
      "Epoch [27/200], Batch [40/107], D_loss: 0.3698, G_loss: 2.8427\n",
      "Epoch [27/200], Batch [50/107], D_loss: 0.1792, G_loss: 3.3580\n",
      "Epoch [27/200], Batch [60/107], D_loss: 0.1312, G_loss: 3.8467\n",
      "Epoch [27/200], Batch [70/107], D_loss: 0.0931, G_loss: 3.9465\n",
      "Epoch [27/200], Batch [80/107], D_loss: 0.0651, G_loss: 4.6428\n",
      "Epoch [27/200], Batch [90/107], D_loss: 0.0627, G_loss: 4.1679\n",
      "Epoch [27/200], Batch [100/107], D_loss: 0.0423, G_loss: 4.5998\n",
      "Epoch [27/200], D_loss: 0.2579, G_GAN_loss: 2.3548, G_L1_loss: 1.2143, G_perc_loss: 0.0422, G_total_loss: 3.6113, \n",
      "Epoch [28/200], Batch [0/107], D_loss: 0.0427, G_loss: 4.8028\n",
      "Epoch [28/200], Batch [10/107], D_loss: 0.0372, G_loss: 4.8413\n",
      "Epoch [28/200], Batch [20/107], D_loss: 0.0760, G_loss: 5.1600\n",
      "Epoch [28/200], Batch [30/107], D_loss: 0.0590, G_loss: 5.0700\n",
      "Epoch [28/200], Batch [40/107], D_loss: 0.0267, G_loss: 4.8008\n",
      "Epoch [28/200], Batch [50/107], D_loss: 0.0481, G_loss: 5.4079\n",
      "Epoch [28/200], Batch [60/107], D_loss: 0.1001, G_loss: 4.5701\n",
      "Epoch [28/200], Batch [70/107], D_loss: 0.0172, G_loss: 5.5375\n",
      "Epoch [28/200], Batch [80/107], D_loss: 0.0152, G_loss: 5.9247\n",
      "Epoch [28/200], Batch [90/107], D_loss: 0.0127, G_loss: 5.9799\n",
      "Epoch [28/200], Batch [100/107], D_loss: 0.0153, G_loss: 5.2729\n",
      "Epoch [28/200], D_loss: 0.0420, G_GAN_loss: 4.1017, G_L1_loss: 1.2048, G_perc_loss: 0.0420, G_total_loss: 5.3485, \n",
      "Epoch [29/200], Batch [0/107], D_loss: 0.0173, G_loss: 5.7371\n",
      "Epoch [29/200], Batch [10/107], D_loss: 0.0146, G_loss: 5.5877\n",
      "Epoch [29/200], Batch [20/107], D_loss: 0.0127, G_loss: 6.3535\n",
      "Epoch [29/200], Batch [30/107], D_loss: 0.0062, G_loss: 6.4665\n",
      "Epoch [29/200], Batch [40/107], D_loss: 0.0083, G_loss: 6.1074\n",
      "Epoch [29/200], Batch [50/107], D_loss: 1.7315, G_loss: 1.6647\n",
      "Epoch [29/200], Batch [60/107], D_loss: 1.3735, G_loss: 5.0972\n",
      "Epoch [29/200], Batch [70/107], D_loss: 0.0681, G_loss: 4.0892\n",
      "Epoch [29/200], Batch [80/107], D_loss: 0.0508, G_loss: 4.4186\n",
      "Epoch [29/200], Batch [90/107], D_loss: 0.0373, G_loss: 4.6465\n",
      "Epoch [29/200], Batch [100/107], D_loss: 0.0281, G_loss: 5.2118\n",
      "Epoch [29/200], D_loss: 0.1846, G_GAN_loss: 4.0588, G_L1_loss: 1.1978, G_perc_loss: 0.0420, G_total_loss: 5.2987, \n",
      "Epoch [30/200], Batch [0/107], D_loss: 0.0463, G_loss: 5.5923\n",
      "Epoch [30/200], Batch [10/107], D_loss: 0.0400, G_loss: 5.3934\n",
      "Epoch [30/200], Batch [20/107], D_loss: 0.0395, G_loss: 4.5463\n",
      "Epoch [30/200], Batch [30/107], D_loss: 0.0198, G_loss: 5.9184\n",
      "Epoch [30/200], Batch [40/107], D_loss: 0.0200, G_loss: 6.0344\n",
      "Epoch [30/200], Batch [50/107], D_loss: 0.0124, G_loss: 5.7543\n",
      "Epoch [30/200], Batch [60/107], D_loss: 0.0260, G_loss: 6.1891\n",
      "Epoch [30/200], Batch [70/107], D_loss: 0.0134, G_loss: 6.1849\n",
      "Epoch [30/200], Batch [80/107], D_loss: 0.0218, G_loss: 6.0178\n",
      "Epoch [30/200], Batch [90/107], D_loss: 0.0100, G_loss: 5.9632\n",
      "Epoch [30/200], Batch [100/107], D_loss: 0.0218, G_loss: 6.6849\n",
      "Epoch [30/200], D_loss: 0.0209, G_GAN_loss: 4.5235, G_L1_loss: 1.1904, G_perc_loss: 0.0419, G_total_loss: 5.7558, \n",
      "Epoch [31/200], Batch [0/107], D_loss: 0.0125, G_loss: 6.4183\n",
      "Epoch [31/200], Batch [10/107], D_loss: 0.0124, G_loss: 6.0035\n",
      "Epoch [31/200], Batch [20/107], D_loss: 0.0352, G_loss: 5.9985\n",
      "Epoch [31/200], Batch [30/107], D_loss: 0.0125, G_loss: 6.1332\n",
      "Epoch [31/200], Batch [40/107], D_loss: 0.0086, G_loss: 6.8768\n",
      "Epoch [31/200], Batch [50/107], D_loss: 0.0118, G_loss: 7.1518\n",
      "Epoch [31/200], Batch [60/107], D_loss: 0.0066, G_loss: 6.5554\n",
      "Epoch [31/200], Batch [70/107], D_loss: 0.0092, G_loss: 6.0483\n",
      "Epoch [31/200], Batch [80/107], D_loss: 0.0046, G_loss: 6.9307\n",
      "Epoch [31/200], Batch [90/107], D_loss: 0.0084, G_loss: 6.3957\n",
      "Epoch [31/200], Batch [100/107], D_loss: 0.0037, G_loss: 7.1901\n",
      "Epoch [31/200], D_loss: 0.0171, G_GAN_loss: 5.2433, G_L1_loss: 1.1850, G_perc_loss: 0.0418, G_total_loss: 6.4701, \n",
      "Epoch [32/200], Batch [0/107], D_loss: 0.0064, G_loss: 7.6466\n",
      "Epoch [32/200], Batch [10/107], D_loss: 0.0047, G_loss: 6.7556\n",
      "Epoch [32/200], Batch [20/107], D_loss: 0.0033, G_loss: 7.1817\n",
      "Epoch [32/200], Batch [30/107], D_loss: 0.0127, G_loss: 5.9884\n",
      "Epoch [32/200], Batch [40/107], D_loss: 0.0057, G_loss: 7.3695\n",
      "Epoch [32/200], Batch [50/107], D_loss: 0.0035, G_loss: 7.5694\n",
      "Epoch [32/200], Batch [60/107], D_loss: 0.0034, G_loss: 7.1084\n",
      "Epoch [32/200], Batch [70/107], D_loss: 0.0041, G_loss: 8.0162\n",
      "Epoch [32/200], Batch [80/107], D_loss: 0.0069, G_loss: 7.4235\n",
      "Epoch [32/200], Batch [90/107], D_loss: 0.0032, G_loss: 7.2352\n",
      "Epoch [32/200], Batch [100/107], D_loss: 0.0040, G_loss: 7.7700\n",
      "Epoch [32/200], D_loss: 0.0084, G_GAN_loss: 5.8266, G_L1_loss: 1.1793, G_perc_loss: 0.0417, G_total_loss: 7.0476, \n",
      "Epoch [33/200], Batch [0/107], D_loss: 0.0054, G_loss: 6.5753\n",
      "Epoch [33/200], Batch [10/107], D_loss: 0.0048, G_loss: 7.9809\n",
      "Epoch [33/200], Batch [20/107], D_loss: 0.0048, G_loss: 6.9750\n",
      "Epoch [33/200], Batch [30/107], D_loss: 0.0024, G_loss: 7.8622\n",
      "Epoch [33/200], Batch [40/107], D_loss: 0.0031, G_loss: 7.9320\n",
      "Epoch [33/200], Batch [50/107], D_loss: 0.0032, G_loss: 6.7394\n",
      "Epoch [33/200], Batch [60/107], D_loss: 0.0392, G_loss: 5.7035\n",
      "Epoch [33/200], Batch [70/107], D_loss: 0.0068, G_loss: 7.6647\n",
      "Epoch [33/200], Batch [80/107], D_loss: 0.0114, G_loss: 5.8073\n",
      "Epoch [33/200], Batch [90/107], D_loss: 0.0077, G_loss: 7.6648\n",
      "Epoch [33/200], Batch [100/107], D_loss: 0.0094, G_loss: 5.3380\n",
      "Epoch [33/200], D_loss: 0.0200, G_GAN_loss: 5.9026, G_L1_loss: 1.1724, G_perc_loss: 0.0416, G_total_loss: 7.1166, \n",
      "Epoch [34/200], Batch [0/107], D_loss: 0.0039, G_loss: 7.3471\n",
      "Epoch [34/200], Batch [10/107], D_loss: 0.0041, G_loss: 7.4698\n",
      "Epoch [34/200], Batch [20/107], D_loss: 0.0032, G_loss: 7.5664\n",
      "Epoch [34/200], Batch [30/107], D_loss: 0.0088, G_loss: 6.1967\n",
      "Epoch [34/200], Batch [40/107], D_loss: 0.0054, G_loss: 7.7396\n",
      "Epoch [34/200], Batch [50/107], D_loss: 0.0035, G_loss: 7.3228\n",
      "Epoch [34/200], Batch [60/107], D_loss: 0.0046, G_loss: 6.4814\n",
      "Epoch [34/200], Batch [70/107], D_loss: 0.0026, G_loss: 7.5857\n",
      "Epoch [34/200], Batch [80/107], D_loss: 0.0022, G_loss: 7.8901\n",
      "Epoch [34/200], Batch [90/107], D_loss: 0.0025, G_loss: 7.3933\n",
      "Epoch [34/200], Batch [100/107], D_loss: 0.0029, G_loss: 7.7022\n",
      "Epoch [34/200], D_loss: 0.0068, G_GAN_loss: 6.1257, G_L1_loss: 1.1684, G_perc_loss: 0.0415, G_total_loss: 7.3355, \n",
      "Epoch [35/200], Batch [0/107], D_loss: 0.0039, G_loss: 7.9527\n",
      "Epoch [35/200], Batch [10/107], D_loss: 0.0031, G_loss: 8.1788\n",
      "Epoch [35/200], Batch [20/107], D_loss: 0.0024, G_loss: 7.2489\n",
      "Epoch [35/200], Batch [30/107], D_loss: 0.0040, G_loss: 7.1760\n",
      "Epoch [35/200], Batch [40/107], D_loss: 0.0030, G_loss: 8.0878\n",
      "Epoch [35/200], Batch [50/107], D_loss: 0.0021, G_loss: 8.1802\n",
      "Epoch [35/200], Batch [60/107], D_loss: 0.0015, G_loss: 7.7571\n",
      "Epoch [35/200], Batch [70/107], D_loss: 0.0052, G_loss: 6.1913\n",
      "Epoch [35/200], Batch [80/107], D_loss: 0.0049, G_loss: 6.4245\n",
      "Epoch [35/200], Batch [90/107], D_loss: 0.0105, G_loss: 5.9427\n",
      "Epoch [35/200], Batch [100/107], D_loss: 0.0039, G_loss: 7.7218\n",
      "Epoch [35/200], D_loss: 0.0087, G_GAN_loss: 6.4378, G_L1_loss: 1.1630, G_perc_loss: 0.0415, G_total_loss: 7.6422, \n",
      "Epoch [36/200], Batch [0/107], D_loss: 0.0028, G_loss: 7.5916\n",
      "Epoch [36/200], Batch [10/107], D_loss: 0.0025, G_loss: 8.0736\n",
      "Epoch [36/200], Batch [20/107], D_loss: 0.0044, G_loss: 8.6116\n",
      "Epoch [36/200], Batch [30/107], D_loss: 0.0038, G_loss: 7.4891\n",
      "Epoch [36/200], Batch [40/107], D_loss: 0.0032, G_loss: 7.9903\n",
      "Epoch [36/200], Batch [50/107], D_loss: 0.0037, G_loss: 7.1627\n",
      "Epoch [36/200], Batch [60/107], D_loss: 0.5844, G_loss: 5.1465\n",
      "Epoch [36/200], Batch [70/107], D_loss: 0.0229, G_loss: 6.3014\n",
      "Epoch [36/200], Batch [80/107], D_loss: 0.0089, G_loss: 6.5775\n",
      "Epoch [36/200], Batch [90/107], D_loss: 0.0071, G_loss: 7.5047\n",
      "Epoch [36/200], Batch [100/107], D_loss: 0.0023, G_loss: 7.8764\n",
      "Epoch [36/200], D_loss: 0.0433, G_GAN_loss: 6.2323, G_L1_loss: 1.1572, G_perc_loss: 0.0414, G_total_loss: 7.4310, \n",
      "Epoch [37/200], Batch [0/107], D_loss: 0.0050, G_loss: 7.0734\n",
      "Epoch [37/200], Batch [10/107], D_loss: 0.0039, G_loss: 7.0062\n",
      "Epoch [37/200], D_loss: 0.0048, G_GAN_loss: 6.4081, G_L1_loss: 1.1530, G_perc_loss: 0.0413, G_total_loss: 7.6024, \n",
      "Epoch [38/200], Batch [0/107], D_loss: 0.0015, G_loss: 8.8538\n",
      "Epoch [38/200], Batch [10/107], D_loss: 0.0022, G_loss: 8.7406\n",
      "Epoch [38/200], Batch [20/107], D_loss: 0.0138, G_loss: 8.9965\n",
      "Epoch [38/200], Batch [30/107], D_loss: 0.0059, G_loss: 6.3765\n",
      "Epoch [38/200], Batch [40/107], D_loss: 0.0019, G_loss: 7.1991\n",
      "Epoch [38/200], Batch [50/107], D_loss: 0.0032, G_loss: 9.5768\n",
      "Epoch [38/200], Batch [60/107], D_loss: 0.0013, G_loss: 8.6038\n",
      "Epoch [38/200], Batch [70/107], D_loss: 0.0007, G_loss: 8.9136\n",
      "Epoch [38/200], Batch [80/107], D_loss: 0.0008, G_loss: 8.7182\n",
      "Epoch [38/200], Batch [90/107], D_loss: 0.0020, G_loss: 7.8490\n",
      "Epoch [38/200], Batch [100/107], D_loss: 0.0009, G_loss: 7.9347\n",
      "Epoch [38/200], D_loss: 0.0028, G_GAN_loss: 6.9348, G_L1_loss: 1.1972, G_perc_loss: 0.0422, G_total_loss: 8.1742, \n",
      "Epoch [39/200], Batch [0/107], D_loss: 0.0012, G_loss: 8.3468\n",
      "Epoch [39/200], Batch [10/107], D_loss: 0.0008, G_loss: 9.1175\n",
      "Epoch [39/200], Batch [20/107], D_loss: 0.0029, G_loss: 8.3097\n",
      "Epoch [39/200], Batch [30/107], D_loss: 0.0011, G_loss: 8.6932\n",
      "Epoch [39/200], Batch [40/107], D_loss: 0.0017, G_loss: 8.8929\n",
      "Epoch [39/200], Batch [50/107], D_loss: 0.0009, G_loss: 8.6329\n",
      "Epoch [39/200], Batch [60/107], D_loss: 0.0032, G_loss: 7.0521\n",
      "Epoch [39/200], Batch [70/107], D_loss: 0.0017, G_loss: 8.2170\n",
      "Epoch [39/200], Batch [80/107], D_loss: 0.0015, G_loss: 8.0172\n",
      "Epoch [39/200], Batch [90/107], D_loss: 0.0011, G_loss: 8.0179\n",
      "Epoch [39/200], Batch [100/107], D_loss: 0.0017, G_loss: 8.4471\n",
      "Epoch [39/200], D_loss: 0.0018, G_GAN_loss: 7.1855, G_L1_loss: 1.1549, G_perc_loss: 0.0412, G_total_loss: 8.3816, \n",
      "Epoch [40/200], Batch [0/107], D_loss: 0.0009, G_loss: 8.7134\n",
      "Epoch [40/200], Batch [10/107], D_loss: 0.0013, G_loss: 8.2171\n",
      "Epoch [40/200], Batch [20/107], D_loss: 0.0013, G_loss: 9.1987\n",
      "Epoch [40/200], Batch [30/107], D_loss: 0.0011, G_loss: 8.4634\n",
      "Epoch [40/200], Batch [40/107], D_loss: 0.0150, G_loss: 7.1355\n",
      "Epoch [40/200], Batch [50/107], D_loss: 0.0015, G_loss: 8.2672\n",
      "Epoch [40/200], Batch [60/107], D_loss: 0.0013, G_loss: 7.9682\n",
      "Epoch [40/200], Batch [70/107], D_loss: 0.0129, G_loss: 5.3002\n",
      "Epoch [40/200], Batch [80/107], D_loss: 0.0009, G_loss: 8.2736\n",
      "Epoch [40/200], Batch [90/107], D_loss: 0.0035, G_loss: 6.9578\n",
      "Epoch [40/200], Batch [100/107], D_loss: 0.0010, G_loss: 8.9794\n",
      "Epoch [40/200], D_loss: 0.0026, G_GAN_loss: 7.2715, G_L1_loss: 1.1539, G_perc_loss: 0.0413, G_total_loss: 8.4667, \n",
      "Epoch [41/200], Batch [0/107], D_loss: 0.0007, G_loss: 8.8395\n",
      "Epoch [41/200], Batch [10/107], D_loss: 0.0009, G_loss: 9.2874\n",
      "Epoch [41/200], Batch [20/107], D_loss: 0.0009, G_loss: 7.8308\n",
      "Epoch [41/200], Batch [30/107], D_loss: 0.0015, G_loss: 9.3155\n",
      "Epoch [41/200], Batch [40/107], D_loss: 0.0010, G_loss: 8.9730\n",
      "Epoch [41/200], Batch [50/107], D_loss: 0.0022, G_loss: 9.4862\n",
      "Epoch [41/200], Batch [60/107], D_loss: 0.0012, G_loss: 8.4767\n",
      "Epoch [41/200], Batch [70/107], D_loss: 0.0022, G_loss: 7.3227\n",
      "Epoch [41/200], Batch [80/107], D_loss: 0.0009, G_loss: 9.1688\n",
      "Epoch [41/200], Batch [90/107], D_loss: 0.0012, G_loss: 9.3616\n",
      "Epoch [41/200], Batch [100/107], D_loss: 0.0010, G_loss: 9.7697\n",
      "Epoch [41/200], D_loss: 0.0023, G_GAN_loss: 7.6549, G_L1_loss: 1.1407, G_perc_loss: 0.0411, G_total_loss: 8.8367, \n",
      "Epoch [42/200], Batch [0/107], D_loss: 0.0007, G_loss: 8.6110\n",
      "Epoch [42/200], Batch [10/107], D_loss: 0.0011, G_loss: 7.9502\n",
      "Epoch [42/200], Batch [20/107], D_loss: 0.0010, G_loss: 6.8907\n",
      "Epoch [42/200], Batch [30/107], D_loss: 0.0005, G_loss: 9.2497\n",
      "Epoch [42/200], Batch [40/107], D_loss: 0.0010, G_loss: 8.5503\n",
      "Epoch [42/200], Batch [50/107], D_loss: 0.0010, G_loss: 8.7312\n",
      "Epoch [42/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.4373\n",
      "Epoch [42/200], Batch [70/107], D_loss: 0.0014, G_loss: 9.5056\n",
      "Epoch [42/200], Batch [80/107], D_loss: 0.0012, G_loss: 7.9597\n",
      "Epoch [42/200], Batch [90/107], D_loss: 0.0005, G_loss: 8.9360\n",
      "Epoch [42/200], Batch [100/107], D_loss: 0.0069, G_loss: 6.4792\n",
      "Epoch [42/200], D_loss: 0.0013, G_GAN_loss: 7.6820, G_L1_loss: 1.1353, G_perc_loss: 0.0409, G_total_loss: 8.8582, \n",
      "Epoch [43/200], Batch [0/107], D_loss: 0.0007, G_loss: 8.2162\n",
      "Epoch [43/200], Batch [10/107], D_loss: 0.0028, G_loss: 8.6534\n",
      "Epoch [43/200], Batch [20/107], D_loss: 0.0015, G_loss: 9.8122\n",
      "Epoch [43/200], Batch [30/107], D_loss: 0.0007, G_loss: 8.4618\n",
      "Epoch [43/200], Batch [40/107], D_loss: 0.0017, G_loss: 7.5404\n",
      "Epoch [43/200], Batch [50/107], D_loss: 0.0008, G_loss: 8.4541\n",
      "Epoch [43/200], Batch [60/107], D_loss: 0.0011, G_loss: 8.2156\n",
      "Epoch [43/200], Batch [70/107], D_loss: 0.0006, G_loss: 8.8821\n",
      "Epoch [43/200], Batch [80/107], D_loss: 0.0007, G_loss: 9.0754\n",
      "Epoch [43/200], Batch [90/107], D_loss: 0.0007, G_loss: 9.9083\n",
      "Epoch [43/200], Batch [100/107], D_loss: 0.0006, G_loss: 9.0791\n",
      "Epoch [43/200], D_loss: 0.0013, G_GAN_loss: 7.7784, G_L1_loss: 1.1319, G_perc_loss: 0.0408, G_total_loss: 8.9511, \n",
      "Epoch [44/200], Batch [0/107], D_loss: 0.0047, G_loss: 7.6744\n",
      "Epoch [44/200], Batch [10/107], D_loss: 0.0015, G_loss: 8.0621\n",
      "Epoch [44/200], Batch [20/107], D_loss: 0.0052, G_loss: 6.8392\n",
      "Epoch [44/200], Batch [30/107], D_loss: 0.0008, G_loss: 8.6258\n",
      "Epoch [44/200], Batch [40/107], D_loss: 0.0005, G_loss: 9.7856\n",
      "Epoch [44/200], Batch [50/107], D_loss: 0.0007, G_loss: 8.1083\n",
      "Epoch [44/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.6276\n",
      "Epoch [44/200], Batch [70/107], D_loss: 0.0024, G_loss: 10.0968\n",
      "Epoch [44/200], Batch [80/107], D_loss: 0.0009, G_loss: 8.0698\n",
      "Epoch [44/200], Batch [90/107], D_loss: 0.0012, G_loss: 8.3829\n",
      "Epoch [44/200], Batch [100/107], D_loss: 0.0011, G_loss: 8.4981\n",
      "Epoch [44/200], D_loss: 0.0015, G_GAN_loss: 7.8686, G_L1_loss: 1.1275, G_perc_loss: 0.0406, G_total_loss: 9.0368, \n",
      "Epoch [45/200], Batch [0/107], D_loss: 0.0006, G_loss: 8.8390\n",
      "Epoch [45/200], Batch [10/107], D_loss: 0.4443, G_loss: 4.4718\n",
      "Epoch [45/200], Batch [20/107], D_loss: 5.1099, G_loss: 5.4119\n",
      "Epoch [45/200], Batch [30/107], D_loss: 0.2999, G_loss: 4.0677\n",
      "Epoch [45/200], Batch [40/107], D_loss: 0.2075, G_loss: 4.3023\n",
      "Epoch [45/200], Batch [50/107], D_loss: 0.0555, G_loss: 6.3336\n",
      "Epoch [45/200], Batch [60/107], D_loss: 0.0612, G_loss: 6.0292\n",
      "Epoch [45/200], Batch [70/107], D_loss: 0.0971, G_loss: 6.4460\n",
      "Epoch [45/200], Batch [80/107], D_loss: 0.0089, G_loss: 6.3088\n",
      "Epoch [45/200], Batch [90/107], D_loss: 0.0111, G_loss: 6.7830\n",
      "Epoch [45/200], Batch [100/107], D_loss: 0.0213, G_loss: 5.1423\n",
      "Epoch [45/200], D_loss: 0.4716, G_GAN_loss: 4.8606, G_L1_loss: 1.2107, G_perc_loss: 0.0424, G_total_loss: 6.1137, \n",
      "Epoch [46/200], Batch [0/107], D_loss: 0.1278, G_loss: 5.5254\n",
      "Epoch [46/200], Batch [10/107], D_loss: 0.1206, G_loss: 7.2601\n",
      "Epoch [46/200], Batch [20/107], D_loss: 0.0076, G_loss: 5.7643\n",
      "Epoch [46/200], Batch [30/107], D_loss: 0.0095, G_loss: 6.8507\n",
      "Epoch [46/200], Batch [40/107], D_loss: 0.0045, G_loss: 7.5977\n",
      "Epoch [46/200], Batch [50/107], D_loss: 0.0093, G_loss: 6.0440\n",
      "Epoch [46/200], Batch [60/107], D_loss: 0.0048, G_loss: 6.9936\n",
      "Epoch [46/200], Batch [70/107], D_loss: 0.0049, G_loss: 7.1406\n",
      "Epoch [46/200], Batch [80/107], D_loss: 0.0062, G_loss: 6.9679\n",
      "Epoch [46/200], Batch [90/107], D_loss: 0.0042, G_loss: 6.9578\n",
      "Epoch [46/200], Batch [100/107], D_loss: 0.0037, G_loss: 6.5413\n",
      "Epoch [46/200], D_loss: 0.0249, G_GAN_loss: 5.5527, G_L1_loss: 1.1307, G_perc_loss: 0.0406, G_total_loss: 6.7241, \n",
      "Epoch [47/200], Batch [0/107], D_loss: 0.0042, G_loss: 7.5832\n",
      "Epoch [47/200], Batch [10/107], D_loss: 0.0042, G_loss: 6.6870\n",
      "Epoch [47/200], Batch [20/107], D_loss: 0.0256, G_loss: 5.3305\n",
      "Epoch [47/200], Batch [30/107], D_loss: 0.0124, G_loss: 8.0417\n",
      "Epoch [47/200], Batch [40/107], D_loss: 0.0034, G_loss: 7.4405\n",
      "Epoch [47/200], Batch [50/107], D_loss: 0.0501, G_loss: 8.0569\n",
      "Epoch [47/200], Batch [60/107], D_loss: 0.0112, G_loss: 6.9581\n",
      "Epoch [47/200], Batch [70/107], D_loss: 0.0074, G_loss: 6.1933\n",
      "Epoch [47/200], Batch [80/107], D_loss: 0.0027, G_loss: 7.6638\n",
      "Epoch [47/200], Batch [90/107], D_loss: 0.0031, G_loss: 8.0584\n",
      "Epoch [47/200], Batch [100/107], D_loss: 0.0044, G_loss: 6.4608\n",
      "Epoch [47/200], D_loss: 0.0127, G_GAN_loss: 6.0347, G_L1_loss: 1.1239, G_perc_loss: 0.0403, G_total_loss: 7.1988, \n",
      "Epoch [48/200], Batch [0/107], D_loss: 0.0039, G_loss: 7.8098\n",
      "Epoch [48/200], Batch [10/107], D_loss: 0.0022, G_loss: 8.1601\n",
      "Epoch [48/200], Batch [20/107], D_loss: 0.0023, G_loss: 8.2329\n",
      "Epoch [48/200], Batch [30/107], D_loss: 0.0045, G_loss: 7.2014\n",
      "Epoch [48/200], Batch [40/107], D_loss: 0.0056, G_loss: 7.6463\n",
      "Epoch [48/200], Batch [50/107], D_loss: 0.0024, G_loss: 7.3908\n",
      "Epoch [48/200], Batch [60/107], D_loss: 0.0027, G_loss: 8.4577\n",
      "Epoch [48/200], Batch [70/107], D_loss: 0.0116, G_loss: 6.0598\n",
      "Epoch [48/200], Batch [80/107], D_loss: 0.0023, G_loss: 7.9846\n",
      "Epoch [48/200], Batch [90/107], D_loss: 0.0040, G_loss: 6.6853\n",
      "Epoch [48/200], Batch [100/107], D_loss: 0.0029, G_loss: 8.8154\n",
      "Epoch [48/200], D_loss: 0.0055, G_GAN_loss: 6.4947, G_L1_loss: 1.1191, G_perc_loss: 0.0400, G_total_loss: 7.6539, \n",
      "Epoch [49/200], Batch [0/107], D_loss: 0.0017, G_loss: 7.8505\n",
      "Epoch [49/200], Batch [10/107], D_loss: 0.0034, G_loss: 8.8449\n",
      "Epoch [49/200], Batch [20/107], D_loss: 0.0018, G_loss: 7.7158\n",
      "Epoch [49/200], Batch [30/107], D_loss: 0.0018, G_loss: 8.0692\n",
      "Epoch [49/200], Batch [40/107], D_loss: 0.0033, G_loss: 7.0315\n",
      "Epoch [49/200], Batch [50/107], D_loss: 0.0022, G_loss: 8.4230\n",
      "Epoch [49/200], Batch [60/107], D_loss: 0.0023, G_loss: 6.9447\n",
      "Epoch [49/200], Batch [70/107], D_loss: 0.0021, G_loss: 8.6538\n",
      "Epoch [49/200], Batch [80/107], D_loss: 0.0038, G_loss: 6.8367\n",
      "Epoch [49/200], Batch [90/107], D_loss: 0.0018, G_loss: 8.6059\n",
      "Epoch [49/200], Batch [100/107], D_loss: 0.0012, G_loss: 8.4735\n",
      "Epoch [49/200], D_loss: 0.0059, G_GAN_loss: 6.6735, G_L1_loss: 1.1144, G_perc_loss: 0.0398, G_total_loss: 7.8276, \n",
      "Epoch [50/200], Batch [0/107], D_loss: 0.0016, G_loss: 8.1798\n",
      "Epoch [50/200], Batch [10/107], D_loss: 0.0040, G_loss: 7.6716\n",
      "Epoch [50/200], Batch [20/107], D_loss: 0.0043, G_loss: 6.4463\n",
      "Epoch [50/200], Batch [30/107], D_loss: 0.0039, G_loss: 6.3205\n",
      "Epoch [50/200], Batch [40/107], D_loss: 0.0011, G_loss: 8.0598\n",
      "Epoch [50/200], Batch [50/107], D_loss: 0.0044, G_loss: 8.4813\n",
      "Epoch [50/200], Batch [60/107], D_loss: 0.0013, G_loss: 8.2565\n",
      "Epoch [50/200], Batch [70/107], D_loss: 0.0036, G_loss: 6.9997\n",
      "Epoch [50/200], Batch [80/107], D_loss: 0.0010, G_loss: 8.5938\n",
      "Epoch [50/200], Batch [90/107], D_loss: 0.0036, G_loss: 9.0454\n",
      "Epoch [50/200], Batch [100/107], D_loss: 0.0015, G_loss: 7.7124\n",
      "Epoch [50/200], D_loss: 0.0033, G_GAN_loss: 6.8120, G_L1_loss: 1.1184, G_perc_loss: 0.0400, G_total_loss: 7.9704, \n",
      "Epoch [51/200], Batch [0/107], D_loss: 0.0042, G_loss: 6.1670\n",
      "Epoch [51/200], Batch [10/107], D_loss: 0.0011, G_loss: 8.3083\n",
      "Epoch [51/200], Batch [20/107], D_loss: 0.0013, G_loss: 9.1334\n",
      "Epoch [51/200], Batch [30/107], D_loss: 0.0010, G_loss: 8.8698\n",
      "Epoch [51/200], Batch [40/107], D_loss: 0.0009, G_loss: 8.9288\n",
      "Epoch [51/200], Batch [50/107], D_loss: 0.0020, G_loss: 7.3714\n",
      "Epoch [51/200], Batch [60/107], D_loss: 0.0014, G_loss: 7.8744\n",
      "Epoch [51/200], Batch [70/107], D_loss: 0.0096, G_loss: 8.6098\n",
      "Epoch [51/200], Batch [80/107], D_loss: 0.0010, G_loss: 8.8263\n",
      "Epoch [51/200], Batch [90/107], D_loss: 0.0017, G_loss: 7.3788\n",
      "Epoch [51/200], Batch [100/107], D_loss: 0.0009, G_loss: 8.9005\n",
      "Epoch [51/200], D_loss: 0.0019, G_GAN_loss: 7.2132, G_L1_loss: 1.1088, G_perc_loss: 0.0394, G_total_loss: 8.3614, \n",
      "Epoch [52/200], Batch [0/107], D_loss: 0.0073, G_loss: 6.4691\n",
      "Epoch [52/200], Batch [10/107], D_loss: 0.0021, G_loss: 9.2805\n",
      "Epoch [52/200], Batch [20/107], D_loss: 0.0007, G_loss: 9.2319\n",
      "Epoch [52/200], Batch [30/107], D_loss: 0.0009, G_loss: 8.3588\n",
      "Epoch [52/200], Batch [40/107], D_loss: 0.0014, G_loss: 7.2850\n",
      "Epoch [52/200], Batch [50/107], D_loss: 0.0009, G_loss: 8.3077\n",
      "Epoch [52/200], Batch [60/107], D_loss: 0.0007, G_loss: 9.4264\n",
      "Epoch [52/200], Batch [70/107], D_loss: 0.0008, G_loss: 8.3853\n",
      "Epoch [52/200], Batch [80/107], D_loss: 0.0011, G_loss: 8.3947\n",
      "Epoch [52/200], Batch [90/107], D_loss: 0.0041, G_loss: 7.0526\n",
      "Epoch [52/200], Batch [100/107], D_loss: 0.0015, G_loss: 9.4753\n",
      "Epoch [52/200], D_loss: 0.0019, G_GAN_loss: 7.2731, G_L1_loss: 1.1076, G_perc_loss: 0.0392, G_total_loss: 8.4199, \n",
      "Epoch [53/200], Batch [0/107], D_loss: 0.0019, G_loss: 7.4218\n",
      "Epoch [53/200], Batch [10/107], D_loss: 0.0009, G_loss: 9.2690\n",
      "Epoch [53/200], Batch [20/107], D_loss: 0.0019, G_loss: 7.1562\n",
      "Epoch [53/200], Batch [30/107], D_loss: 0.0250, G_loss: 8.1372\n",
      "Epoch [53/200], Batch [40/107], D_loss: 0.0019, G_loss: 8.1966\n",
      "Epoch [53/200], Batch [50/107], D_loss: 0.0022, G_loss: 7.8797\n",
      "Epoch [53/200], Batch [60/107], D_loss: 0.0010, G_loss: 8.6978\n",
      "Epoch [53/200], Batch [70/107], D_loss: 0.0008, G_loss: 8.6579\n",
      "Epoch [53/200], Batch [80/107], D_loss: 0.0017, G_loss: 8.3842\n",
      "Epoch [53/200], Batch [90/107], D_loss: 0.0023, G_loss: 7.7882\n",
      "Epoch [53/200], Batch [100/107], D_loss: 0.0011, G_loss: 8.9600\n",
      "Epoch [53/200], D_loss: 0.0035, G_GAN_loss: 7.1520, G_L1_loss: 1.1027, G_perc_loss: 0.0390, G_total_loss: 8.2937, \n",
      "Epoch [54/200], Batch [0/107], D_loss: 0.0160, G_loss: 7.3165\n",
      "Epoch [54/200], Batch [10/107], D_loss: 0.0098, G_loss: 6.5416\n",
      "Epoch [54/200], Batch [20/107], D_loss: 0.0070, G_loss: 6.9029\n",
      "Epoch [54/200], Batch [30/107], D_loss: 0.0017, G_loss: 7.6395\n",
      "Epoch [54/200], Batch [40/107], D_loss: 0.0009, G_loss: 9.0347\n",
      "Epoch [54/200], Batch [50/107], D_loss: 0.0009, G_loss: 8.6051\n",
      "Epoch [54/200], Batch [60/107], D_loss: 0.0185, G_loss: 8.1130\n",
      "Epoch [54/200], Batch [70/107], D_loss: 0.0052, G_loss: 7.3410\n",
      "Epoch [54/200], Batch [80/107], D_loss: 0.0040, G_loss: 7.3848\n",
      "Epoch [54/200], Batch [90/107], D_loss: 0.0023, G_loss: 9.6861\n",
      "Epoch [54/200], Batch [100/107], D_loss: 0.0028, G_loss: 8.7130\n",
      "Epoch [54/200], D_loss: 0.0609, G_GAN_loss: 6.4420, G_L1_loss: 1.1006, G_perc_loss: 0.0389, G_total_loss: 7.5814, \n",
      "Epoch [55/200], Batch [0/107], D_loss: 0.0041, G_loss: 6.2953\n",
      "Epoch [55/200], Batch [10/107], D_loss: 0.0009, G_loss: 9.3662\n",
      "Epoch [55/200], Batch [20/107], D_loss: 0.0013, G_loss: 8.6169\n",
      "Epoch [55/200], Batch [30/107], D_loss: 0.0028, G_loss: 8.9614\n",
      "Epoch [55/200], Batch [40/107], D_loss: 0.0007, G_loss: 9.0417\n",
      "Epoch [55/200], Batch [50/107], D_loss: 0.0012, G_loss: 8.2256\n",
      "Epoch [55/200], Batch [60/107], D_loss: 0.0027, G_loss: 8.8884\n",
      "Epoch [55/200], Batch [70/107], D_loss: 0.0028, G_loss: 7.1872\n",
      "Epoch [55/200], Batch [80/107], D_loss: 0.0012, G_loss: 9.1783\n",
      "Epoch [55/200], Batch [90/107], D_loss: 0.0039, G_loss: 9.4717\n",
      "Epoch [55/200], Batch [100/107], D_loss: 0.0008, G_loss: 9.0507\n",
      "Epoch [55/200], D_loss: 0.0021, G_GAN_loss: 7.2735, G_L1_loss: 1.1387, G_perc_loss: 0.0398, G_total_loss: 8.4521, \n",
      "Epoch [56/200], Batch [0/107], D_loss: 0.0016, G_loss: 9.4467\n",
      "Epoch [56/200], Batch [10/107], D_loss: 0.0009, G_loss: 8.3385\n",
      "Epoch [56/200], Batch [20/107], D_loss: 0.0008, G_loss: 9.2566\n",
      "Epoch [56/200], Batch [30/107], D_loss: 0.0010, G_loss: 8.1220\n",
      "Epoch [56/200], Batch [40/107], D_loss: 0.0014, G_loss: 7.7834\n",
      "Epoch [56/200], Batch [50/107], D_loss: 0.0015, G_loss: 9.0614\n",
      "Epoch [56/200], Batch [60/107], D_loss: 0.0010, G_loss: 7.6865\n",
      "Epoch [56/200], Batch [70/107], D_loss: 0.0031, G_loss: 7.5208\n",
      "Epoch [56/200], Batch [80/107], D_loss: 0.0009, G_loss: 10.0150\n",
      "Epoch [56/200], Batch [90/107], D_loss: 0.0015, G_loss: 8.5459\n",
      "Epoch [56/200], Batch [100/107], D_loss: 0.0007, G_loss: 9.7491\n",
      "Epoch [56/200], D_loss: 0.0022, G_GAN_loss: 7.4677, G_L1_loss: 1.1008, G_perc_loss: 0.0387, G_total_loss: 8.6072, \n",
      "Epoch [57/200], Batch [0/107], D_loss: 0.0013, G_loss: 6.9759\n",
      "Epoch [57/200], Batch [10/107], D_loss: 0.0010, G_loss: 9.3963\n",
      "Epoch [57/200], Batch [20/107], D_loss: 0.0142, G_loss: 7.1636\n",
      "Epoch [57/200], Batch [30/107], D_loss: 0.0007, G_loss: 8.5286\n",
      "Epoch [57/200], Batch [40/107], D_loss: 0.0006, G_loss: 8.5972\n",
      "Epoch [57/200], Batch [50/107], D_loss: 0.0007, G_loss: 9.0821\n",
      "Epoch [57/200], Batch [60/107], D_loss: 0.0013, G_loss: 10.0316\n",
      "Epoch [57/200], Batch [70/107], D_loss: 0.0008, G_loss: 9.3095\n",
      "Epoch [57/200], Batch [80/107], D_loss: 0.0021, G_loss: 6.7997\n",
      "Epoch [57/200], Batch [90/107], D_loss: 0.0005, G_loss: 9.0327\n",
      "Epoch [57/200], Batch [100/107], D_loss: 0.0004, G_loss: 10.0110\n",
      "Epoch [57/200], D_loss: 0.0018, G_GAN_loss: 7.5761, G_L1_loss: 1.0946, G_perc_loss: 0.0384, G_total_loss: 8.7092, \n",
      "Epoch [58/200], Batch [0/107], D_loss: 0.0005, G_loss: 9.4073\n",
      "Epoch [58/200], Batch [10/107], D_loss: 0.0042, G_loss: 7.8239\n",
      "Epoch [58/200], Batch [20/107], D_loss: 0.0014, G_loss: 7.8878\n",
      "Epoch [58/200], Batch [30/107], D_loss: 0.0009, G_loss: 9.3152\n",
      "Epoch [58/200], Batch [40/107], D_loss: 0.0012, G_loss: 7.8007\n",
      "Epoch [58/200], Batch [50/107], D_loss: 0.0009, G_loss: 9.1021\n",
      "Epoch [58/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.4900\n",
      "Epoch [58/200], Batch [70/107], D_loss: 0.0017, G_loss: 7.5939\n",
      "Epoch [58/200], Batch [80/107], D_loss: 0.0005, G_loss: 8.2402\n",
      "Epoch [58/200], Batch [90/107], D_loss: 0.0006, G_loss: 8.8860\n",
      "Epoch [58/200], Batch [100/107], D_loss: 0.0009, G_loss: 9.5597\n",
      "Epoch [58/200], D_loss: 0.0014, G_GAN_loss: 7.8135, G_L1_loss: 1.0933, G_perc_loss: 0.0383, G_total_loss: 8.9451, \n",
      "Epoch [59/200], Batch [0/107], D_loss: 0.0010, G_loss: 7.1732\n",
      "Epoch [59/200], Batch [10/107], D_loss: 0.0008, G_loss: 9.4722\n",
      "Epoch [59/200], Batch [20/107], D_loss: 0.0009, G_loss: 8.8716\n",
      "Epoch [59/200], Batch [30/107], D_loss: 0.0009, G_loss: 10.1164\n",
      "Epoch [59/200], Batch [40/107], D_loss: 0.0004, G_loss: 9.0859\n",
      "Epoch [59/200], Batch [50/107], D_loss: 0.0061, G_loss: 9.8342\n",
      "Epoch [59/200], Batch [60/107], D_loss: 0.0011, G_loss: 8.1675\n",
      "Epoch [59/200], Batch [70/107], D_loss: 0.0006, G_loss: 9.1738\n",
      "Epoch [59/200], Batch [80/107], D_loss: 0.0007, G_loss: 9.5396\n",
      "Epoch [59/200], Batch [90/107], D_loss: 0.0031, G_loss: 7.1315\n",
      "Epoch [59/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.4883\n",
      "Epoch [59/200], D_loss: 0.0017, G_GAN_loss: 7.8988, G_L1_loss: 1.0900, G_perc_loss: 0.0381, G_total_loss: 9.0269, \n",
      "Epoch [60/200], Batch [0/107], D_loss: 0.0008, G_loss: 10.1551\n",
      "Epoch [60/200], Batch [10/107], D_loss: 0.0007, G_loss: 9.2176\n",
      "Epoch [60/200], Batch [20/107], D_loss: 0.0016, G_loss: 7.6301\n",
      "Epoch [60/200], Batch [30/107], D_loss: 0.0012, G_loss: 8.6166\n",
      "Epoch [60/200], Batch [40/107], D_loss: 0.0006, G_loss: 10.4627\n",
      "Epoch [60/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.4785\n",
      "Epoch [60/200], Batch [60/107], D_loss: 0.0019, G_loss: 10.1916\n",
      "Epoch [60/200], Batch [70/107], D_loss: 0.0006, G_loss: 9.4368\n",
      "Epoch [60/200], Batch [80/107], D_loss: 0.0006, G_loss: 9.3156\n",
      "Epoch [60/200], Batch [90/107], D_loss: 0.0006, G_loss: 8.4935\n",
      "Epoch [60/200], Batch [100/107], D_loss: 0.0012, G_loss: 8.2810\n",
      "Epoch [60/200], D_loss: 0.0011, G_GAN_loss: 8.0693, G_L1_loss: 1.0884, G_perc_loss: 0.0381, G_total_loss: 9.1958, \n",
      "Epoch [61/200], Batch [0/107], D_loss: 0.0010, G_loss: 7.8815\n",
      "Epoch [61/200], Batch [10/107], D_loss: 0.0004, G_loss: 9.1166\n",
      "Epoch [61/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.5816\n",
      "Epoch [61/200], Batch [30/107], D_loss: 0.0006, G_loss: 7.8377\n",
      "Epoch [61/200], Batch [40/107], D_loss: 0.0007, G_loss: 9.3924\n",
      "Epoch [61/200], Batch [50/107], D_loss: 0.0019, G_loss: 10.1613\n",
      "Epoch [61/200], Batch [60/107], D_loss: 0.0024, G_loss: 7.7896\n",
      "Epoch [61/200], Batch [70/107], D_loss: 0.0007, G_loss: 10.1546\n",
      "Epoch [61/200], Batch [80/107], D_loss: 0.0003, G_loss: 9.9086\n",
      "Epoch [61/200], Batch [90/107], D_loss: 0.0053, G_loss: 10.2376\n",
      "Epoch [61/200], Batch [100/107], D_loss: 0.0002, G_loss: 9.6935\n",
      "Epoch [61/200], D_loss: 0.0010, G_GAN_loss: 8.1875, G_L1_loss: 1.0852, G_perc_loss: 0.0378, G_total_loss: 9.3104, \n",
      "Epoch [62/200], Batch [0/107], D_loss: 0.0003, G_loss: 10.0556\n",
      "Epoch [62/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.4041\n",
      "Epoch [62/200], Batch [20/107], D_loss: 0.0010, G_loss: 10.3197\n",
      "Epoch [62/200], Batch [30/107], D_loss: 0.0048, G_loss: 6.1679\n",
      "Epoch [62/200], Batch [40/107], D_loss: 0.0005, G_loss: 9.6477\n",
      "Epoch [62/200], Batch [50/107], D_loss: 0.0012, G_loss: 7.8277\n",
      "Epoch [62/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.0183\n",
      "Epoch [62/200], Batch [70/107], D_loss: 0.0059, G_loss: 4.8414\n",
      "Epoch [62/200], Batch [80/107], D_loss: 0.0371, G_loss: 6.7641\n",
      "Epoch [62/200], Batch [90/107], D_loss: 0.0070, G_loss: 9.3636\n",
      "Epoch [62/200], Batch [100/107], D_loss: 0.0368, G_loss: 9.9680\n",
      "Epoch [62/200], D_loss: 0.0563, G_GAN_loss: 7.6199, G_L1_loss: 1.0849, G_perc_loss: 0.0378, G_total_loss: 8.7425, \n",
      "Epoch [63/200], Batch [0/107], D_loss: 0.1158, G_loss: 9.0734\n",
      "Epoch [63/200], Batch [10/107], D_loss: 0.1009, G_loss: 7.2612\n",
      "Epoch [63/200], Batch [20/107], D_loss: 0.0051, G_loss: 5.5996\n",
      "Epoch [63/200], Batch [30/107], D_loss: 0.0052, G_loss: 8.7015\n",
      "Epoch [63/200], Batch [40/107], D_loss: 0.0010, G_loss: 9.7322\n",
      "Epoch [63/200], Batch [50/107], D_loss: 0.0084, G_loss: 6.0248\n",
      "Epoch [63/200], Batch [60/107], D_loss: 0.0015, G_loss: 7.8765\n",
      "Epoch [63/200], Batch [70/107], D_loss: 0.0016, G_loss: 8.4878\n",
      "Epoch [63/200], Batch [80/107], D_loss: 0.0021, G_loss: 9.4669\n",
      "Epoch [63/200], Batch [90/107], D_loss: 0.0009, G_loss: 9.2447\n",
      "Epoch [63/200], Batch [100/107], D_loss: 0.0006, G_loss: 9.3394\n",
      "Epoch [63/200], D_loss: 0.0144, G_GAN_loss: 7.3834, G_L1_loss: 1.0836, G_perc_loss: 0.0378, G_total_loss: 8.5047, \n",
      "Epoch [64/200], Batch [0/107], D_loss: 0.0021, G_loss: 7.4782\n",
      "Epoch [64/200], Batch [10/107], D_loss: 0.0006, G_loss: 7.9952\n",
      "Epoch [64/200], Batch [20/107], D_loss: 0.0018, G_loss: 7.1099\n",
      "Epoch [64/200], Batch [30/107], D_loss: 0.0006, G_loss: 9.8502\n",
      "Epoch [64/200], Batch [40/107], D_loss: 0.0009, G_loss: 8.9842\n",
      "Epoch [64/200], Batch [50/107], D_loss: 0.0018, G_loss: 8.0778\n",
      "Epoch [64/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.7521\n",
      "Epoch [64/200], Batch [70/107], D_loss: 0.0011, G_loss: 9.4973\n",
      "Epoch [64/200], Batch [80/107], D_loss: 0.0007, G_loss: 9.5925\n",
      "Epoch [64/200], Batch [90/107], D_loss: 0.0007, G_loss: 9.5344\n",
      "Epoch [64/200], Batch [100/107], D_loss: 0.0013, G_loss: 9.6579\n",
      "Epoch [64/200], D_loss: 0.0020, G_GAN_loss: 7.6633, G_L1_loss: 1.0798, G_perc_loss: 0.0374, G_total_loss: 8.7805, \n",
      "Epoch [65/200], Batch [0/107], D_loss: 0.0026, G_loss: 6.9060\n",
      "Epoch [65/200], Batch [10/107], D_loss: 0.0008, G_loss: 10.0234\n",
      "Epoch [65/200], Batch [20/107], D_loss: 0.0010, G_loss: 8.6399\n",
      "Epoch [65/200], Batch [30/107], D_loss: 0.0026, G_loss: 7.8404\n",
      "Epoch [65/200], Batch [40/107], D_loss: 0.0008, G_loss: 7.9514\n",
      "Epoch [65/200], Batch [50/107], D_loss: 0.0013, G_loss: 9.9499\n",
      "Epoch [65/200], Batch [60/107], D_loss: 0.0032, G_loss: 10.1635\n",
      "Epoch [65/200], Batch [70/107], D_loss: 0.0007, G_loss: 9.9991\n",
      "Epoch [65/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.4586\n",
      "Epoch [65/200], Batch [90/107], D_loss: 0.0047, G_loss: 10.0262\n",
      "Epoch [65/200], Batch [100/107], D_loss: 0.0008, G_loss: 8.4170\n",
      "Epoch [65/200], D_loss: 0.0017, G_GAN_loss: 7.9457, G_L1_loss: 1.0791, G_perc_loss: 0.0374, G_total_loss: 9.0622, \n",
      "Epoch [66/200], Batch [0/107], D_loss: 0.0117, G_loss: 6.6133\n",
      "Epoch [66/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.7579\n",
      "Epoch [66/200], Batch [20/107], D_loss: 0.0019, G_loss: 9.9525\n",
      "Epoch [66/200], Batch [30/107], D_loss: 0.0004, G_loss: 9.0845\n",
      "Epoch [66/200], Batch [40/107], D_loss: 0.0007, G_loss: 8.5408\n",
      "Epoch [66/200], Batch [50/107], D_loss: 0.0004, G_loss: 8.7147\n",
      "Epoch [66/200], Batch [60/107], D_loss: 0.0006, G_loss: 8.3758\n",
      "Epoch [66/200], Batch [70/107], D_loss: 0.0005, G_loss: 9.8873\n",
      "Epoch [66/200], Batch [80/107], D_loss: 0.0005, G_loss: 7.8908\n",
      "Epoch [66/200], Batch [90/107], D_loss: 0.0040, G_loss: 7.0190\n",
      "Epoch [66/200], Batch [100/107], D_loss: 0.0006, G_loss: 9.2718\n",
      "Epoch [66/200], D_loss: 0.0009, G_GAN_loss: 8.2023, G_L1_loss: 1.0762, G_perc_loss: 0.0371, G_total_loss: 9.3156, \n",
      "Epoch [67/200], Batch [0/107], D_loss: 0.0005, G_loss: 10.5345\n",
      "Epoch [67/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.0583\n",
      "Epoch [67/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.3692\n",
      "Epoch [67/200], Batch [30/107], D_loss: 0.0008, G_loss: 10.0009\n",
      "Epoch [67/200], Batch [40/107], D_loss: 0.0025, G_loss: 10.6185\n",
      "Epoch [67/200], Batch [50/107], D_loss: 0.0014, G_loss: 10.6024\n",
      "Epoch [67/200], Batch [60/107], D_loss: 0.0011, G_loss: 7.9099\n",
      "Epoch [67/200], Batch [70/107], D_loss: 0.0030, G_loss: 7.1258\n",
      "Epoch [67/200], Batch [80/107], D_loss: 0.0003, G_loss: 9.5569\n",
      "Epoch [67/200], Batch [90/107], D_loss: 0.0008, G_loss: 8.0522\n",
      "Epoch [67/200], Batch [100/107], D_loss: 0.0004, G_loss: 9.7374\n",
      "Epoch [67/200], D_loss: 0.0012, G_GAN_loss: 8.0633, G_L1_loss: 1.0752, G_perc_loss: 0.0371, G_total_loss: 9.1757, \n",
      "Epoch [68/200], Batch [0/107], D_loss: 0.0048, G_loss: 7.1836\n",
      "Epoch [68/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.9291\n",
      "Epoch [68/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.4938\n",
      "Epoch [68/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.5172\n",
      "Epoch [68/200], Batch [40/107], D_loss: 0.0005, G_loss: 10.2845\n",
      "Epoch [68/200], Batch [50/107], D_loss: 0.0018, G_loss: 6.7494\n",
      "Epoch [68/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.6337\n",
      "Epoch [68/200], Batch [70/107], D_loss: 0.0005, G_loss: 8.9240\n",
      "Epoch [68/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.2843\n",
      "Epoch [68/200], Batch [90/107], D_loss: 0.0003, G_loss: 10.6150\n",
      "Epoch [68/200], Batch [100/107], D_loss: 0.0004, G_loss: 8.9435\n",
      "Epoch [68/200], D_loss: 0.0006, G_GAN_loss: 8.4801, G_L1_loss: 1.0800, G_perc_loss: 0.0378, G_total_loss: 9.5979, \n",
      "Epoch [69/200], Batch [0/107], D_loss: 0.0009, G_loss: 10.8998\n",
      "Epoch [69/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.1412\n",
      "Epoch [69/200], Batch [20/107], D_loss: 0.0007, G_loss: 8.6316\n",
      "Epoch [69/200], Batch [30/107], D_loss: 0.0004, G_loss: 9.8621\n",
      "Epoch [69/200], Batch [40/107], D_loss: 0.0005, G_loss: 8.2807\n",
      "Epoch [69/200], Batch [50/107], D_loss: 0.0003, G_loss: 10.0980\n",
      "Epoch [69/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.6085\n",
      "Epoch [69/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.8368\n",
      "Epoch [69/200], Batch [80/107], D_loss: 0.0003, G_loss: 10.3642\n",
      "Epoch [69/200], Batch [90/107], D_loss: 0.0010, G_loss: 8.0759\n",
      "Epoch [69/200], Batch [100/107], D_loss: 0.0009, G_loss: 7.1744\n",
      "Epoch [69/200], D_loss: 0.0007, G_GAN_loss: 8.6115, G_L1_loss: 1.0708, G_perc_loss: 0.0368, G_total_loss: 9.7191, \n",
      "Epoch [70/200], Batch [0/107], D_loss: 0.0008, G_loss: 8.8478\n",
      "Epoch [70/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.2507\n",
      "Epoch [70/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.4782\n",
      "Epoch [70/200], Batch [30/107], D_loss: 0.0002, G_loss: 9.9469\n",
      "Epoch [70/200], Batch [40/107], D_loss: 0.0008, G_loss: 10.3200\n",
      "Epoch [70/200], Batch [50/107], D_loss: 0.0007, G_loss: 8.0326\n",
      "Epoch [70/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.9927\n",
      "Epoch [70/200], Batch [70/107], D_loss: 0.0060, G_loss: 9.7997\n",
      "Epoch [70/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.6460\n",
      "Epoch [70/200], Batch [90/107], D_loss: 0.0005, G_loss: 9.8933\n",
      "Epoch [70/200], Batch [100/107], D_loss: 0.0004, G_loss: 9.6855\n",
      "Epoch [70/200], D_loss: 0.0008, G_GAN_loss: 8.6118, G_L1_loss: 1.0680, G_perc_loss: 0.0366, G_total_loss: 9.7164, \n",
      "Epoch [71/200], Batch [0/107], D_loss: 0.0014, G_loss: 7.3758\n",
      "Epoch [71/200], Batch [10/107], D_loss: 0.0002, G_loss: 10.3877\n",
      "Epoch [71/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.2829\n",
      "Epoch [71/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.7085\n",
      "Epoch [71/200], Batch [40/107], D_loss: 0.0007, G_loss: 10.4501\n",
      "Epoch [71/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.4508\n",
      "Epoch [71/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.5524\n",
      "Epoch [71/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.6166\n",
      "Epoch [71/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.4304\n",
      "Epoch [71/200], Batch [90/107], D_loss: 0.0005, G_loss: 10.6208\n",
      "Epoch [71/200], Batch [100/107], D_loss: 0.0001, G_loss: 10.2378\n",
      "Epoch [71/200], D_loss: 0.0004, G_GAN_loss: 8.8914, G_L1_loss: 1.0655, G_perc_loss: 0.0365, G_total_loss: 9.9934, \n",
      "Epoch [72/200], Batch [0/107], D_loss: 0.0003, G_loss: 11.3245\n",
      "Epoch [72/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.2858\n",
      "Epoch [72/200], Batch [20/107], D_loss: 0.0003, G_loss: 9.6643\n",
      "Epoch [72/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.2915\n",
      "Epoch [72/200], Batch [40/107], D_loss: 0.0013, G_loss: 11.0427\n",
      "Epoch [72/200], Batch [50/107], D_loss: 0.0029, G_loss: 6.6029\n",
      "Epoch [72/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.5396\n",
      "Epoch [72/200], Batch [70/107], D_loss: 0.0005, G_loss: 10.7592\n",
      "Epoch [72/200], Batch [80/107], D_loss: 0.0001, G_loss: 10.5024\n",
      "Epoch [72/200], Batch [90/107], D_loss: 0.0001, G_loss: 9.9822\n",
      "Epoch [72/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.2959\n",
      "Epoch [72/200], D_loss: 0.0006, G_GAN_loss: 8.8322, G_L1_loss: 1.0640, G_perc_loss: 0.0364, G_total_loss: 9.9326, \n",
      "Epoch [73/200], Batch [0/107], D_loss: 0.0013, G_loss: 8.4593\n",
      "Epoch [73/200], Batch [10/107], D_loss: 0.0002, G_loss: 9.7971\n",
      "Epoch [73/200], Batch [20/107], D_loss: 0.0007, G_loss: 10.5817\n",
      "Epoch [73/200], Batch [30/107], D_loss: 0.0002, G_loss: 10.4144\n",
      "Epoch [73/200], Batch [40/107], D_loss: 0.0005, G_loss: 8.2606\n",
      "Epoch [73/200], Batch [50/107], D_loss: 0.0005, G_loss: 10.5189\n",
      "Epoch [73/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.3921\n",
      "Epoch [73/200], Batch [70/107], D_loss: 0.0002, G_loss: 9.7616\n",
      "Epoch [73/200], Batch [80/107], D_loss: 0.0006, G_loss: 9.3336\n",
      "Epoch [73/200], Batch [90/107], D_loss: 0.0006, G_loss: 9.4140\n",
      "Epoch [73/200], Batch [100/107], D_loss: 0.0002, G_loss: 10.9094\n",
      "Epoch [73/200], D_loss: 0.0004, G_GAN_loss: 9.0593, G_L1_loss: 1.0670, G_perc_loss: 0.0367, G_total_loss: 10.1629, \n",
      "Epoch [74/200], Batch [0/107], D_loss: 0.0003, G_loss: 10.2054\n",
      "Epoch [74/200], Batch [10/107], D_loss: 0.0025, G_loss: 8.0719\n",
      "Epoch [74/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.9806\n",
      "Epoch [74/200], Batch [30/107], D_loss: 0.0009, G_loss: 11.4401\n",
      "Epoch [74/200], Batch [40/107], D_loss: 0.0003, G_loss: 11.1553\n",
      "Epoch [74/200], Batch [50/107], D_loss: 0.0003, G_loss: 10.8612\n",
      "Epoch [74/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.4596\n",
      "Epoch [74/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.8102\n",
      "Epoch [74/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.2068\n",
      "Epoch [74/200], Batch [90/107], D_loss: 0.0004, G_loss: 10.5658\n",
      "Epoch [74/200], Batch [100/107], D_loss: 0.0002, G_loss: 10.7761\n",
      "Epoch [74/200], D_loss: 0.0004, G_GAN_loss: 9.0163, G_L1_loss: 1.0597, G_perc_loss: 0.0361, G_total_loss: 10.1122, \n",
      "Epoch [75/200], Batch [0/107], D_loss: 0.0003, G_loss: 8.8593\n",
      "Epoch [75/200], Batch [10/107], D_loss: 0.0007, G_loss: 7.6651\n",
      "Epoch [75/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.7132\n",
      "Epoch [75/200], Batch [30/107], D_loss: 0.0004, G_loss: 10.1707\n",
      "Epoch [75/200], Batch [40/107], D_loss: 0.0004, G_loss: 9.4613\n",
      "Epoch [75/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.6915\n",
      "Epoch [75/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.1622\n",
      "Epoch [75/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.0002\n",
      "Epoch [75/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.2518\n",
      "Epoch [75/200], Batch [90/107], D_loss: 0.0009, G_loss: 7.6881\n",
      "Epoch [75/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.0108\n",
      "Epoch [75/200], D_loss: 0.0004, G_GAN_loss: 9.0205, G_L1_loss: 1.0582, G_perc_loss: 0.0360, G_total_loss: 10.1147, \n",
      "Epoch [76/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.9822\n",
      "Epoch [76/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.5036\n",
      "Epoch [76/200], Batch [20/107], D_loss: 0.0040, G_loss: 7.7805\n",
      "Epoch [76/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.3089\n",
      "Epoch [76/200], Batch [40/107], D_loss: 0.0002, G_loss: 9.8237\n",
      "Epoch [76/200], Batch [50/107], D_loss: 0.0002, G_loss: 9.3835\n",
      "Epoch [76/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.2994\n",
      "Epoch [76/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.7545\n",
      "Epoch [76/200], Batch [80/107], D_loss: 0.0009, G_loss: 11.7011\n",
      "Epoch [76/200], Batch [90/107], D_loss: 0.0008, G_loss: 8.7684\n",
      "Epoch [76/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.0690\n",
      "Epoch [76/200], D_loss: 0.0004, G_GAN_loss: 9.0741, G_L1_loss: 1.0561, G_perc_loss: 0.0359, G_total_loss: 10.1661, \n",
      "Epoch [77/200], Batch [0/107], D_loss: 0.0002, G_loss: 9.7160\n",
      "Epoch [77/200], Batch [10/107], D_loss: 0.0006, G_loss: 11.1928\n",
      "Epoch [77/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.1161\n",
      "Epoch [77/200], Batch [30/107], D_loss: 0.0002, G_loss: 11.8527\n",
      "Epoch [77/200], Batch [40/107], D_loss: 0.0004, G_loss: 11.0561\n",
      "Epoch [77/200], Batch [50/107], D_loss: 0.0001, G_loss: 11.2143\n",
      "Epoch [77/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.2140\n",
      "Epoch [77/200], Batch [70/107], D_loss: 0.0005, G_loss: 9.5918\n",
      "Epoch [77/200], Batch [80/107], D_loss: 0.0008, G_loss: 11.6637\n",
      "Epoch [77/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.0843\n",
      "Epoch [77/200], Batch [100/107], D_loss: 0.0002, G_loss: 10.4169\n",
      "Epoch [77/200], D_loss: 0.0003, G_GAN_loss: 9.4718, G_L1_loss: 1.0623, G_perc_loss: 0.0371, G_total_loss: 10.5713, \n",
      "Epoch [78/200], Batch [0/107], D_loss: 0.0003, G_loss: 9.7404\n",
      "Epoch [78/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.7159\n",
      "Epoch [78/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.6515\n",
      "Epoch [78/200], Batch [30/107], D_loss: 0.0002, G_loss: 8.4632\n",
      "Epoch [78/200], Batch [40/107], D_loss: 0.0002, G_loss: 11.3931\n",
      "Epoch [78/200], Batch [50/107], D_loss: 0.0005, G_loss: 7.9513\n",
      "Epoch [78/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.5465\n",
      "Epoch [78/200], Batch [70/107], D_loss: 0.0013, G_loss: 8.7669\n",
      "Epoch [78/200], Batch [80/107], D_loss: 0.0002, G_loss: 11.1217\n",
      "Epoch [78/200], Batch [90/107], D_loss: 0.0005, G_loss: 7.9650\n",
      "Epoch [78/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.5176\n",
      "Epoch [78/200], D_loss: 0.0003, G_GAN_loss: 9.3331, G_L1_loss: 1.0606, G_perc_loss: 0.0366, G_total_loss: 10.4302, \n",
      "Epoch [79/200], Batch [0/107], D_loss: 0.0003, G_loss: 11.2211\n",
      "Epoch [79/200], Batch [10/107], D_loss: 0.0002, G_loss: 11.3618\n",
      "Epoch [79/200], Batch [20/107], D_loss: 0.0011, G_loss: 9.0108\n",
      "Epoch [79/200], Batch [30/107], D_loss: 0.0002, G_loss: 11.6894\n",
      "Epoch [79/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.8904\n",
      "Epoch [79/200], Batch [50/107], D_loss: 0.0003, G_loss: 10.4834\n",
      "Epoch [79/200], Batch [60/107], D_loss: 0.0001, G_loss: 10.6630\n",
      "Epoch [79/200], Batch [70/107], D_loss: 0.0000, G_loss: 11.4405\n",
      "Epoch [79/200], Batch [80/107], D_loss: 0.0002, G_loss: 11.2401\n",
      "Epoch [79/200], Batch [90/107], D_loss: 0.0002, G_loss: 11.7373\n",
      "Epoch [79/200], Batch [100/107], D_loss: 0.0001, G_loss: 10.6184\n",
      "Epoch [79/200], D_loss: 0.0003, G_GAN_loss: 9.4780, G_L1_loss: 1.0537, G_perc_loss: 0.0357, G_total_loss: 10.5674, \n",
      "Epoch [80/200], Batch [0/107], D_loss: 0.0002, G_loss: 11.5597\n",
      "Epoch [80/200], Batch [10/107], D_loss: 0.0002, G_loss: 11.1642\n",
      "Epoch [80/200], Batch [20/107], D_loss: 0.0005, G_loss: 9.3572\n",
      "Epoch [80/200], Batch [30/107], D_loss: 0.0002, G_loss: 11.6463\n",
      "Epoch [80/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.0745\n",
      "Epoch [80/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.0689\n",
      "Epoch [80/200], Batch [60/107], D_loss: 0.0004, G_loss: 8.9812\n",
      "Epoch [80/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.8952\n",
      "Epoch [80/200], Batch [80/107], D_loss: 0.0013, G_loss: 8.1192\n",
      "Epoch [80/200], Batch [90/107], D_loss: 0.0002, G_loss: 11.3134\n",
      "Epoch [80/200], Batch [100/107], D_loss: 0.0002, G_loss: 11.7836\n",
      "Epoch [80/200], D_loss: 0.0003, G_GAN_loss: 9.5237, G_L1_loss: 1.0504, G_perc_loss: 0.0355, G_total_loss: 10.6097, \n",
      "Epoch [81/200], Batch [0/107], D_loss: 0.0005, G_loss: 8.7586\n",
      "Epoch [81/200], Batch [10/107], D_loss: 0.0006, G_loss: 11.1102\n",
      "Epoch [81/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.2754\n",
      "Epoch [81/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.5859\n",
      "Epoch [81/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.2423\n",
      "Epoch [81/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.8617\n",
      "Epoch [81/200], Batch [60/107], D_loss: 0.0004, G_loss: 10.6900\n",
      "Epoch [81/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.5872\n",
      "Epoch [81/200], Batch [80/107], D_loss: 0.0002, G_loss: 11.8587\n",
      "Epoch [81/200], Batch [90/107], D_loss: 0.0001, G_loss: 12.0995\n",
      "Epoch [81/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.8081\n",
      "Epoch [81/200], D_loss: 0.0004, G_GAN_loss: 9.5935, G_L1_loss: 1.0495, G_perc_loss: 0.0354, G_total_loss: 10.6785, \n",
      "Epoch [82/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.7628\n",
      "Epoch [82/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.6656\n",
      "Epoch [82/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.2479\n",
      "Epoch [82/200], Batch [30/107], D_loss: 0.0001, G_loss: 10.5837\n",
      "Epoch [82/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.8168\n",
      "Epoch [82/200], Batch [50/107], D_loss: 0.0002, G_loss: 9.2722\n",
      "Epoch [82/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.0728\n",
      "Epoch [82/200], Batch [70/107], D_loss: 0.0020, G_loss: 9.3613\n",
      "Epoch [82/200], Batch [80/107], D_loss: 0.0004, G_loss: 11.6398\n",
      "Epoch [82/200], Batch [90/107], D_loss: 0.0002, G_loss: 9.1883\n",
      "Epoch [82/200], Batch [100/107], D_loss: 0.0002, G_loss: 11.8967\n",
      "Epoch [82/200], D_loss: 0.0003, G_GAN_loss: 9.6196, G_L1_loss: 1.0488, G_perc_loss: 0.0354, G_total_loss: 10.7038, \n",
      "Epoch [83/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.5784\n",
      "Epoch [83/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.9454\n",
      "Epoch [83/200], Batch [20/107], D_loss: 0.0001, G_loss: 11.3158\n",
      "Epoch [83/200], Batch [30/107], D_loss: 0.0002, G_loss: 11.2977\n",
      "Epoch [83/200], Batch [40/107], D_loss: 0.0002, G_loss: 11.1276\n",
      "Epoch [83/200], Batch [50/107], D_loss: 0.0004, G_loss: 11.6617\n",
      "Epoch [83/200], Batch [60/107], D_loss: 0.0001, G_loss: 12.0581\n",
      "Epoch [83/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.7164\n",
      "Epoch [83/200], Batch [80/107], D_loss: 0.0002, G_loss: 9.6068\n",
      "Epoch [83/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.1574\n",
      "Epoch [83/200], Batch [100/107], D_loss: 0.0002, G_loss: 11.6533\n",
      "Epoch [83/200], D_loss: 0.0002, G_GAN_loss: 9.6863, G_L1_loss: 1.0460, G_perc_loss: 0.0352, G_total_loss: 10.7676, \n",
      "Epoch [84/200], Batch [0/107], D_loss: 0.0006, G_loss: 8.1393\n",
      "Epoch [84/200], Batch [10/107], D_loss: 0.0010, G_loss: 7.1153\n",
      "Epoch [84/200], Batch [20/107], D_loss: 0.0001, G_loss: 11.1597\n",
      "Epoch [84/200], Batch [30/107], D_loss: 0.0001, G_loss: 9.6189\n",
      "Epoch [84/200], Batch [40/107], D_loss: 0.0004, G_loss: 11.8463\n",
      "Epoch [84/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.3341\n",
      "Epoch [84/200], Batch [60/107], D_loss: 0.0001, G_loss: 10.7974\n",
      "Epoch [84/200], Batch [70/107], D_loss: 0.0002, G_loss: 11.8354\n",
      "Epoch [84/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.6667\n",
      "Epoch [84/200], Batch [90/107], D_loss: 0.0005, G_loss: 9.0623\n",
      "Epoch [84/200], Batch [100/107], D_loss: 0.0002, G_loss: 11.7757\n",
      "Epoch [84/200], D_loss: 0.0002, G_GAN_loss: 9.8472, G_L1_loss: 1.0454, G_perc_loss: 0.0352, G_total_loss: 10.9278, \n",
      "Epoch [85/200], Batch [0/107], D_loss: 0.0002, G_loss: 11.7146\n",
      "Epoch [85/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.2817\n",
      "Epoch [85/200], Batch [20/107], D_loss: 0.0001, G_loss: 11.6523\n",
      "Epoch [85/200], Batch [30/107], D_loss: 0.0019, G_loss: 11.9816\n",
      "Epoch [85/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.0674\n",
      "Epoch [85/200], Batch [50/107], D_loss: 0.0001, G_loss: 11.6720\n",
      "Epoch [85/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.2713\n",
      "Epoch [85/200], Batch [70/107], D_loss: 0.0012, G_loss: 12.4843\n",
      "Epoch [85/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.4488\n",
      "Epoch [85/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.9877\n",
      "Epoch [85/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.6563\n",
      "Epoch [85/200], D_loss: 0.0002, G_GAN_loss: 9.9678, G_L1_loss: 1.0440, G_perc_loss: 0.0350, G_total_loss: 11.0469, \n",
      "Epoch [86/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.0552\n",
      "Epoch [86/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.6002\n",
      "Epoch [86/200], Batch [20/107], D_loss: 0.0000, G_loss: 12.1707\n",
      "Epoch [86/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.8720\n",
      "Epoch [86/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.0262\n",
      "Epoch [86/200], Batch [50/107], D_loss: 0.0001, G_loss: 11.1720\n",
      "Epoch [86/200], Batch [60/107], D_loss: 0.0001, G_loss: 10.9877\n",
      "Epoch [86/200], Batch [70/107], D_loss: 0.0004, G_loss: 10.1883\n",
      "Epoch [86/200], Batch [80/107], D_loss: 0.0012, G_loss: 11.6566\n",
      "Epoch [86/200], Batch [90/107], D_loss: 3.0280, G_loss: 9.0521\n",
      "Epoch [86/200], Batch [100/107], D_loss: 2.0225, G_loss: 2.4135\n",
      "Epoch [86/200], D_loss: 0.4976, G_GAN_loss: 8.9284, G_L1_loss: 1.0412, G_perc_loss: 0.0349, G_total_loss: 10.0046, \n",
      "Epoch [87/200], Batch [0/107], D_loss: 0.7704, G_loss: 3.6725\n",
      "Epoch [87/200], Batch [10/107], D_loss: 0.5739, G_loss: 2.9918\n",
      "Epoch [87/200], Batch [20/107], D_loss: 0.1094, G_loss: 3.3845\n",
      "Epoch [87/200], Batch [30/107], D_loss: 0.0677, G_loss: 4.6741\n",
      "Epoch [87/200], Batch [40/107], D_loss: 0.3447, G_loss: 3.2227\n",
      "Epoch [87/200], Batch [50/107], D_loss: 0.0344, G_loss: 5.5969\n",
      "Epoch [87/200], Batch [60/107], D_loss: 0.0266, G_loss: 5.7262\n",
      "Epoch [87/200], Batch [70/107], D_loss: 0.0646, G_loss: 5.2943\n",
      "Epoch [87/200], Batch [80/107], D_loss: 0.1646, G_loss: 6.1661\n",
      "Epoch [87/200], Batch [90/107], D_loss: 0.0222, G_loss: 6.0429\n",
      "Epoch [87/200], Batch [100/107], D_loss: 0.0551, G_loss: 4.7506\n",
      "Epoch [87/200], D_loss: 0.1636, G_GAN_loss: 3.6648, G_L1_loss: 1.0424, G_perc_loss: 0.0357, G_total_loss: 4.7429, \n",
      "Epoch [88/200], Batch [0/107], D_loss: 0.0129, G_loss: 5.8164\n",
      "Epoch [88/200], Batch [10/107], D_loss: 0.0221, G_loss: 5.8866\n",
      "Epoch [88/200], Batch [20/107], D_loss: 0.0263, G_loss: 5.2951\n",
      "Epoch [88/200], Batch [30/107], D_loss: 0.0269, G_loss: 4.8582\n",
      "Epoch [88/200], Batch [40/107], D_loss: 0.0122, G_loss: 4.7057\n",
      "Epoch [88/200], Batch [50/107], D_loss: 0.0108, G_loss: 5.9386\n",
      "Epoch [88/200], Batch [60/107], D_loss: 0.0770, G_loss: 4.6519\n",
      "Epoch [88/200], Batch [70/107], D_loss: 0.0501, G_loss: 8.1850\n",
      "Epoch [88/200], Batch [80/107], D_loss: 0.0132, G_loss: 6.0214\n",
      "Epoch [88/200], Batch [90/107], D_loss: 0.0312, G_loss: 5.4314\n",
      "Epoch [88/200], Batch [100/107], D_loss: 0.0080, G_loss: 6.7368\n",
      "Epoch [88/200], D_loss: 0.0375, G_GAN_loss: 5.0318, G_L1_loss: 1.0394, G_perc_loss: 0.0349, G_total_loss: 6.1060, \n",
      "Epoch [89/200], Batch [0/107], D_loss: 0.0065, G_loss: 6.7744\n",
      "Epoch [89/200], Batch [10/107], D_loss: 0.0116, G_loss: 7.7691\n",
      "Epoch [89/200], Batch [20/107], D_loss: 0.0255, G_loss: 7.0460\n",
      "Epoch [89/200], Batch [100/107], D_loss: 0.0069, G_loss: 8.4347\n",
      "Epoch [89/200], D_loss: 0.0294, G_GAN_loss: 5.6893, G_L1_loss: 1.0391, G_perc_loss: 0.0350, G_total_loss: 6.7634, \n",
      "Epoch [90/200], Batch [0/107], D_loss: 0.0220, G_loss: 5.2063\n",
      "Epoch [90/200], Batch [10/107], D_loss: 0.0083, G_loss: 6.1289\n",
      "Epoch [90/200], Batch [20/107], D_loss: 0.0056, G_loss: 4.9338\n",
      "Epoch [90/200], Batch [30/107], D_loss: 0.0035, G_loss: 7.9433\n",
      "Epoch [90/200], Batch [40/107], D_loss: 0.0041, G_loss: 8.3858\n",
      "Epoch [91/200], Batch [20/107], D_loss: 0.1969, G_loss: 7.7771\n",
      "Epoch [91/200], Batch [30/107], D_loss: 0.0060, G_loss: 6.5791\n",
      "Epoch [91/200], Batch [40/107], D_loss: 0.0053, G_loss: 7.3164\n",
      "Epoch [91/200], Batch [50/107], D_loss: 0.0080, G_loss: 8.4568\n",
      "Epoch [91/200], Batch [60/107], D_loss: 0.0029, G_loss: 6.2216\n",
      "Epoch [91/200], Batch [70/107], D_loss: 0.0028, G_loss: 7.2595\n",
      "Epoch [91/200], Batch [80/107], D_loss: 0.0023, G_loss: 7.3895\n",
      "Epoch [91/200], Batch [90/107], D_loss: 0.0033, G_loss: 6.9780\n",
      "Epoch [91/200], Batch [100/107], D_loss: 0.0092, G_loss: 8.4986\n",
      "Epoch [91/200], D_loss: 0.0202, G_GAN_loss: 6.2685, G_L1_loss: 1.0365, G_perc_loss: 0.0346, G_total_loss: 7.3397, \n",
      "Epoch [92/200], Batch [0/107], D_loss: 0.0048, G_loss: 6.7414\n",
      "Epoch [92/200], Batch [10/107], D_loss: 0.0018, G_loss: 7.5443\n",
      "Epoch [92/200], Batch [20/107], D_loss: 0.0034, G_loss: 7.5388\n",
      "Epoch [92/200], Batch [30/107], D_loss: 0.0962, G_loss: 6.6338\n",
      "Epoch [92/200], Batch [40/107], D_loss: 0.0099, G_loss: 7.6576\n",
      "Epoch [92/200], Batch [50/107], D_loss: 0.0045, G_loss: 8.5729\n",
      "Epoch [92/200], Batch [60/107], D_loss: 0.0103, G_loss: 8.7700\n",
      "Epoch [92/200], Batch [70/107], D_loss: 0.0104, G_loss: 6.0524\n",
      "Epoch [92/200], Batch [80/107], D_loss: 0.0041, G_loss: 7.3463\n",
      "Epoch [92/200], Batch [90/107], D_loss: 0.0022, G_loss: 7.2739\n",
      "Epoch [92/200], Batch [100/107], D_loss: 0.0042, G_loss: 6.6965\n",
      "Epoch [92/200], D_loss: 0.0211, G_GAN_loss: 6.5223, G_L1_loss: 1.0345, G_perc_loss: 0.0345, G_total_loss: 7.5913, \n",
      "Epoch [93/200], Batch [0/107], D_loss: 0.0024, G_loss: 8.7559\n",
      "Epoch [93/200], Batch [10/107], D_loss: 0.0014, G_loss: 7.8006\n",
      "Epoch [93/200], Batch [20/107], D_loss: 0.0043, G_loss: 9.8379\n",
      "Epoch [93/200], Batch [30/107], D_loss: 0.0020, G_loss: 7.6296\n",
      "Epoch [93/200], Batch [40/107], D_loss: 0.0018, G_loss: 8.6725\n",
      "Epoch [93/200], Batch [50/107], D_loss: 0.0026, G_loss: 6.8178\n",
      "Epoch [93/200], Batch [60/107], D_loss: 0.0028, G_loss: 8.0759\n",
      "Epoch [93/200], Batch [70/107], D_loss: 0.0036, G_loss: 9.1263\n",
      "Epoch [93/200], Batch [80/107], D_loss: 0.0011, G_loss: 8.5623\n",
      "Epoch [93/200], Batch [90/107], D_loss: 0.0022, G_loss: 8.2083\n",
      "Epoch [93/200], Batch [100/107], D_loss: 0.0046, G_loss: 8.9929\n",
      "Epoch [93/200], D_loss: 0.0033, G_GAN_loss: 7.1321, G_L1_loss: 1.0329, G_perc_loss: 0.0344, G_total_loss: 8.1994, \n",
      "Epoch [94/200], Batch [0/107], D_loss: 0.0022, G_loss: 7.1413\n",
      "Epoch [94/200], Batch [10/107], D_loss: 0.0017, G_loss: 7.3781\n",
      "Epoch [94/200], Batch [20/107], D_loss: 0.0008, G_loss: 8.1459\n",
      "Epoch [94/200], Batch [30/107], D_loss: 0.0026, G_loss: 6.9805\n",
      "Epoch [94/200], Batch [40/107], D_loss: 0.0025, G_loss: 8.4316\n",
      "Epoch [94/200], Batch [50/107], D_loss: 0.0016, G_loss: 9.6962\n",
      "Epoch [94/200], Batch [60/107], D_loss: 0.0009, G_loss: 9.3242\n",
      "Epoch [94/200], Batch [70/107], D_loss: 0.0012, G_loss: 8.6465\n",
      "Epoch [94/200], Batch [80/107], D_loss: 0.0016, G_loss: 9.7682\n",
      "Epoch [94/200], Batch [90/107], D_loss: 0.0013, G_loss: 7.9840\n",
      "Epoch [94/200], Batch [100/107], D_loss: 0.0043, G_loss: 6.7266\n",
      "Epoch [94/200], D_loss: 0.0029, G_GAN_loss: 7.4821, G_L1_loss: 1.0330, G_perc_loss: 0.0344, G_total_loss: 8.5495, \n",
      "Epoch [95/200], Batch [0/107], D_loss: 0.0008, G_loss: 8.5952\n",
      "Epoch [95/200], Batch [10/107], D_loss: 0.0027, G_loss: 6.5102\n",
      "Epoch [95/200], Batch [20/107], D_loss: 0.0053, G_loss: 6.0283\n",
      "Epoch [95/200], Batch [30/107], D_loss: 0.0159, G_loss: 6.8569\n",
      "Epoch [95/200], Batch [40/107], D_loss: 0.0012, G_loss: 8.9676\n",
      "Epoch [95/200], Batch [50/107], D_loss: 0.0087, G_loss: 9.4081\n",
      "Epoch [95/200], Batch [60/107], D_loss: 0.0028, G_loss: 6.9396\n",
      "Epoch [95/200], Batch [70/107], D_loss: 0.0016, G_loss: 8.1132\n",
      "Epoch [95/200], Batch [80/107], D_loss: 0.0016, G_loss: 7.7176\n",
      "Epoch [95/200], Batch [90/107], D_loss: 0.0020, G_loss: 8.5367\n",
      "Epoch [95/200], Batch [100/107], D_loss: 0.0029, G_loss: 8.5406\n",
      "Epoch [95/200], D_loss: 0.0055, G_GAN_loss: 7.2921, G_L1_loss: 1.0303, G_perc_loss: 0.0343, G_total_loss: 8.3566, \n",
      "Epoch [96/200], Batch [0/107], D_loss: 0.0012, G_loss: 8.2859\n",
      "Epoch [96/200], Batch [10/107], D_loss: 0.0008, G_loss: 8.6376\n",
      "Epoch [96/200], Batch [20/107], D_loss: 0.0009, G_loss: 8.7309\n",
      "Epoch [96/200], Batch [30/107], D_loss: 0.0068, G_loss: 7.2029\n",
      "Epoch [96/200], Batch [40/107], D_loss: 0.0051, G_loss: 6.7379\n",
      "Epoch [96/200], Batch [50/107], D_loss: 0.0058, G_loss: 8.8249\n",
      "Epoch [96/200], Batch [60/107], D_loss: 0.0017, G_loss: 9.9912\n",
      "Epoch [96/200], Batch [70/107], D_loss: 0.4132, G_loss: 8.1853\n",
      "Epoch [96/200], Batch [80/107], D_loss: 0.4552, G_loss: 2.1462\n",
      "Epoch [96/200], Batch [90/107], D_loss: 0.1382, G_loss: 4.6493\n",
      "Epoch [96/200], Batch [100/107], D_loss: 0.0724, G_loss: 5.2396\n",
      "Epoch [96/200], D_loss: 0.2230, G_GAN_loss: 6.2484, G_L1_loss: 1.0272, G_perc_loss: 0.0343, G_total_loss: 7.3099, \n",
      "Epoch [97/200], Batch [0/107], D_loss: 0.5711, G_loss: 5.4747\n",
      "Epoch [97/200], Batch [10/107], D_loss: 0.0136, G_loss: 5.6674\n",
      "Epoch [97/200], Batch [20/107], D_loss: 0.0176, G_loss: 7.8500\n",
      "Epoch [97/200], Batch [30/107], D_loss: 0.0355, G_loss: 5.9305\n",
      "Epoch [97/200], Batch [40/107], D_loss: 0.0079, G_loss: 6.2849\n",
      "Epoch [97/200], Batch [50/107], D_loss: 0.0042, G_loss: 6.7534\n",
      "Epoch [97/200], Batch [60/107], D_loss: 0.0246, G_loss: 7.8708\n",
      "Epoch [97/200], Batch [70/107], D_loss: 0.0094, G_loss: 7.8846\n",
      "Epoch [97/200], Batch [80/107], D_loss: 0.0205, G_loss: 6.4884\n",
      "Epoch [97/200], Batch [90/107], D_loss: 0.0097, G_loss: 6.7198\n",
      "Epoch [97/200], Batch [100/107], D_loss: 0.0022, G_loss: 8.2225\n",
      "Epoch [97/200], D_loss: 0.0206, G_GAN_loss: 6.0415, G_L1_loss: 1.0300, G_perc_loss: 0.0347, G_total_loss: 7.1061, \n",
      "Epoch [98/200], Batch [0/107], D_loss: 0.0022, G_loss: 7.7784\n",
      "Epoch [98/200], Batch [10/107], D_loss: 0.0032, G_loss: 8.1004\n",
      "Epoch [98/200], Batch [20/107], D_loss: 0.0041, G_loss: 7.7682\n",
      "Epoch [98/200], Batch [30/107], D_loss: 0.0048, G_loss: 7.3594\n",
      "Epoch [98/200], Batch [40/107], D_loss: 0.0033, G_loss: 8.3456\n",
      "Epoch [98/200], Batch [50/107], D_loss: 0.3793, G_loss: 5.2287\n",
      "Epoch [98/200], Batch [60/107], D_loss: 0.0234, G_loss: 7.8067\n",
      "Epoch [98/200], Batch [70/107], D_loss: 0.0292, G_loss: 6.2631\n",
      "Epoch [98/200], Batch [80/107], D_loss: 0.0058, G_loss: 8.4590\n",
      "Epoch [98/200], Batch [90/107], D_loss: 0.0042, G_loss: 7.7032\n",
      "Epoch [98/200], Batch [100/107], D_loss: 0.0322, G_loss: 7.0959\n",
      "Epoch [98/200], D_loss: 0.0529, G_GAN_loss: 6.3113, G_L1_loss: 1.0320, G_perc_loss: 0.0346, G_total_loss: 7.3779, \n",
      "Epoch [99/200], Batch [0/107], D_loss: 0.0037, G_loss: 7.2215\n",
      "Epoch [99/200], Batch [10/107], D_loss: 0.0037, G_loss: 6.8714\n",
      "Epoch [99/200], Batch [20/107], D_loss: 0.0028, G_loss: 6.5390\n",
      "Epoch [99/200], Batch [30/107], D_loss: 0.0061, G_loss: 9.0925\n",
      "Epoch [99/200], Batch [40/107], D_loss: 0.0061, G_loss: 9.5943\n",
      "Epoch [99/200], Batch [50/107], D_loss: 0.0018, G_loss: 8.2275\n",
      "Epoch [99/200], Batch [60/107], D_loss: 0.0042, G_loss: 8.1052\n",
      "Epoch [99/200], Batch [70/107], D_loss: 0.0128, G_loss: 9.4336\n",
      "Epoch [99/200], Batch [80/107], D_loss: 0.0054, G_loss: 9.2659\n",
      "Epoch [99/200], Batch [90/107], D_loss: 0.0022, G_loss: 9.3053\n",
      "Epoch [99/200], Batch [100/107], D_loss: 0.0030, G_loss: 8.9831\n",
      "Epoch [99/200], D_loss: 0.0054, G_GAN_loss: 6.8568, G_L1_loss: 1.0271, G_perc_loss: 0.0341, G_total_loss: 7.9180, \n",
      "Epoch [100/200], Batch [0/107], D_loss: 0.0028, G_loss: 6.5810\n",
      "Epoch [100/200], Batch [10/107], D_loss: 0.0020, G_loss: 8.8919\n",
      "Epoch [100/200], Batch [20/107], D_loss: 0.0024, G_loss: 7.7209\n",
      "Epoch [100/200], Batch [30/107], D_loss: 0.0008, G_loss: 8.8306\n",
      "Epoch [100/200], Batch [40/107], D_loss: 0.0030, G_loss: 7.9280\n",
      "Epoch [100/200], Batch [50/107], D_loss: 0.0009, G_loss: 8.5681\n",
      "Epoch [100/200], Batch [60/107], D_loss: 0.0020, G_loss: 9.6955\n",
      "Epoch [100/200], Batch [70/107], D_loss: 0.0061, G_loss: 10.0928\n",
      "Epoch [100/200], Batch [80/107], D_loss: 0.0038, G_loss: 6.9171\n",
      "Epoch [100/200], Batch [90/107], D_loss: 0.0012, G_loss: 7.7910\n",
      "Epoch [100/200], Batch [100/107], D_loss: 0.0015, G_loss: 8.4348\n",
      "Epoch [100/200], D_loss: 0.0042, G_GAN_loss: 6.9608, G_L1_loss: 1.0259, G_perc_loss: 0.0340, G_total_loss: 8.0207, \n",
      "Epoch [101/200], Batch [0/107], D_loss: 0.0079, G_loss: 6.0998\n",
      "Epoch [101/200], Batch [10/107], D_loss: 0.0017, G_loss: 9.1765\n",
      "Epoch [101/200], Batch [20/107], D_loss: 0.0013, G_loss: 8.2364\n",
      "Epoch [101/200], Batch [30/107], D_loss: 0.0015, G_loss: 6.8022\n",
      "Epoch [101/200], Batch [40/107], D_loss: 0.0011, G_loss: 9.1832\n",
      "Epoch [101/200], Batch [50/107], D_loss: 0.0019, G_loss: 8.1484\n",
      "Epoch [101/200], Batch [60/107], D_loss: 0.0040, G_loss: 9.1310\n",
      "Epoch [101/200], Batch [70/107], D_loss: 0.0031, G_loss: 9.9081\n",
      "Epoch [101/200], Batch [80/107], D_loss: 0.0020, G_loss: 9.6999\n",
      "Epoch [101/200], Batch [90/107], D_loss: 0.0325, G_loss: 9.5648\n",
      "Epoch [101/200], Batch [100/107], D_loss: 0.0035, G_loss: 7.0001\n",
      "Epoch [101/200], D_loss: 0.0112, G_GAN_loss: 7.2496, G_L1_loss: 1.0215, G_perc_loss: 0.0338, G_total_loss: 8.3050, \n",
      "Epoch [102/200], Batch [0/107], D_loss: 0.0440, G_loss: 9.1435\n",
      "Epoch [102/200], Batch [10/107], D_loss: 0.0036, G_loss: 7.3914\n",
      "Epoch [102/200], Batch [20/107], D_loss: 0.0042, G_loss: 9.6890\n",
      "Epoch [102/200], Batch [30/107], D_loss: 0.0121, G_loss: 8.9519\n",
      "Epoch [102/200], Batch [40/107], D_loss: 0.0013, G_loss: 8.2525\n",
      "Epoch [102/200], Batch [50/107], D_loss: 0.0016, G_loss: 8.1267\n",
      "Epoch [102/200], Batch [60/107], D_loss: 0.0018, G_loss: 8.7221\n",
      "Epoch [102/200], Batch [70/107], D_loss: 0.0007, G_loss: 9.2216\n",
      "Epoch [102/200], Batch [80/107], D_loss: 0.0020, G_loss: 9.2567\n",
      "Epoch [102/200], Batch [90/107], D_loss: 0.0010, G_loss: 8.8346\n",
      "Epoch [102/200], Batch [100/107], D_loss: 0.0006, G_loss: 9.2222\n",
      "Epoch [102/200], D_loss: 0.0089, G_GAN_loss: 7.4171, G_L1_loss: 1.0211, G_perc_loss: 0.0337, G_total_loss: 8.4719, \n",
      "Epoch [103/200], Batch [0/107], D_loss: 0.0035, G_loss: 6.3028\n",
      "Epoch [103/200], Batch [10/107], D_loss: 0.0014, G_loss: 9.5074\n",
      "Epoch [103/200], Batch [20/107], D_loss: 0.0010, G_loss: 9.6268\n",
      "Epoch [103/200], Batch [30/107], D_loss: 0.0008, G_loss: 8.7326\n",
      "Epoch [103/200], Batch [40/107], D_loss: 0.0021, G_loss: 10.3530\n",
      "Epoch [103/200], Batch [50/107], D_loss: 0.0068, G_loss: 7.8640\n",
      "Epoch [103/200], Batch [60/107], D_loss: 0.0015, G_loss: 10.2387\n",
      "Epoch [103/200], Batch [70/107], D_loss: 0.0019, G_loss: 9.4855\n",
      "Epoch [103/200], Batch [80/107], D_loss: 0.0036, G_loss: 8.6054\n",
      "Epoch [103/200], Batch [90/107], D_loss: 0.0007, G_loss: 9.2578\n",
      "Epoch [103/200], Batch [100/107], D_loss: 0.0008, G_loss: 9.5068\n",
      "Epoch [103/200], D_loss: 0.0055, G_GAN_loss: 7.5698, G_L1_loss: 1.0214, G_perc_loss: 0.0342, G_total_loss: 8.6254, \n",
      "Epoch [104/200], Batch [0/107], D_loss: 0.0011, G_loss: 9.4821\n",
      "Epoch [104/200], Batch [10/107], D_loss: 0.0007, G_loss: 10.1607\n",
      "Epoch [104/200], Batch [20/107], D_loss: 0.0006, G_loss: 9.2295\n",
      "Epoch [104/200], Batch [30/107], D_loss: 0.0166, G_loss: 10.0787\n",
      "Epoch [104/200], Batch [40/107], D_loss: 0.0089, G_loss: 6.6586\n",
      "Epoch [104/200], Batch [50/107], D_loss: 0.0045, G_loss: 6.4350\n",
      "Epoch [104/200], Batch [60/107], D_loss: 0.0012, G_loss: 8.4035\n",
      "Epoch [104/200], Batch [70/107], D_loss: 0.0008, G_loss: 9.1036\n",
      "Epoch [104/200], Batch [80/107], D_loss: 0.0014, G_loss: 9.5625\n",
      "Epoch [104/200], Batch [90/107], D_loss: 0.0013, G_loss: 8.8892\n",
      "Epoch [104/200], Batch [100/107], D_loss: 0.0012, G_loss: 9.0735\n",
      "Epoch [104/200], D_loss: 0.0086, G_GAN_loss: 7.4362, G_L1_loss: 1.0191, G_perc_loss: 0.0337, G_total_loss: 8.4890, \n",
      "Epoch [105/200], Batch [0/107], D_loss: 0.0005, G_loss: 9.0513\n",
      "Epoch [105/200], Batch [10/107], D_loss: 0.0016, G_loss: 8.0267\n",
      "Epoch [105/200], Batch [20/107], D_loss: 0.0011, G_loss: 7.8973\n",
      "Epoch [105/200], Batch [30/107], D_loss: 0.0013, G_loss: 7.2546\n",
      "Epoch [105/200], Batch [40/107], D_loss: 0.0425, G_loss: 9.6388\n",
      "Epoch [105/200], Batch [50/107], D_loss: 0.2148, G_loss: 9.3009\n",
      "Epoch [105/200], Batch [60/107], D_loss: 0.0006, G_loss: 9.8718\n",
      "Epoch [105/200], Batch [70/107], D_loss: 0.0005, G_loss: 8.9447\n",
      "Epoch [105/200], Batch [80/107], D_loss: 0.0048, G_loss: 9.3236\n",
      "Epoch [105/200], Batch [90/107], D_loss: 0.0011, G_loss: 10.2094\n",
      "Epoch [105/200], Batch [100/107], D_loss: 0.0021, G_loss: 8.2785\n",
      "Epoch [105/200], D_loss: 0.0103, G_GAN_loss: 7.8043, G_L1_loss: 1.0208, G_perc_loss: 0.0338, G_total_loss: 8.8588, \n",
      "Epoch [106/200], Batch [0/107], D_loss: 0.0120, G_loss: 8.6718\n",
      "Epoch [106/200], Batch [10/107], D_loss: 0.0013, G_loss: 9.7667\n",
      "Epoch [106/200], Batch [20/107], D_loss: 0.0010, G_loss: 9.4763\n",
      "Epoch [106/200], Batch [30/107], D_loss: 0.0016, G_loss: 6.2003\n",
      "Epoch [106/200], Batch [40/107], D_loss: 0.0014, G_loss: 10.3302\n",
      "Epoch [106/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.0809\n",
      "Epoch [106/200], Batch [60/107], D_loss: 0.0006, G_loss: 9.6786\n",
      "Epoch [106/200], Batch [70/107], D_loss: 0.0005, G_loss: 8.9512\n",
      "Epoch [106/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.2793\n",
      "Epoch [106/200], Batch [90/107], D_loss: 0.0004, G_loss: 10.3757\n",
      "Epoch [106/200], Batch [100/107], D_loss: 0.0004, G_loss: 10.2400\n",
      "Epoch [106/200], D_loss: 0.0022, G_GAN_loss: 8.2090, G_L1_loss: 1.0171, G_perc_loss: 0.0336, G_total_loss: 9.2597, \n",
      "Epoch [107/200], Batch [0/107], D_loss: 0.0012, G_loss: 8.3323\n",
      "Epoch [107/200], Batch [10/107], D_loss: 0.0031, G_loss: 10.8978\n",
      "Epoch [107/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.3317\n",
      "Epoch [107/200], Batch [30/107], D_loss: 0.0010, G_loss: 7.9796\n",
      "Epoch [107/200], Batch [40/107], D_loss: 0.0012, G_loss: 9.2174\n",
      "Epoch [107/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.4396\n",
      "Epoch [107/200], Batch [60/107], D_loss: 0.0010, G_loss: 9.3179\n",
      "Epoch [107/200], Batch [70/107], D_loss: 0.0013, G_loss: 11.0454\n",
      "Epoch [107/200], Batch [80/107], D_loss: 0.0063, G_loss: 6.7731\n",
      "Epoch [107/200], Batch [90/107], D_loss: 0.0537, G_loss: 6.0285\n",
      "Epoch [107/200], Batch [100/107], D_loss: 0.0043, G_loss: 7.1429\n",
      "Epoch [107/200], D_loss: 0.0084, G_GAN_loss: 8.3876, G_L1_loss: 1.0172, G_perc_loss: 0.0335, G_total_loss: 9.4383, \n",
      "Epoch [108/200], Batch [0/107], D_loss: 0.0006, G_loss: 9.4005\n",
      "Epoch [108/200], Batch [10/107], D_loss: 0.0018, G_loss: 11.2020\n",
      "Epoch [108/200], Batch [20/107], D_loss: 0.0014, G_loss: 9.3454\n",
      "Epoch [108/200], Batch [30/107], D_loss: 0.0009, G_loss: 10.4756\n",
      "Epoch [108/200], Batch [40/107], D_loss: 0.0011, G_loss: 8.9629\n",
      "Epoch [108/200], Batch [50/107], D_loss: 0.0017, G_loss: 9.6606\n",
      "Epoch [108/200], Batch [60/107], D_loss: 0.4603, G_loss: 5.5657\n",
      "Epoch [108/200], Batch [70/107], D_loss: 0.0125, G_loss: 9.9624\n",
      "Epoch [108/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.5749\n",
      "Epoch [108/200], Batch [90/107], D_loss: 0.1129, G_loss: 9.5476\n",
      "Epoch [108/200], Batch [100/107], D_loss: 0.0022, G_loss: 10.2228\n",
      "Epoch [108/200], D_loss: 0.0168, G_GAN_loss: 8.4040, G_L1_loss: 1.0184, G_perc_loss: 0.0335, G_total_loss: 9.4559, \n",
      "Epoch [109/200], Batch [0/107], D_loss: 0.0013, G_loss: 9.1032\n",
      "Epoch [109/200], Batch [10/107], D_loss: 0.0032, G_loss: 7.0296\n",
      "Epoch [109/200], Batch [20/107], D_loss: 0.0008, G_loss: 9.0336\n",
      "Epoch [109/200], Batch [30/107], D_loss: 0.0008, G_loss: 9.7757\n",
      "Epoch [109/200], Batch [40/107], D_loss: 0.0006, G_loss: 9.3441\n",
      "Epoch [109/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.9616\n",
      "Epoch [109/200], Batch [60/107], D_loss: 0.0227, G_loss: 6.9071\n",
      "Epoch [109/200], Batch [70/107], D_loss: 0.0003, G_loss: 10.2539\n",
      "Epoch [109/200], Batch [80/107], D_loss: 0.0010, G_loss: 8.9621\n",
      "Epoch [109/200], Batch [90/107], D_loss: 0.0005, G_loss: 9.8411\n",
      "Epoch [109/200], Batch [100/107], D_loss: 0.0005, G_loss: 8.6675\n",
      "Epoch [109/200], D_loss: 0.0018, G_GAN_loss: 8.4820, G_L1_loss: 1.0175, G_perc_loss: 0.0336, G_total_loss: 9.5331, \n",
      "Epoch [110/200], Batch [0/107], D_loss: 0.0006, G_loss: 9.9289\n",
      "Epoch [110/200], Batch [10/107], D_loss: 0.0011, G_loss: 7.7318\n",
      "Epoch [110/200], Batch [20/107], D_loss: 0.0010, G_loss: 8.8179\n",
      "Epoch [110/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.0380\n",
      "Epoch [110/200], Batch [40/107], D_loss: 0.0004, G_loss: 8.4991\n",
      "Epoch [110/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.9217\n",
      "Epoch [110/200], Batch [60/107], D_loss: 0.0008, G_loss: 8.6714\n",
      "Epoch [110/200], Batch [70/107], D_loss: 0.0004, G_loss: 10.3078\n",
      "Epoch [110/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.3066\n",
      "Epoch [110/200], Batch [90/107], D_loss: 0.0007, G_loss: 11.0895\n",
      "Epoch [110/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.5156\n",
      "Epoch [110/200], D_loss: 0.0010, G_GAN_loss: 8.7856, G_L1_loss: 1.0170, G_perc_loss: 0.0335, G_total_loss: 9.8361, \n",
      "Epoch [111/200], Batch [0/107], D_loss: 0.0010, G_loss: 10.7042\n",
      "Epoch [111/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.2199\n",
      "Epoch [111/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.0496\n",
      "Epoch [111/200], Batch [30/107], D_loss: 0.0008, G_loss: 11.6888\n",
      "Epoch [111/200], Batch [40/107], D_loss: 0.0011, G_loss: 7.8388\n",
      "Epoch [111/200], Batch [50/107], D_loss: 0.0009, G_loss: 8.8842\n",
      "Epoch [111/200], Batch [60/107], D_loss: 0.0023, G_loss: 7.0004\n",
      "Epoch [111/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.9703\n",
      "Epoch [111/200], Batch [80/107], D_loss: 0.0003, G_loss: 9.8241\n",
      "Epoch [111/200], Batch [90/107], D_loss: 0.0003, G_loss: 10.1682\n",
      "Epoch [111/200], Batch [100/107], D_loss: 0.0004, G_loss: 9.7131\n",
      "Epoch [111/200], D_loss: 0.0008, G_GAN_loss: 8.7780, G_L1_loss: 1.0146, G_perc_loss: 0.0333, G_total_loss: 9.8259, \n",
      "Epoch [112/200], Batch [0/107], D_loss: 0.0003, G_loss: 9.0989\n",
      "Epoch [112/200], Batch [10/107], D_loss: 0.0004, G_loss: 10.8291\n",
      "Epoch [112/200], Batch [20/107], D_loss: 0.2399, G_loss: 11.2682\n",
      "Epoch [112/200], Batch [30/107], D_loss: 0.0176, G_loss: 9.6092\n",
      "Epoch [112/200], Batch [40/107], D_loss: 0.0280, G_loss: 6.8979\n",
      "Epoch [112/200], Batch [50/107], D_loss: 0.0087, G_loss: 7.6440\n",
      "Epoch [112/200], Batch [60/107], D_loss: 0.0033, G_loss: 7.6874\n",
      "Epoch [112/200], Batch [70/107], D_loss: 0.0141, G_loss: 10.3499\n",
      "Epoch [112/200], Batch [80/107], D_loss: 0.0061, G_loss: 8.7499\n",
      "Epoch [112/200], Batch [90/107], D_loss: 0.0026, G_loss: 8.6929\n",
      "Epoch [112/200], Batch [100/107], D_loss: 0.0024, G_loss: 10.4370\n",
      "Epoch [112/200], D_loss: 0.0927, G_GAN_loss: 7.3086, G_L1_loss: 1.0135, G_perc_loss: 0.0333, G_total_loss: 8.3554, \n",
      "Epoch [113/200], Batch [0/107], D_loss: 0.0019, G_loss: 9.1151\n",
      "Epoch [113/200], Batch [10/107], D_loss: 0.0049, G_loss: 5.8669\n",
      "Epoch [113/200], Batch [20/107], D_loss: 0.0053, G_loss: 9.1420\n",
      "Epoch [113/200], Batch [30/107], D_loss: 0.0014, G_loss: 9.6597\n",
      "Epoch [113/200], Batch [40/107], D_loss: 0.0030, G_loss: 9.0929\n",
      "Epoch [113/200], Batch [50/107], D_loss: 0.0054, G_loss: 9.4908\n",
      "Epoch [113/200], Batch [60/107], D_loss: 0.0130, G_loss: 6.1605\n",
      "Epoch [113/200], Batch [70/107], D_loss: 0.0027, G_loss: 9.0552\n",
      "Epoch [113/200], Batch [80/107], D_loss: 0.0086, G_loss: 6.5052\n",
      "Epoch [113/200], Batch [90/107], D_loss: 0.0041, G_loss: 9.2272\n",
      "Epoch [113/200], Batch [100/107], D_loss: 0.0012, G_loss: 8.5367\n",
      "Epoch [113/200], D_loss: 0.0034, G_GAN_loss: 7.5228, G_L1_loss: 1.0131, G_perc_loss: 0.0332, G_total_loss: 8.5691, \n",
      "Epoch [114/200], Batch [0/107], D_loss: 0.0037, G_loss: 6.9701\n",
      "Epoch [114/200], Batch [10/107], D_loss: 0.0013, G_loss: 8.6079\n",
      "Epoch [114/200], Batch [20/107], D_loss: 0.0039, G_loss: 10.3157\n",
      "Epoch [114/200], Batch [30/107], D_loss: 0.0008, G_loss: 9.3595\n",
      "Epoch [114/200], Batch [40/107], D_loss: 0.0074, G_loss: 6.4656\n",
      "Epoch [114/200], Batch [50/107], D_loss: 0.0038, G_loss: 7.1830\n",
      "Epoch [114/200], Batch [60/107], D_loss: 0.0016, G_loss: 9.1097\n",
      "Epoch [114/200], Batch [70/107], D_loss: 0.0023, G_loss: 10.9563\n",
      "Epoch [114/200], Batch [80/107], D_loss: 0.0008, G_loss: 8.6162\n",
      "Epoch [114/200], Batch [90/107], D_loss: 0.0010, G_loss: 8.1565\n",
      "Epoch [114/200], Batch [100/107], D_loss: 0.0007, G_loss: 9.7145\n",
      "Epoch [114/200], D_loss: 0.0021, G_GAN_loss: 7.7026, G_L1_loss: 1.0110, G_perc_loss: 0.0331, G_total_loss: 8.7467, \n",
      "Epoch [115/200], Batch [0/107], D_loss: 0.0009, G_loss: 8.3862\n",
      "Epoch [115/200], Batch [10/107], D_loss: 0.0017, G_loss: 7.2315\n",
      "Epoch [115/200], Batch [20/107], D_loss: 0.0008, G_loss: 8.3890\n",
      "Epoch [115/200], Batch [30/107], D_loss: 0.0010, G_loss: 10.7278\n",
      "Epoch [115/200], Batch [40/107], D_loss: 0.0034, G_loss: 7.8056\n",
      "Epoch [115/200], Batch [50/107], D_loss: 0.0138, G_loss: 11.1674\n",
      "Epoch [115/200], Batch [60/107], D_loss: 0.0013, G_loss: 8.4093\n",
      "Epoch [115/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.7593\n",
      "Epoch [115/200], Batch [80/107], D_loss: 0.0006, G_loss: 10.5166\n",
      "Epoch [115/200], Batch [90/107], D_loss: 0.0068, G_loss: 8.2011\n",
      "Epoch [115/200], Batch [100/107], D_loss: 0.0009, G_loss: 8.3265\n",
      "Epoch [115/200], D_loss: 0.0038, G_GAN_loss: 8.0470, G_L1_loss: 1.0115, G_perc_loss: 0.0331, G_total_loss: 9.0916, \n",
      "Epoch [116/200], Batch [0/107], D_loss: 0.0008, G_loss: 9.2375\n",
      "Epoch [116/200], Batch [10/107], D_loss: 0.0008, G_loss: 10.9390\n",
      "Epoch [116/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.8894\n",
      "Epoch [116/200], Batch [30/107], D_loss: 0.0007, G_loss: 9.6180\n",
      "Epoch [116/200], Batch [40/107], D_loss: 0.0010, G_loss: 10.4733\n",
      "Epoch [116/200], Batch [50/107], D_loss: 0.0014, G_loss: 10.5568\n",
      "Epoch [116/200], Batch [60/107], D_loss: 0.1054, G_loss: 10.8545\n",
      "Epoch [116/200], Batch [70/107], D_loss: 0.0037, G_loss: 9.3571\n",
      "Epoch [116/200], Batch [80/107], D_loss: 0.0014, G_loss: 8.9047\n",
      "Epoch [116/200], Batch [90/107], D_loss: 0.0049, G_loss: 8.8696\n",
      "Epoch [116/200], Batch [100/107], D_loss: 0.0057, G_loss: 10.4322\n",
      "Epoch [116/200], D_loss: 0.0308, G_GAN_loss: 8.2959, G_L1_loss: 1.0304, G_perc_loss: 0.0349, G_total_loss: 9.3612, \n",
      "Epoch [117/200], Batch [0/107], D_loss: 0.0016, G_loss: 10.3984\n",
      "Epoch [117/200], Batch [10/107], D_loss: 0.0009, G_loss: 8.9068\n",
      "Epoch [117/200], Batch [20/107], D_loss: 0.0008, G_loss: 8.9170\n",
      "Epoch [117/200], Batch [30/107], D_loss: 0.0025, G_loss: 6.8032\n",
      "Epoch [117/200], Batch [40/107], D_loss: 0.0009, G_loss: 9.6761\n",
      "Epoch [117/200], Batch [50/107], D_loss: 0.0011, G_loss: 8.0011\n",
      "Epoch [117/200], Batch [60/107], D_loss: 0.0016, G_loss: 11.5882\n",
      "Epoch [117/200], Batch [70/107], D_loss: 0.0015, G_loss: 11.6182\n",
      "Epoch [117/200], Batch [80/107], D_loss: 0.0019, G_loss: 7.0101\n",
      "Epoch [117/200], Batch [90/107], D_loss: 0.0010, G_loss: 9.6577\n",
      "Epoch [117/200], Batch [100/107], D_loss: 0.0004, G_loss: 9.6606\n",
      "Epoch [117/200], D_loss: 0.0015, G_GAN_loss: 8.4788, G_L1_loss: 1.0114, G_perc_loss: 0.0331, G_total_loss: 9.5233, \n",
      "Epoch [118/200], Batch [0/107], D_loss: 0.0006, G_loss: 9.6169\n",
      "Epoch [118/200], Batch [10/107], D_loss: 0.0011, G_loss: 7.5874\n",
      "Epoch [118/200], Batch [20/107], D_loss: 0.0115, G_loss: 6.0301\n",
      "Epoch [118/200], Batch [30/107], D_loss: 0.0020, G_loss: 7.3438\n",
      "Epoch [118/200], Batch [40/107], D_loss: 0.0003, G_loss: 10.1113\n",
      "Epoch [118/200], Batch [50/107], D_loss: 0.0010, G_loss: 11.0787\n",
      "Epoch [118/200], Batch [60/107], D_loss: 0.0004, G_loss: 11.3814\n",
      "Epoch [118/200], Batch [70/107], D_loss: 0.0007, G_loss: 8.6023\n",
      "Epoch [118/200], Batch [80/107], D_loss: 0.0005, G_loss: 10.0999\n",
      "Epoch [118/200], Batch [90/107], D_loss: 0.0033, G_loss: 11.6683\n",
      "Epoch [118/200], Batch [100/107], D_loss: 0.0006, G_loss: 8.4364\n",
      "Epoch [118/200], D_loss: 0.0014, G_GAN_loss: 8.4978, G_L1_loss: 1.0074, G_perc_loss: 0.0330, G_total_loss: 9.5382, \n",
      "Epoch [119/200], Batch [0/107], D_loss: 0.0021, G_loss: 10.6522\n",
      "Epoch [119/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.6080\n",
      "Epoch [119/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.5843\n",
      "Epoch [119/200], Batch [30/107], D_loss: 0.0006, G_loss: 8.4642\n",
      "Epoch [119/200], Batch [40/107], D_loss: 0.0014, G_loss: 7.6636\n",
      "Epoch [119/200], Batch [50/107], D_loss: 0.0010, G_loss: 9.3483\n",
      "Epoch [119/200], Batch [60/107], D_loss: 0.0009, G_loss: 10.1622\n",
      "Epoch [119/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.6733\n",
      "Epoch [119/200], Batch [80/107], D_loss: 0.0008, G_loss: 8.7708\n",
      "Epoch [119/200], Batch [90/107], D_loss: 0.0002, G_loss: 9.4812\n",
      "Epoch [119/200], Batch [100/107], D_loss: 0.0020, G_loss: 7.9600\n",
      "Epoch [119/200], D_loss: 0.0012, G_GAN_loss: 8.6752, G_L1_loss: 1.0071, G_perc_loss: 0.0329, G_total_loss: 9.7152, \n",
      "Epoch [120/200], Batch [0/107], D_loss: 0.0334, G_loss: 12.0024\n",
      "Epoch [120/200], Batch [10/107], D_loss: 0.0002, G_loss: 9.9180\n",
      "Epoch [120/200], Batch [20/107], D_loss: 0.0027, G_loss: 7.0967\n",
      "Epoch [120/200], Batch [30/107], D_loss: 0.0005, G_loss: 9.8904\n",
      "Epoch [120/200], Batch [40/107], D_loss: 0.0005, G_loss: 8.3511\n",
      "Epoch [120/200], Batch [50/107], D_loss: 0.0003, G_loss: 11.0598\n",
      "Epoch [120/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.8132\n",
      "Epoch [120/200], Batch [70/107], D_loss: 0.0004, G_loss: 10.2678\n",
      "Epoch [120/200], Batch [80/107], D_loss: 0.0003, G_loss: 9.8982\n",
      "Epoch [120/200], Batch [90/107], D_loss: 0.0005, G_loss: 8.4979\n",
      "Epoch [120/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.0782\n",
      "Epoch [120/200], D_loss: 0.0011, G_GAN_loss: 8.6170, G_L1_loss: 1.0060, G_perc_loss: 0.0329, G_total_loss: 9.6559, \n",
      "Epoch [121/200], Batch [0/107], D_loss: 0.0004, G_loss: 9.3099\n",
      "Epoch [121/200], Batch [10/107], D_loss: 0.0002, G_loss: 11.2751\n",
      "Epoch [121/200], Batch [20/107], D_loss: 0.0005, G_loss: 9.7268\n",
      "Epoch [121/200], Batch [30/107], D_loss: 0.0002, G_loss: 9.6339\n",
      "Epoch [121/200], Batch [40/107], D_loss: 0.0012, G_loss: 9.6427\n",
      "Epoch [121/200], Batch [50/107], D_loss: 0.0002, G_loss: 11.3430\n",
      "Epoch [121/200], Batch [60/107], D_loss: 0.0002, G_loss: 11.2580\n",
      "Epoch [121/200], Batch [70/107], D_loss: 0.0003, G_loss: 10.5952\n",
      "Epoch [121/200], Batch [80/107], D_loss: 0.0003, G_loss: 10.3321\n",
      "Epoch [121/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.6141\n",
      "Epoch [121/200], Batch [100/107], D_loss: 0.0005, G_loss: 8.9138\n",
      "Epoch [121/200], D_loss: 0.0007, G_GAN_loss: 9.0324, G_L1_loss: 1.0049, G_perc_loss: 0.0329, G_total_loss: 10.0702, \n",
      "Epoch [122/200], Batch [0/107], D_loss: 0.0003, G_loss: 8.2478\n",
      "Epoch [122/200], Batch [10/107], D_loss: 0.7774, G_loss: 10.7948\n",
      "Epoch [122/200], Batch [20/107], D_loss: 0.0038, G_loss: 8.2717\n",
      "Epoch [122/200], Batch [30/107], D_loss: 0.0018, G_loss: 8.6616\n",
      "Epoch [122/200], Batch [40/107], D_loss: 0.0038, G_loss: 8.7069\n",
      "Epoch [122/200], Batch [50/107], D_loss: 0.0007, G_loss: 9.1573\n",
      "Epoch [122/200], Batch [60/107], D_loss: 0.0008, G_loss: 8.7020\n",
      "Epoch [122/200], Batch [70/107], D_loss: 0.0007, G_loss: 9.5088\n",
      "Epoch [122/200], Batch [80/107], D_loss: 0.0400, G_loss: 4.4072\n",
      "Epoch [122/200], Batch [90/107], D_loss: 0.0023, G_loss: 7.6054\n",
      "Epoch [122/200], Batch [100/107], D_loss: 0.0021, G_loss: 9.3175\n",
      "Epoch [122/200], D_loss: 0.0421, G_GAN_loss: 7.7538, G_L1_loss: 1.0059, G_perc_loss: 0.0330, G_total_loss: 8.7928, \n",
      "Epoch [123/200], Batch [0/107], D_loss: 0.0007, G_loss: 10.4819\n",
      "Epoch [123/200], Batch [10/107], D_loss: 0.0006, G_loss: 10.6363\n",
      "Epoch [123/200], Batch [20/107], D_loss: 0.0013, G_loss: 12.3305\n",
      "Epoch [123/200], Batch [30/107], D_loss: 0.0005, G_loss: 10.6428\n",
      "Epoch [123/200], Batch [40/107], D_loss: 0.0004, G_loss: 10.1340\n",
      "Epoch [123/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.9513\n",
      "Epoch [123/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.8048\n",
      "Epoch [123/200], Batch [70/107], D_loss: 0.0003, G_loss: 10.9302\n",
      "Epoch [123/200], Batch [80/107], D_loss: 0.0006, G_loss: 9.1456\n",
      "Epoch [123/200], Batch [90/107], D_loss: 0.0009, G_loss: 8.7477\n",
      "Epoch [123/200], Batch [100/107], D_loss: 0.0008, G_loss: 8.7191\n",
      "Epoch [123/200], D_loss: 0.0010, G_GAN_loss: 8.9523, G_L1_loss: 1.0141, G_perc_loss: 0.0335, G_total_loss: 9.9999, \n",
      "Epoch [124/200], Batch [0/107], D_loss: 0.0008, G_loss: 10.3222\n",
      "Epoch [124/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.3718\n",
      "Epoch [124/200], Batch [20/107], D_loss: 0.0006, G_loss: 8.2128\n",
      "Epoch [124/200], Batch [30/107], D_loss: 0.0025, G_loss: 7.6371\n",
      "Epoch [124/200], Batch [40/107], D_loss: 0.0003, G_loss: 9.4930\n",
      "Epoch [124/200], Batch [50/107], D_loss: 0.0005, G_loss: 10.1649\n",
      "Epoch [124/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.0679\n",
      "Epoch [124/200], Batch [70/107], D_loss: 0.0012, G_loss: 8.5705\n",
      "Epoch [124/200], Batch [80/107], D_loss: 0.0006, G_loss: 10.6691\n",
      "Epoch [124/200], Batch [90/107], D_loss: 0.0006, G_loss: 10.6474\n",
      "Epoch [124/200], Batch [100/107], D_loss: 0.0005, G_loss: 10.9846\n",
      "Epoch [124/200], D_loss: 0.0012, G_GAN_loss: 9.0136, G_L1_loss: 1.0061, G_perc_loss: 0.0330, G_total_loss: 10.0528, \n",
      "Epoch [125/200], Batch [0/107], D_loss: 0.0007, G_loss: 8.8390\n",
      "Epoch [125/200], Batch [10/107], D_loss: 0.0004, G_loss: 9.6867\n",
      "Epoch [125/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.3829\n",
      "Epoch [125/200], Batch [30/107], D_loss: 0.0006, G_loss: 9.3896\n",
      "Epoch [125/200], Batch [40/107], D_loss: 0.0002, G_loss: 11.1863\n",
      "Epoch [125/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.7336\n",
      "Epoch [125/200], Batch [60/107], D_loss: 0.0027, G_loss: 6.9805\n",
      "Epoch [125/200], Batch [70/107], D_loss: 0.0003, G_loss: 10.6027\n",
      "Epoch [125/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.4769\n",
      "Epoch [125/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.7843\n",
      "Epoch [125/200], Batch [100/107], D_loss: 0.0006, G_loss: 12.0793\n",
      "Epoch [125/200], D_loss: 0.0007, G_GAN_loss: 9.0536, G_L1_loss: 1.0028, G_perc_loss: 0.0327, G_total_loss: 10.0891, \n",
      "Epoch [126/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.9255\n",
      "Epoch [126/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.6600\n",
      "Epoch [126/200], Batch [20/107], D_loss: 0.0009, G_loss: 8.7308\n",
      "Epoch [126/200], Batch [30/107], D_loss: 0.0003, G_loss: 9.8872\n",
      "Epoch [126/200], Batch [40/107], D_loss: 0.0005, G_loss: 8.6619\n",
      "Epoch [126/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.2681\n",
      "Epoch [126/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.1569\n",
      "Epoch [126/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.8558\n",
      "Epoch [126/200], Batch [80/107], D_loss: 0.0004, G_loss: 11.6996\n",
      "Epoch [126/200], Batch [90/107], D_loss: 0.0008, G_loss: 8.3718\n",
      "Epoch [126/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.2195\n",
      "Epoch [126/200], D_loss: 0.0006, G_GAN_loss: 9.6216, G_L1_loss: 1.0045, G_perc_loss: 0.0327, G_total_loss: 10.6588, \n",
      "Epoch [127/200], Batch [0/107], D_loss: 0.0003, G_loss: 11.7702\n",
      "Epoch [127/200], Batch [10/107], D_loss: 0.0002, G_loss: 10.6907\n",
      "Epoch [127/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.9310\n",
      "Epoch [127/200], Batch [30/107], D_loss: 0.0003, G_loss: 9.3936\n",
      "Epoch [127/200], Batch [40/107], D_loss: 0.0010, G_loss: 12.1907\n",
      "Epoch [127/200], Batch [50/107], D_loss: 0.0014, G_loss: 7.9699\n",
      "Epoch [127/200], Batch [60/107], D_loss: 0.0002, G_loss: 11.5765\n",
      "Epoch [127/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.6468\n",
      "Epoch [127/200], Batch [80/107], D_loss: 0.0043, G_loss: 8.2960\n",
      "Epoch [127/200], Batch [90/107], D_loss: 0.0035, G_loss: 12.8337\n",
      "Epoch [127/200], Batch [100/107], D_loss: 0.0002, G_loss: 9.8042\n",
      "Epoch [127/200], D_loss: 0.0013, G_GAN_loss: 9.5496, G_L1_loss: 1.0020, G_perc_loss: 0.0326, G_total_loss: 10.5842, \n",
      "Epoch [128/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.1071\n",
      "Epoch [128/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.1523\n",
      "Epoch [128/200], Batch [20/107], D_loss: 0.0007, G_loss: 10.8455\n",
      "Epoch [128/200], Batch [30/107], D_loss: 0.0001, G_loss: 10.1842\n",
      "Epoch [128/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.2490\n",
      "Epoch [128/200], Batch [50/107], D_loss: 0.0007, G_loss: 8.6150\n",
      "Epoch [128/200], Batch [60/107], D_loss: 0.0006, G_loss: 12.2280\n",
      "Epoch [128/200], Batch [70/107], D_loss: 0.0086, G_loss: 10.0529\n",
      "Epoch [128/200], Batch [80/107], D_loss: 0.0063, G_loss: 8.8179\n",
      "Epoch [128/200], Batch [90/107], D_loss: 0.0023, G_loss: 8.4815\n",
      "Epoch [128/200], Batch [100/107], D_loss: 0.0016, G_loss: 9.4924\n",
      "Epoch [128/200], D_loss: 0.0413, G_GAN_loss: 8.9426, G_L1_loss: 1.0007, G_perc_loss: 0.0326, G_total_loss: 9.9759, \n",
      "Epoch [129/200], Batch [0/107], D_loss: 0.0005, G_loss: 11.1267\n",
      "Epoch [129/200], Batch [10/107], D_loss: 0.0026, G_loss: 7.6421\n",
      "Epoch [129/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.8038\n",
      "Epoch [129/200], Batch [30/107], D_loss: 0.0008, G_loss: 10.5240\n",
      "Epoch [129/200], Batch [40/107], D_loss: 0.0009, G_loss: 9.8258\n",
      "Epoch [129/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.8487\n",
      "Epoch [129/200], Batch [60/107], D_loss: 0.0009, G_loss: 10.3564\n",
      "Epoch [129/200], Batch [70/107], D_loss: 0.0007, G_loss: 9.8323\n",
      "Epoch [129/200], Batch [80/107], D_loss: 0.0012, G_loss: 12.8699\n",
      "Epoch [129/200], Batch [90/107], D_loss: 0.0006, G_loss: 11.3852\n",
      "Epoch [129/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.7762\n",
      "Epoch [129/200], D_loss: 0.0010, G_GAN_loss: 9.0107, G_L1_loss: 1.0028, G_perc_loss: 0.0332, G_total_loss: 10.0467, \n",
      "Epoch [130/200], Batch [0/107], D_loss: 0.0005, G_loss: 8.6606\n",
      "Epoch [130/200], Batch [10/107], D_loss: 0.0025, G_loss: 11.4352\n",
      "Epoch [130/200], Batch [20/107], D_loss: 0.0003, G_loss: 9.8132\n",
      "Epoch [130/200], Batch [30/107], D_loss: 0.0008, G_loss: 8.5419\n",
      "Epoch [130/200], Batch [40/107], D_loss: 0.0004, G_loss: 9.4098\n",
      "Epoch [130/200], Batch [50/107], D_loss: 0.0013, G_loss: 10.6268\n",
      "Epoch [130/200], Batch [60/107], D_loss: 0.0005, G_loss: 12.1782\n",
      "Epoch [130/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.9428\n",
      "Epoch [130/200], Batch [80/107], D_loss: 0.0003, G_loss: 10.4424\n",
      "Epoch [130/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.6681\n",
      "Epoch [130/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.7587\n",
      "Epoch [130/200], D_loss: 0.0009, G_GAN_loss: 9.0839, G_L1_loss: 1.0002, G_perc_loss: 0.0326, G_total_loss: 10.1166, \n",
      "Epoch [131/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.4889\n",
      "Epoch [131/200], Batch [10/107], D_loss: 0.0033, G_loss: 12.7142\n",
      "Epoch [131/200], Batch [20/107], D_loss: 0.0005, G_loss: 9.1917\n",
      "Epoch [131/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.5863\n",
      "Epoch [131/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.9403\n",
      "Epoch [131/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.9949\n",
      "Epoch [131/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.2224\n",
      "Epoch [131/200], Batch [70/107], D_loss: 0.0002, G_loss: 9.3396\n",
      "Epoch [131/200], Batch [80/107], D_loss: 0.0007, G_loss: 8.5525\n",
      "Epoch [131/200], Batch [90/107], D_loss: 0.0019, G_loss: 7.5983\n",
      "Epoch [131/200], Batch [100/107], D_loss: 0.0002, G_loss: 10.2390\n",
      "Epoch [131/200], D_loss: 0.0007, G_GAN_loss: 9.3465, G_L1_loss: 1.0014, G_perc_loss: 0.0325, G_total_loss: 10.3804, \n",
      "Epoch [132/200], Batch [0/107], D_loss: 0.0002, G_loss: 11.6607\n",
      "Epoch [132/200], Batch [10/107], D_loss: 0.0002, G_loss: 9.4641\n",
      "Epoch [132/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.1143\n",
      "Epoch [132/200], Batch [30/107], D_loss: 0.0039, G_loss: 7.9166\n",
      "Epoch [132/200], Batch [40/107], D_loss: 0.0004, G_loss: 11.3169\n",
      "Epoch [132/200], Batch [50/107], D_loss: 0.0005, G_loss: 8.2744\n",
      "Epoch [132/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.2877\n",
      "Epoch [132/200], Batch [70/107], D_loss: 0.0008, G_loss: 8.6525\n",
      "Epoch [132/200], Batch [80/107], D_loss: 0.0001, G_loss: 10.9517\n",
      "Epoch [132/200], Batch [90/107], D_loss: 0.0017, G_loss: 8.9294\n",
      "Epoch [132/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.1200\n",
      "Epoch [132/200], D_loss: 0.0007, G_GAN_loss: 9.4439, G_L1_loss: 0.9993, G_perc_loss: 0.0324, G_total_loss: 10.4757, \n",
      "Epoch [133/200], Batch [0/107], D_loss: 0.0004, G_loss: 7.9665\n",
      "Epoch [133/200], Batch [10/107], D_loss: 0.0004, G_loss: 9.6092\n",
      "Epoch [133/200], Batch [20/107], D_loss: 0.0018, G_loss: 7.1771\n",
      "Epoch [133/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.5786\n",
      "Epoch [133/200], Batch [40/107], D_loss: 0.0002, G_loss: 9.9510\n",
      "Epoch [133/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.4475\n",
      "Epoch [133/200], Batch [60/107], D_loss: 0.0001, G_loss: 10.8872\n",
      "Epoch [133/200], Batch [70/107], D_loss: 0.0037, G_loss: 13.1486\n",
      "Epoch [133/200], Batch [80/107], D_loss: 0.0002, G_loss: 11.6449\n",
      "Epoch [133/200], Batch [90/107], D_loss: 0.0004, G_loss: 9.3288\n",
      "Epoch [133/200], Batch [100/107], D_loss: 0.0004, G_loss: 11.5964\n",
      "Epoch [133/200], D_loss: 0.0005, G_GAN_loss: 9.6989, G_L1_loss: 0.9991, G_perc_loss: 0.0324, G_total_loss: 10.7305, \n",
      "Epoch [134/200], Batch [0/107], D_loss: 0.0007, G_loss: 9.1997\n",
      "Epoch [134/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.8886\n",
      "Epoch [134/200], Batch [20/107], D_loss: 0.0026, G_loss: 12.9462\n",
      "Epoch [134/200], Batch [30/107], D_loss: 0.0043, G_loss: 7.2782\n",
      "Epoch [134/200], Batch [40/107], D_loss: 0.0007, G_loss: 10.2390\n",
      "Epoch [134/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.2142\n",
      "Epoch [134/200], Batch [60/107], D_loss: 0.0002, G_loss: 12.1472\n",
      "Epoch [134/200], Batch [70/107], D_loss: 0.0002, G_loss: 8.7967\n",
      "Epoch [134/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.5227\n",
      "Epoch [134/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.7284\n",
      "Epoch [134/200], Batch [100/107], D_loss: 0.0008, G_loss: 9.0794\n",
      "Epoch [134/200], D_loss: 0.0008, G_GAN_loss: 9.6447, G_L1_loss: 0.9993, G_perc_loss: 0.0324, G_total_loss: 10.6763, \n",
      "Epoch [135/200], Batch [0/107], D_loss: 0.0005, G_loss: 9.9636\n",
      "Epoch [135/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.3683\n",
      "Epoch [135/200], Batch [20/107], D_loss: 0.0013, G_loss: 7.5874\n",
      "Epoch [135/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.1812\n",
      "Epoch [135/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.6198\n",
      "Epoch [135/200], Batch [50/107], D_loss: 0.0001, G_loss: 12.6168\n",
      "Epoch [135/200], Batch [60/107], D_loss: 0.0002, G_loss: 11.9617\n",
      "Epoch [135/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.2674\n",
      "Epoch [135/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.8308\n",
      "Epoch [135/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.7591\n",
      "Epoch [135/200], Batch [100/107], D_loss: 0.0021, G_loss: 8.4773\n",
      "Epoch [135/200], D_loss: 0.0003, G_GAN_loss: 9.8209, G_L1_loss: 1.0001, G_perc_loss: 0.0332, G_total_loss: 10.8542, \n",
      "Epoch [136/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.8521\n",
      "Epoch [136/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.8809\n",
      "Epoch [136/200], Batch [20/107], D_loss: 0.0001, G_loss: 13.1351\n",
      "Epoch [136/200], Batch [30/107], D_loss: 0.0001, G_loss: 9.5253\n",
      "Epoch [136/200], Batch [40/107], D_loss: 0.0002, G_loss: 9.9573\n",
      "Epoch [136/200], Batch [50/107], D_loss: 0.0004, G_loss: 11.0036\n",
      "Epoch [136/200], Batch [60/107], D_loss: 0.0006, G_loss: 11.6574\n",
      "Epoch [136/200], Batch [70/107], D_loss: 0.0003, G_loss: 12.0807\n",
      "Epoch [136/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.4696\n",
      "Epoch [136/200], Batch [90/107], D_loss: 0.0003, G_loss: 13.1327\n",
      "Epoch [136/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.8997\n",
      "Epoch [136/200], D_loss: 0.0003, G_GAN_loss: 9.8250, G_L1_loss: 0.9950, G_perc_loss: 0.0324, G_total_loss: 10.8523, \n",
      "Epoch [137/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.2784\n",
      "Epoch [137/200], Batch [10/107], D_loss: 0.0002, G_loss: 11.0629\n",
      "Epoch [137/200], Batch [20/107], D_loss: 0.0000, G_loss: 12.8893\n",
      "Epoch [137/200], Batch [30/107], D_loss: 0.0004, G_loss: 8.9683\n",
      "Epoch [137/200], Batch [40/107], D_loss: 0.0005, G_loss: 9.2915\n",
      "Epoch [137/200], Batch [50/107], D_loss: 0.0001, G_loss: 11.6342\n",
      "Epoch [137/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.9485\n",
      "Epoch [137/200], Batch [70/107], D_loss: 0.0005, G_loss: 7.7653\n",
      "Epoch [137/200], Batch [80/107], D_loss: 0.0009, G_loss: 8.2614\n",
      "Epoch [137/200], Batch [90/107], D_loss: 0.0000, G_loss: 12.6751\n",
      "Epoch [137/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.4505\n",
      "Epoch [137/200], D_loss: 0.0004, G_GAN_loss: 10.0986, G_L1_loss: 0.9951, G_perc_loss: 0.0323, G_total_loss: 11.1260, \n",
      "Epoch [138/200], Batch [0/107], D_loss: 0.0001, G_loss: 12.8648\n",
      "Epoch [138/200], Batch [10/107], D_loss: 0.0002, G_loss: 10.1042\n",
      "Epoch [138/200], Batch [20/107], D_loss: 0.0001, G_loss: 11.6686\n",
      "Epoch [138/200], Batch [30/107], D_loss: 0.0001, G_loss: 12.0443\n",
      "Epoch [138/200], Batch [40/107], D_loss: 0.0002, G_loss: 12.6143\n",
      "Epoch [138/200], Batch [50/107], D_loss: 0.0002, G_loss: 11.4853\n",
      "Epoch [138/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.4010\n",
      "Epoch [138/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.1421\n",
      "Epoch [138/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.5651\n",
      "Epoch [138/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.2093\n",
      "Epoch [138/200], Batch [100/107], D_loss: 0.0001, G_loss: 12.3824\n",
      "Epoch [138/200], D_loss: 0.0004, G_GAN_loss: 10.0374, G_L1_loss: 0.9950, G_perc_loss: 0.0322, G_total_loss: 11.0646, \n",
      "Epoch [139/200], Batch [0/107], D_loss: 0.0001, G_loss: 11.3798\n",
      "Epoch [139/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.1451\n",
      "Epoch [139/200], Batch [20/107], D_loss: 0.0001, G_loss: 10.7953\n",
      "Epoch [139/200], Batch [30/107], D_loss: 0.0002, G_loss: 12.9006\n",
      "Epoch [139/200], Batch [40/107], D_loss: 0.0001, G_loss: 12.7570\n",
      "Epoch [139/200], Batch [50/107], D_loss: 0.0005, G_loss: 8.9505\n",
      "Epoch [139/200], Batch [60/107], D_loss: 0.0001, G_loss: 10.7946\n",
      "Epoch [139/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.6256\n",
      "Epoch [139/200], Batch [80/107], D_loss: 0.0000, G_loss: 12.1778\n",
      "Epoch [139/200], Batch [90/107], D_loss: 0.0004, G_loss: 13.3924\n",
      "Epoch [139/200], Batch [100/107], D_loss: 0.0004, G_loss: 9.7128\n",
      "Epoch [139/200], D_loss: 0.0004, G_GAN_loss: 10.0245, G_L1_loss: 0.9932, G_perc_loss: 0.0322, G_total_loss: 11.0498, \n",
      "Epoch [140/200], Batch [0/107], D_loss: 0.0090, G_loss: 10.4801\n",
      "Epoch [140/200], Batch [10/107], D_loss: 0.0017, G_loss: 8.2016\n",
      "Epoch [140/200], Batch [20/107], D_loss: 0.0016, G_loss: 12.0566\n",
      "Epoch [140/200], Batch [30/107], D_loss: 0.0001, G_loss: 12.0102\n",
      "Epoch [140/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.2697\n",
      "Epoch [140/200], Batch [50/107], D_loss: 0.0002, G_loss: 12.5499\n",
      "Epoch [140/200], Batch [60/107], D_loss: 0.0001, G_loss: 13.0648\n",
      "Epoch [140/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.8105\n",
      "Epoch [140/200], Batch [80/107], D_loss: 4.0892, G_loss: 21.0321\n",
      "Epoch [140/200], Batch [90/107], D_loss: 1.3985, G_loss: 2.8442\n",
      "Epoch [140/200], Batch [100/107], D_loss: 1.5380, G_loss: 2.3919\n",
      "Epoch [140/200], D_loss: 0.6809, G_GAN_loss: 9.0072, G_L1_loss: 1.0821, G_perc_loss: 0.0351, G_total_loss: 10.1244, \n",
      "Epoch [141/200], Batch [0/107], D_loss: 1.2749, G_loss: 2.6733\n",
      "Epoch [141/200], Batch [10/107], D_loss: 0.3581, G_loss: 3.4887\n",
      "Epoch [141/200], Batch [20/107], D_loss: 0.3730, G_loss: 3.3924\n",
      "Epoch [141/200], Batch [30/107], D_loss: 0.3330, G_loss: 3.3230\n",
      "Epoch [141/200], Batch [40/107], D_loss: 0.3033, G_loss: 2.5925\n",
      "Epoch [141/200], Batch [50/107], D_loss: 0.1208, G_loss: 4.7724\n",
      "Epoch [141/200], Batch [60/107], D_loss: 0.1293, G_loss: 3.3419\n",
      "Epoch [141/200], Batch [70/107], D_loss: 0.2189, G_loss: 4.1690\n",
      "Epoch [141/200], Batch [80/107], D_loss: 0.0646, G_loss: 4.7717\n",
      "Epoch [141/200], Batch [90/107], D_loss: 0.0959, G_loss: 5.9282\n",
      "Epoch [141/200], Batch [100/107], D_loss: 0.0669, G_loss: 3.8036\n",
      "Epoch [141/200], D_loss: 0.3003, G_GAN_loss: 2.9998, G_L1_loss: 1.0159, G_perc_loss: 0.0334, G_total_loss: 4.0491, \n",
      "Epoch [142/200], Batch [0/107], D_loss: 0.0233, G_loss: 5.9330\n",
      "Epoch [142/200], Batch [10/107], D_loss: 0.2213, G_loss: 3.6229\n",
      "Epoch [142/200], Batch [20/107], D_loss: 0.0794, G_loss: 5.5176\n",
      "Epoch [142/200], Batch [30/107], D_loss: 0.1312, G_loss: 5.0806\n",
      "Epoch [142/200], Batch [40/107], D_loss: 0.1162, G_loss: 3.5897\n",
      "Epoch [142/200], Batch [50/107], D_loss: 0.0389, G_loss: 4.5030\n",
      "Epoch [142/200], Batch [60/107], D_loss: 0.0223, G_loss: 5.8552\n",
      "Epoch [142/200], Batch [70/107], D_loss: 0.0192, G_loss: 4.9862\n",
      "Epoch [142/200], Batch [80/107], D_loss: 0.0715, G_loss: 4.5119\n",
      "Epoch [142/200], Batch [90/107], D_loss: 0.0677, G_loss: 6.9115\n",
      "Epoch [142/200], Batch [100/107], D_loss: 0.0143, G_loss: 5.5081\n",
      "Epoch [142/200], D_loss: 0.0668, G_GAN_loss: 4.6397, G_L1_loss: 1.0014, G_perc_loss: 0.0325, G_total_loss: 5.6737, \n",
      "Epoch [143/200], Batch [0/107], D_loss: 0.0091, G_loss: 6.2396\n",
      "Epoch [143/200], Batch [10/107], D_loss: 0.0559, G_loss: 5.0691\n",
      "Epoch [143/200], Batch [20/107], D_loss: 0.0619, G_loss: 5.3775\n",
      "Epoch [143/200], Batch [30/107], D_loss: 0.1294, G_loss: 6.7204\n",
      "Epoch [143/200], Batch [40/107], D_loss: 0.0451, G_loss: 7.3335\n",
      "Epoch [143/200], Batch [50/107], D_loss: 0.0764, G_loss: 3.8574\n",
      "Epoch [143/200], Batch [60/107], D_loss: 0.0377, G_loss: 5.5574\n",
      "Epoch [143/200], Batch [70/107], D_loss: 0.0083, G_loss: 6.3333\n",
      "Epoch [143/200], Batch [80/107], D_loss: 0.0230, G_loss: 5.3161\n",
      "Epoch [143/200], Batch [90/107], D_loss: 0.0118, G_loss: 6.0309\n",
      "Epoch [143/200], Batch [100/107], D_loss: 0.0124, G_loss: 6.0924\n",
      "Epoch [143/200], D_loss: 0.0620, G_GAN_loss: 4.9756, G_L1_loss: 0.9980, G_perc_loss: 0.0323, G_total_loss: 6.0059, \n",
      "Epoch [144/200], Batch [0/107], D_loss: 0.0081, G_loss: 6.9020\n",
      "Epoch [144/200], Batch [10/107], D_loss: 0.0114, G_loss: 6.0774\n",
      "Epoch [144/200], Batch [20/107], D_loss: 0.0076, G_loss: 6.5404\n",
      "Epoch [144/200], Batch [30/107], D_loss: 0.0085, G_loss: 6.3649\n",
      "Epoch [144/200], Batch [40/107], D_loss: 0.2422, G_loss: 7.0995\n",
      "Epoch [144/200], Batch [50/107], D_loss: 0.0074, G_loss: 7.6527\n",
      "Epoch [144/200], Batch [60/107], D_loss: 0.0189, G_loss: 5.3134\n",
      "Epoch [144/200], Batch [70/107], D_loss: 0.0074, G_loss: 6.4395\n",
      "Epoch [144/200], Batch [80/107], D_loss: 0.0096, G_loss: 7.2088\n",
      "Epoch [144/200], Batch [90/107], D_loss: 0.0194, G_loss: 7.5081\n",
      "Epoch [144/200], Batch [100/107], D_loss: 0.0200, G_loss: 6.1129\n",
      "Epoch [144/200], D_loss: 0.0289, G_GAN_loss: 5.6878, G_L1_loss: 0.9925, G_perc_loss: 0.0321, G_total_loss: 6.7124, \n",
      "Epoch [145/200], Batch [0/107], D_loss: 0.0041, G_loss: 7.5511\n",
      "Epoch [145/200], Batch [10/107], D_loss: 0.0090, G_loss: 8.0192\n",
      "Epoch [145/200], Batch [20/107], D_loss: 0.0056, G_loss: 6.2571\n",
      "Epoch [145/200], Batch [30/107], D_loss: 0.0077, G_loss: 6.9701\n",
      "Epoch [145/200], Batch [40/107], D_loss: 0.0085, G_loss: 5.5622\n",
      "Epoch [145/200], Batch [50/107], D_loss: 0.0055, G_loss: 8.7394\n",
      "Epoch [145/200], Batch [60/107], D_loss: 0.0075, G_loss: 7.2054\n",
      "Epoch [145/200], Batch [70/107], D_loss: 0.0107, G_loss: 5.9048\n",
      "Epoch [145/200], Batch [80/107], D_loss: 0.1078, G_loss: 6.3605\n",
      "Epoch [145/200], Batch [90/107], D_loss: 0.0076, G_loss: 6.3825\n",
      "Epoch [145/200], Batch [100/107], D_loss: 0.0251, G_loss: 4.7919\n",
      "Epoch [145/200], D_loss: 0.0277, G_GAN_loss: 5.7815, G_L1_loss: 0.9922, G_perc_loss: 0.0321, G_total_loss: 6.8058, \n",
      "Epoch [146/200], Batch [0/107], D_loss: 0.0048, G_loss: 7.7540\n",
      "Epoch [146/200], Batch [10/107], D_loss: 0.0178, G_loss: 6.0924\n",
      "Epoch [146/200], Batch [20/107], D_loss: 0.0050, G_loss: 8.6317\n",
      "Epoch [146/200], Batch [30/107], D_loss: 0.0031, G_loss: 7.5516\n",
      "Epoch [146/200], Batch [40/107], D_loss: 0.0073, G_loss: 6.6755\n",
      "Epoch [146/200], Batch [50/107], D_loss: 0.0050, G_loss: 6.4944\n",
      "Epoch [146/200], Batch [60/107], D_loss: 0.0051, G_loss: 8.8261\n",
      "Epoch [146/200], Batch [70/107], D_loss: 0.0029, G_loss: 7.8162\n",
      "Epoch [146/200], Batch [80/107], D_loss: 0.0039, G_loss: 7.5292\n",
      "Epoch [146/200], Batch [90/107], D_loss: 0.0033, G_loss: 8.5835\n",
      "Epoch [146/200], Batch [100/107], D_loss: 0.0016, G_loss: 7.9190\n",
      "Epoch [146/200], D_loss: 0.0090, G_GAN_loss: 6.5822, G_L1_loss: 0.9922, G_perc_loss: 0.0320, G_total_loss: 7.6065, \n",
      "Epoch [147/200], Batch [0/107], D_loss: 0.0023, G_loss: 8.1896\n",
      "Epoch [147/200], Batch [10/107], D_loss: 0.0071, G_loss: 6.0193\n",
      "Epoch [147/200], Batch [20/107], D_loss: 0.0010, G_loss: 9.0944\n",
      "Epoch [147/200], Batch [30/107], D_loss: 0.0066, G_loss: 6.8847\n",
      "Epoch [147/200], Batch [40/107], D_loss: 0.0023, G_loss: 7.7708\n",
      "Epoch [147/200], Batch [50/107], D_loss: 0.0043, G_loss: 6.6803\n",
      "Epoch [147/200], Batch [60/107], D_loss: 0.0051, G_loss: 6.8539\n",
      "Epoch [147/200], Batch [70/107], D_loss: 0.0027, G_loss: 7.6828\n",
      "Epoch [147/200], Batch [80/107], D_loss: 0.0014, G_loss: 8.1000\n",
      "Epoch [147/200], Batch [90/107], D_loss: 0.1770, G_loss: 8.0716\n",
      "Epoch [147/200], Batch [100/107], D_loss: 0.0024, G_loss: 7.2514\n",
      "Epoch [147/200], D_loss: 0.0089, G_GAN_loss: 6.8258, G_L1_loss: 0.9926, G_perc_loss: 0.0321, G_total_loss: 7.8505, \n",
      "Epoch [148/200], Batch [0/107], D_loss: 0.0051, G_loss: 8.0412\n",
      "Epoch [148/200], Batch [10/107], D_loss: 0.0037, G_loss: 6.6636\n",
      "Epoch [148/200], Batch [20/107], D_loss: 0.0028, G_loss: 8.9209\n",
      "Epoch [148/200], Batch [30/107], D_loss: 0.0169, G_loss: 5.9151\n",
      "Epoch [148/200], Batch [40/107], D_loss: 0.0018, G_loss: 7.0916\n",
      "Epoch [148/200], Batch [50/107], D_loss: 0.0018, G_loss: 8.7568\n",
      "Epoch [148/200], Batch [60/107], D_loss: 0.0016, G_loss: 6.9141\n",
      "Epoch [148/200], Batch [70/107], D_loss: 0.0010, G_loss: 9.0591\n",
      "Epoch [148/200], Batch [80/107], D_loss: 0.0012, G_loss: 8.3998\n",
      "Epoch [148/200], Batch [90/107], D_loss: 0.0009, G_loss: 8.8435\n",
      "Epoch [148/200], Batch [100/107], D_loss: 0.0044, G_loss: 6.3259\n",
      "Epoch [148/200], D_loss: 0.0048, G_GAN_loss: 6.8117, G_L1_loss: 0.9909, G_perc_loss: 0.0321, G_total_loss: 7.8347, \n",
      "Epoch [149/200], Batch [0/107], D_loss: 0.0210, G_loss: 9.6701\n",
      "Epoch [149/200], Batch [10/107], D_loss: 0.0043, G_loss: 6.9387\n",
      "Epoch [149/200], Batch [20/107], D_loss: 0.0024, G_loss: 9.2309\n",
      "Epoch [149/200], Batch [30/107], D_loss: 0.0015, G_loss: 7.3867\n",
      "Epoch [149/200], Batch [40/107], D_loss: 0.0019, G_loss: 7.5602\n",
      "Epoch [149/200], Batch [50/107], D_loss: 0.0013, G_loss: 8.4983\n",
      "Epoch [149/200], Batch [60/107], D_loss: 0.0024, G_loss: 7.8731\n",
      "Epoch [149/200], Batch [70/107], D_loss: 0.0015, G_loss: 9.6352\n",
      "Epoch [149/200], Batch [80/107], D_loss: 0.0015, G_loss: 9.0538\n",
      "Epoch [149/200], Batch [90/107], D_loss: 0.0014, G_loss: 7.9649\n",
      "Epoch [149/200], Batch [100/107], D_loss: 0.0009, G_loss: 8.4434\n",
      "Epoch [149/200], D_loss: 0.0075, G_GAN_loss: 7.0741, G_L1_loss: 0.9908, G_perc_loss: 0.0320, G_total_loss: 8.0969, \n",
      "Epoch [150/200], Batch [0/107], D_loss: 0.0009, G_loss: 9.3924\n",
      "Epoch [150/200], Batch [10/107], D_loss: 0.0011, G_loss: 8.3701\n",
      "Epoch [150/200], Batch [20/107], D_loss: 0.0024, G_loss: 8.0358\n",
      "Epoch [150/200], Batch [30/107], D_loss: 0.0008, G_loss: 10.0660\n",
      "Epoch [150/200], Batch [40/107], D_loss: 0.0031, G_loss: 8.4795\n",
      "Epoch [150/200], Batch [50/107], D_loss: 0.0022, G_loss: 9.5267\n",
      "Epoch [150/200], Batch [60/107], D_loss: 0.0010, G_loss: 8.9076\n",
      "Epoch [150/200], Batch [70/107], D_loss: 0.0006, G_loss: 9.7188\n",
      "Epoch [150/200], Batch [80/107], D_loss: 0.0007, G_loss: 10.2788\n",
      "Epoch [150/200], Batch [90/107], D_loss: 0.0009, G_loss: 8.4222\n",
      "Epoch [150/200], Batch [100/107], D_loss: 0.0203, G_loss: 6.8664\n",
      "Epoch [150/200], D_loss: 0.0111, G_GAN_loss: 7.5280, G_L1_loss: 0.9893, G_perc_loss: 0.0319, G_total_loss: 8.5491, \n",
      "Epoch [151/200], Batch [0/107], D_loss: 0.0051, G_loss: 9.7918\n",
      "Epoch [151/200], Batch [10/107], D_loss: 0.0045, G_loss: 8.4151\n",
      "Epoch [151/200], Batch [20/107], D_loss: 0.0014, G_loss: 8.6853\n",
      "Epoch [151/200], Batch [30/107], D_loss: 0.0085, G_loss: 10.0731\n",
      "Epoch [151/200], Batch [40/107], D_loss: 0.0006, G_loss: 9.2386\n",
      "Epoch [151/200], Batch [50/107], D_loss: 0.0083, G_loss: 9.6029\n",
      "Epoch [151/200], Batch [60/107], D_loss: 0.0006, G_loss: 9.5955\n",
      "Epoch [151/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.3156\n",
      "Epoch [151/200], Batch [80/107], D_loss: 0.0098, G_loss: 5.5800\n",
      "Epoch [151/200], Batch [90/107], D_loss: 0.0006, G_loss: 9.6339\n",
      "Epoch [151/200], Batch [100/107], D_loss: 0.0028, G_loss: 7.2866\n",
      "Epoch [151/200], D_loss: 0.0067, G_GAN_loss: 7.6685, G_L1_loss: 0.9867, G_perc_loss: 0.0319, G_total_loss: 8.6871, \n",
      "Epoch [152/200], Batch [0/107], D_loss: 0.0037, G_loss: 6.3910\n",
      "Epoch [152/200], Batch [10/107], D_loss: 0.0065, G_loss: 6.6351\n",
      "Epoch [152/200], Batch [20/107], D_loss: 0.0023, G_loss: 6.9996\n",
      "Epoch [152/200], Batch [30/107], D_loss: 0.0008, G_loss: 9.8347\n",
      "Epoch [152/200], Batch [40/107], D_loss: 0.7231, G_loss: 5.8845\n",
      "Epoch [152/200], Batch [50/107], D_loss: 0.0241, G_loss: 5.7211\n",
      "Epoch [152/200], Batch [60/107], D_loss: 0.0018, G_loss: 8.1328\n",
      "Epoch [152/200], Batch [70/107], D_loss: 0.0027, G_loss: 8.8405\n",
      "Epoch [152/200], Batch [80/107], D_loss: 0.0044, G_loss: 8.4503\n",
      "Epoch [152/200], Batch [90/107], D_loss: 0.0021, G_loss: 8.5809\n",
      "Epoch [152/200], Batch [100/107], D_loss: 0.0017, G_loss: 7.6858\n",
      "Epoch [152/200], D_loss: 0.0364, G_GAN_loss: 7.1603, G_L1_loss: 0.9847, G_perc_loss: 0.0319, G_total_loss: 8.1768, \n",
      "Epoch [153/200], Batch [0/107], D_loss: 0.0021, G_loss: 8.2872\n",
      "Epoch [153/200], Batch [10/107], D_loss: 0.0064, G_loss: 6.6697\n",
      "Epoch [153/200], Batch [20/107], D_loss: 0.0126, G_loss: 5.4474\n",
      "Epoch [153/200], Batch [30/107], D_loss: 0.0018, G_loss: 8.0695\n",
      "Epoch [153/200], Batch [40/107], D_loss: 0.0050, G_loss: 6.1821\n",
      "Epoch [153/200], Batch [50/107], D_loss: 0.0121, G_loss: 5.4378\n",
      "Epoch [153/200], Batch [60/107], D_loss: 0.0011, G_loss: 8.3674\n",
      "Epoch [153/200], Batch [70/107], D_loss: 0.0026, G_loss: 8.4372\n",
      "Epoch [153/200], Batch [80/107], D_loss: 0.0010, G_loss: 8.0043\n",
      "Epoch [153/200], Batch [90/107], D_loss: 0.0017, G_loss: 7.8296\n",
      "Epoch [153/200], Batch [100/107], D_loss: 0.0034, G_loss: 9.0959\n",
      "Epoch [153/200], D_loss: 0.0030, G_GAN_loss: 7.1504, G_L1_loss: 0.9884, G_perc_loss: 0.0319, G_total_loss: 8.1707, \n",
      "Epoch [154/200], Batch [0/107], D_loss: 0.0009, G_loss: 9.4416\n",
      "Epoch [154/200], Batch [10/107], D_loss: 0.0013, G_loss: 8.2474\n",
      "Epoch [154/200], Batch [20/107], D_loss: 0.0011, G_loss: 8.1507\n",
      "Epoch [154/200], Batch [30/107], D_loss: 0.0062, G_loss: 9.1046\n",
      "Epoch [154/200], Batch [40/107], D_loss: 0.0018, G_loss: 9.8243\n",
      "Epoch [154/200], Batch [50/107], D_loss: 0.0007, G_loss: 8.3365\n",
      "Epoch [154/200], Batch [60/107], D_loss: 0.0031, G_loss: 9.9471\n",
      "Epoch [154/200], Batch [70/107], D_loss: 0.0012, G_loss: 8.7905\n",
      "Epoch [154/200], Batch [80/107], D_loss: 0.0011, G_loss: 7.3645\n",
      "Epoch [154/200], Batch [90/107], D_loss: 0.0008, G_loss: 9.0020\n",
      "Epoch [154/200], Batch [100/107], D_loss: 0.0037, G_loss: 6.6434\n",
      "Epoch [154/200], D_loss: 0.0020, G_GAN_loss: 7.4477, G_L1_loss: 0.9921, G_perc_loss: 0.0325, G_total_loss: 8.4722, \n",
      "Epoch [155/200], Batch [0/107], D_loss: 0.0042, G_loss: 9.8256\n",
      "Epoch [155/200], Batch [10/107], D_loss: 0.0018, G_loss: 7.3484\n",
      "Epoch [155/200], Batch [20/107], D_loss: 0.0034, G_loss: 7.1122\n",
      "Epoch [155/200], Batch [30/107], D_loss: 0.0010, G_loss: 8.1270\n",
      "Epoch [155/200], Batch [40/107], D_loss: 0.0009, G_loss: 9.8805\n",
      "Epoch [155/200], Batch [50/107], D_loss: 0.0005, G_loss: 9.0582\n",
      "Epoch [155/200], Batch [60/107], D_loss: 0.0008, G_loss: 10.0666\n",
      "Epoch [155/200], Batch [70/107], D_loss: 0.0038, G_loss: 9.7957\n",
      "Epoch [155/200], Batch [80/107], D_loss: 0.0005, G_loss: 8.6231\n",
      "Epoch [155/200], Batch [90/107], D_loss: 0.0009, G_loss: 8.1865\n",
      "Epoch [155/200], Batch [100/107], D_loss: 0.0011, G_loss: 9.0551\n",
      "Epoch [155/200], D_loss: 0.0022, G_GAN_loss: 7.4755, G_L1_loss: 0.9859, G_perc_loss: 0.0318, G_total_loss: 8.4932, \n",
      "Epoch [156/200], Batch [0/107], D_loss: 0.0009, G_loss: 9.2836\n",
      "Epoch [156/200], Batch [10/107], D_loss: 0.0007, G_loss: 8.5773\n",
      "Epoch [156/200], Batch [20/107], D_loss: 0.0125, G_loss: 9.9094\n",
      "Epoch [156/200], Batch [30/107], D_loss: 0.0007, G_loss: 8.3942\n",
      "Epoch [156/200], Batch [40/107], D_loss: 0.0016, G_loss: 10.7099\n",
      "Epoch [156/200], Batch [50/107], D_loss: 0.0013, G_loss: 8.2313\n",
      "Epoch [156/200], Batch [60/107], D_loss: 0.0006, G_loss: 8.4665\n",
      "Epoch [156/200], Batch [70/107], D_loss: 0.0013, G_loss: 9.9559\n",
      "Epoch [156/200], Batch [80/107], D_loss: 0.0010, G_loss: 8.0602\n",
      "Epoch [156/200], Batch [90/107], D_loss: 0.0017, G_loss: 7.5023\n",
      "Epoch [156/200], Batch [100/107], D_loss: 0.0043, G_loss: 8.8926\n",
      "Epoch [156/200], D_loss: 0.0017, G_GAN_loss: 7.6149, G_L1_loss: 0.9846, G_perc_loss: 0.0318, G_total_loss: 8.6314, \n",
      "Epoch [157/200], Batch [0/107], D_loss: 0.0016, G_loss: 6.7113\n",
      "Epoch [157/200], Batch [10/107], D_loss: 0.0044, G_loss: 6.5227\n",
      "Epoch [157/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.3011\n",
      "Epoch [157/200], Batch [30/107], D_loss: 0.0013, G_loss: 8.5308\n",
      "Epoch [157/200], Batch [40/107], D_loss: 0.0007, G_loss: 9.2380\n",
      "Epoch [157/200], Batch [50/107], D_loss: 0.0005, G_loss: 8.9286\n",
      "Epoch [157/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.1337\n",
      "Epoch [157/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.3863\n",
      "Epoch [157/200], Batch [80/107], D_loss: 0.0008, G_loss: 7.5543\n",
      "Epoch [157/200], Batch [90/107], D_loss: 0.0006, G_loss: 8.8011\n",
      "Epoch [157/200], Batch [100/107], D_loss: 0.0007, G_loss: 9.5165\n",
      "Epoch [157/200], D_loss: 0.0015, G_GAN_loss: 7.6145, G_L1_loss: 0.9837, G_perc_loss: 0.0318, G_total_loss: 8.6299, \n",
      "Epoch [158/200], Batch [0/107], D_loss: 0.0008, G_loss: 9.8334\n",
      "Epoch [158/200], Batch [10/107], D_loss: 0.0004, G_loss: 9.0266\n",
      "Epoch [158/200], Batch [20/107], D_loss: 0.0006, G_loss: 9.5294\n",
      "Epoch [158/200], Batch [30/107], D_loss: 0.0005, G_loss: 10.1161\n",
      "Epoch [158/200], Batch [40/107], D_loss: 0.0007, G_loss: 8.5898\n",
      "Epoch [158/200], Batch [50/107], D_loss: 0.0006, G_loss: 8.5356\n",
      "Epoch [158/200], Batch [60/107], D_loss: 0.0009, G_loss: 9.5219\n",
      "Epoch [158/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.2910\n",
      "Epoch [158/200], Batch [80/107], D_loss: 0.0009, G_loss: 7.5858\n",
      "Epoch [158/200], Batch [90/107], D_loss: 0.0008, G_loss: 8.3490\n",
      "Epoch [158/200], Batch [100/107], D_loss: 0.0005, G_loss: 8.7167\n",
      "Epoch [158/200], D_loss: 0.0011, G_GAN_loss: 7.9564, G_L1_loss: 0.9828, G_perc_loss: 0.0317, G_total_loss: 8.9710, \n",
      "Epoch [159/200], Batch [0/107], D_loss: 0.0004, G_loss: 8.8471\n",
      "Epoch [159/200], Batch [90/107], D_loss: 0.0003, G_loss: 8.6716\n",
      "Epoch [159/200], Batch [100/107], D_loss: 0.0005, G_loss: 10.2858\n",
      "Epoch [159/200], D_loss: 0.0012, G_GAN_loss: 8.1052, G_L1_loss: 0.9831, G_perc_loss: 0.0317, G_total_loss: 9.1200, \n",
      "Epoch [160/200], Batch [0/107], D_loss: 0.0020, G_loss: 7.9553\n",
      "Epoch [160/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.0523\n",
      "Epoch [160/200], Batch [20/107], D_loss: 0.0009, G_loss: 8.8553\n",
      "Epoch [160/200], Batch [30/107], D_loss: 0.0013, G_loss: 6.9721\n",
      "Epoch [160/200], Batch [40/107], D_loss: 0.0003, G_loss: 10.0911\n",
      "Epoch [160/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.4718\n",
      "Epoch [160/200], Batch [60/107], D_loss: 0.0011, G_loss: 8.5911\n",
      "Epoch [160/200], Batch [70/107], D_loss: 0.0006, G_loss: 8.2831\n",
      "Epoch [160/200], Batch [80/107], D_loss: 0.0004, G_loss: 9.1309\n",
      "Epoch [160/200], Batch [90/107], D_loss: 0.0005, G_loss: 11.1878\n",
      "Epoch [160/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.0810\n",
      "Epoch [160/200], D_loss: 0.0012, G_GAN_loss: 8.1246, G_L1_loss: 0.9825, G_perc_loss: 0.0320, G_total_loss: 9.1391, \n",
      "Epoch [161/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.1535\n",
      "Epoch [161/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.6135\n",
      "Epoch [161/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.3884\n",
      "Epoch [161/200], Batch [30/107], D_loss: 0.0006, G_loss: 10.0180\n",
      "Epoch [161/200], Batch [40/107], D_loss: 0.0003, G_loss: 9.6519\n",
      "Epoch [161/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.9065\n",
      "Epoch [161/200], Batch [60/107], D_loss: 0.0005, G_loss: 8.5760\n",
      "Epoch [161/200], Batch [70/107], D_loss: 0.0005, G_loss: 8.5843\n",
      "Epoch [161/200], Batch [80/107], D_loss: 0.0003, G_loss: 10.5016\n",
      "Epoch [161/200], Batch [90/107], D_loss: 0.0003, G_loss: 9.3806\n",
      "Epoch [161/200], Batch [100/107], D_loss: 0.0005, G_loss: 8.8187\n",
      "Epoch [161/200], D_loss: 0.0013, G_GAN_loss: 8.3219, G_L1_loss: 0.9819, G_perc_loss: 0.0320, G_total_loss: 9.3358, \n",
      "Epoch [162/200], Batch [0/107], D_loss: 0.0127, G_loss: 8.7829\n",
      "Epoch [162/200], Batch [10/107], D_loss: 0.0022, G_loss: 8.5150\n",
      "Epoch [162/200], Batch [20/107], D_loss: 0.0089, G_loss: 6.6454\n",
      "Epoch [162/200], Batch [30/107], D_loss: 0.0010, G_loss: 7.8926\n",
      "Epoch [162/200], Batch [40/107], D_loss: 0.0004, G_loss: 9.2435\n",
      "Epoch [162/200], Batch [50/107], D_loss: 0.0007, G_loss: 10.5219\n",
      "Epoch [162/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.4996\n",
      "Epoch [162/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.1375\n",
      "Epoch [162/200], Batch [80/107], D_loss: 0.0014, G_loss: 7.6625\n",
      "Epoch [162/200], Batch [90/107], D_loss: 0.0005, G_loss: 9.0274\n",
      "Epoch [162/200], Batch [100/107], D_loss: 0.0009, G_loss: 8.1773\n",
      "Epoch [162/200], D_loss: 0.0035, G_GAN_loss: 7.8868, G_L1_loss: 0.9804, G_perc_loss: 0.0317, G_total_loss: 8.8988, \n",
      "Epoch [163/200], Batch [0/107], D_loss: 0.0003, G_loss: 9.4957\n",
      "Epoch [163/200], Batch [10/107], D_loss: 0.0002, G_loss: 10.6921\n",
      "Epoch [163/200], Batch [20/107], D_loss: 0.0007, G_loss: 10.6263\n",
      "Epoch [163/200], Batch [30/107], D_loss: 0.0002, G_loss: 10.3917\n",
      "Epoch [163/200], Batch [40/107], D_loss: 0.0002, G_loss: 9.9837\n",
      "Epoch [163/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.0165\n",
      "Epoch [163/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.2178\n",
      "Epoch [163/200], Batch [70/107], D_loss: 0.0044, G_loss: 7.1418\n",
      "Epoch [163/200], Batch [80/107], D_loss: 0.0004, G_loss: 10.4888\n",
      "Epoch [163/200], Batch [90/107], D_loss: 0.0005, G_loss: 10.3944\n",
      "Epoch [163/200], Batch [100/107], D_loss: 0.0005, G_loss: 9.0621\n",
      "Epoch [163/200], D_loss: 0.0010, G_GAN_loss: 8.5400, G_L1_loss: 0.9825, G_perc_loss: 0.0327, G_total_loss: 9.5551, \n",
      "Epoch [164/200], Batch [0/107], D_loss: 0.0010, G_loss: 8.0826\n",
      "Epoch [164/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.7520\n",
      "Epoch [164/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.5015\n",
      "Epoch [164/200], Batch [30/107], D_loss: 0.0002, G_loss: 10.0430\n",
      "Epoch [164/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.7586\n",
      "Epoch [164/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.2258\n",
      "Epoch [164/200], Batch [60/107], D_loss: 0.0005, G_loss: 9.7099\n",
      "Epoch [164/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.9944\n",
      "Epoch [164/200], Batch [80/107], D_loss: 0.0003, G_loss: 8.6840\n",
      "Epoch [164/200], Batch [90/107], D_loss: 0.0003, G_loss: 11.4298\n",
      "Epoch [164/200], Batch [100/107], D_loss: 0.0005, G_loss: 8.3339\n",
      "Epoch [164/200], D_loss: 0.0005, G_GAN_loss: 8.7593, G_L1_loss: 0.9798, G_perc_loss: 0.0317, G_total_loss: 9.7707, \n",
      "Epoch [165/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.8742\n",
      "Epoch [165/200], Batch [10/107], D_loss: 0.0024, G_loss: 8.1958\n",
      "Epoch [165/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.1129\n",
      "Epoch [165/200], Batch [30/107], D_loss: 0.0003, G_loss: 11.1116\n",
      "Epoch [165/200], Batch [40/107], D_loss: 0.0002, G_loss: 8.8704\n",
      "Epoch [165/200], Batch [50/107], D_loss: 0.0003, G_loss: 11.3788\n",
      "Epoch [165/200], Batch [60/107], D_loss: 0.0016, G_loss: 11.3952\n",
      "Epoch [165/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.8675\n",
      "Epoch [165/200], Batch [80/107], D_loss: 0.0002, G_loss: 9.6094\n",
      "Epoch [165/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.2533\n",
      "Epoch [165/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.0635\n",
      "Epoch [165/200], D_loss: 0.0009, G_GAN_loss: 8.6729, G_L1_loss: 0.9779, G_perc_loss: 0.0322, G_total_loss: 9.6831, \n",
      "Epoch [166/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.9224\n",
      "Epoch [166/200], Batch [10/107], D_loss: 0.0002, G_loss: 9.3094\n",
      "Epoch [166/200], Batch [20/107], D_loss: 0.0046, G_loss: 11.6467\n",
      "Epoch [166/200], Batch [30/107], D_loss: 0.0002, G_loss: 10.2132\n",
      "Epoch [166/200], Batch [40/107], D_loss: 0.0044, G_loss: 10.9907\n",
      "Epoch [166/200], Batch [50/107], D_loss: 0.0015, G_loss: 7.9520\n",
      "Epoch [166/200], Batch [60/107], D_loss: 0.0001, G_loss: 10.7634\n",
      "Epoch [166/200], Batch [70/107], D_loss: 0.0014, G_loss: 10.5227\n",
      "Epoch [166/200], Batch [80/107], D_loss: 0.0005, G_loss: 8.4028\n",
      "Epoch [166/200], Batch [90/107], D_loss: 0.0002, G_loss: 8.9363\n",
      "Epoch [166/200], Batch [100/107], D_loss: 0.0011, G_loss: 11.8099\n",
      "Epoch [166/200], D_loss: 0.0007, G_GAN_loss: 8.7786, G_L1_loss: 0.9765, G_perc_loss: 0.0317, G_total_loss: 9.7868, \n",
      "Epoch [167/200], Batch [0/107], D_loss: 0.0005, G_loss: 11.5518\n",
      "Epoch [167/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.5757\n",
      "Epoch [167/200], Batch [20/107], D_loss: 0.0012, G_loss: 10.7306\n",
      "Epoch [167/200], Batch [30/107], D_loss: 0.0002, G_loss: 10.4610\n",
      "Epoch [167/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.3250\n",
      "Epoch [167/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.3022\n",
      "Epoch [167/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.4578\n",
      "Epoch [167/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.0948\n",
      "Epoch [167/200], Batch [80/107], D_loss: 0.0011, G_loss: 8.1786\n",
      "Epoch [167/200], Batch [90/107], D_loss: 0.0007, G_loss: 10.7131\n",
      "Epoch [167/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.2936\n",
      "Epoch [167/200], D_loss: 0.0004, G_GAN_loss: 9.3392, G_L1_loss: 0.9769, G_perc_loss: 0.0316, G_total_loss: 10.3477, \n",
      "Epoch [168/200], Batch [0/107], D_loss: 0.0002, G_loss: 9.4592\n",
      "Epoch [168/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.5116\n",
      "Epoch [168/200], Batch [20/107], D_loss: 0.0003, G_loss: 10.4607\n",
      "Epoch [168/200], Batch [30/107], D_loss: 0.0003, G_loss: 9.8204\n",
      "Epoch [168/200], Batch [40/107], D_loss: 0.0002, G_loss: 9.6832\n",
      "Epoch [168/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.3209\n",
      "Epoch [168/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.5817\n",
      "Epoch [168/200], Batch [70/107], D_loss: 0.0002, G_loss: 11.3183\n",
      "Epoch [168/200], Batch [80/107], D_loss: 0.0002, G_loss: 11.6602\n",
      "Epoch [168/200], Batch [90/107], D_loss: 0.0001, G_loss: 9.7805\n",
      "Epoch [168/200], Batch [100/107], D_loss: 0.0008, G_loss: 11.9116\n",
      "Epoch [168/200], D_loss: 0.0004, G_GAN_loss: 9.2550, G_L1_loss: 0.9758, G_perc_loss: 0.0319, G_total_loss: 10.2626, \n",
      "Epoch [169/200], Batch [0/107], D_loss: 0.0003, G_loss: 9.3328\n",
      "Epoch [169/200], Batch [10/107], D_loss: 0.0007, G_loss: 8.3033\n",
      "Epoch [169/200], Batch [20/107], D_loss: 0.0003, G_loss: 11.9309\n",
      "Epoch [169/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.2509\n",
      "Epoch [169/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.8137\n",
      "Epoch [169/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.8338\n",
      "Epoch [169/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.9622\n",
      "Epoch [169/200], Batch [70/107], D_loss: 0.0014, G_loss: 11.9512\n",
      "Epoch [169/200], Batch [80/107], D_loss: 0.0003, G_loss: 9.4427\n",
      "Epoch [169/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.9113\n",
      "Epoch [169/200], Batch [100/107], D_loss: 0.0001, G_loss: 10.1755\n",
      "Epoch [169/200], D_loss: 0.0004, G_GAN_loss: 9.1464, G_L1_loss: 0.9731, G_perc_loss: 0.0316, G_total_loss: 10.1511, \n",
      "Epoch [170/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.1448\n",
      "Epoch [170/200], Batch [10/107], D_loss: 0.0004, G_loss: 10.4246\n",
      "Epoch [170/200], Batch [20/107], D_loss: 0.0001, G_loss: 10.5798\n",
      "Epoch [170/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.5289\n",
      "Epoch [170/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.2067\n",
      "Epoch [170/200], Batch [50/107], D_loss: 0.0004, G_loss: 11.0865\n",
      "Epoch [170/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.0153\n",
      "Epoch [170/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.2920\n",
      "Epoch [170/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.1769\n",
      "Epoch [170/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.1042\n",
      "Epoch [170/200], Batch [100/107], D_loss: 0.0008, G_loss: 8.1887\n",
      "Epoch [170/200], D_loss: 0.0003, G_GAN_loss: 9.4568, G_L1_loss: 0.9752, G_perc_loss: 0.0319, G_total_loss: 10.4639, \n",
      "Epoch [171/200], Batch [0/107], D_loss: 0.0011, G_loss: 12.2907\n",
      "Epoch [171/200], Batch [10/107], D_loss: 0.0006, G_loss: 10.0988\n",
      "Epoch [171/200], Batch [20/107], D_loss: 0.0005, G_loss: 9.1921\n",
      "Epoch [171/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.1672\n",
      "Epoch [171/200], Batch [40/107], D_loss: 0.0002, G_loss: 9.7082\n",
      "Epoch [171/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.1520\n",
      "Epoch [171/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.3103\n",
      "Epoch [171/200], Batch [70/107], D_loss: 0.0001, G_loss: 11.0201\n",
      "Epoch [171/200], Batch [80/107], D_loss: 0.0020, G_loss: 7.5554\n",
      "Epoch [171/200], Batch [90/107], D_loss: 0.0001, G_loss: 11.6479\n",
      "Epoch [171/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.3801\n",
      "Epoch [171/200], D_loss: 0.0008, G_GAN_loss: 9.5146, G_L1_loss: 0.9719, G_perc_loss: 0.0319, G_total_loss: 10.5184, \n",
      "Epoch [172/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.9419\n",
      "Epoch [172/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.6293\n",
      "Epoch [172/200], Batch [20/107], D_loss: 0.0002, G_loss: 9.6687\n",
      "Epoch [172/200], Batch [30/107], D_loss: 0.0014, G_loss: 12.5730\n",
      "Epoch [172/200], Batch [40/107], D_loss: 0.0001, G_loss: 10.9131\n",
      "Epoch [172/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.2953\n",
      "Epoch [172/200], Batch [60/107], D_loss: 0.0017, G_loss: 7.7557\n",
      "Epoch [172/200], Batch [70/107], D_loss: 0.0038, G_loss: 7.1744\n",
      "Epoch [172/200], Batch [80/107], D_loss: 0.0002, G_loss: 9.2705\n",
      "Epoch [172/200], Batch [90/107], D_loss: 0.0002, G_loss: 8.6599\n",
      "Epoch [172/200], Batch [100/107], D_loss: 0.0002, G_loss: 12.1360\n",
      "Epoch [172/200], D_loss: 0.0007, G_GAN_loss: 9.6385, G_L1_loss: 0.9756, G_perc_loss: 0.0319, G_total_loss: 10.6460, \n",
      "Epoch [173/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.2480\n",
      "Epoch [173/200], Batch [10/107], D_loss: 0.0001, G_loss: 11.5258\n",
      "Epoch [173/200], Batch [20/107], D_loss: 0.0014, G_loss: 11.7656\n",
      "Epoch [173/200], Batch [30/107], D_loss: 0.0001, G_loss: 11.6240\n",
      "Epoch [173/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.2534\n",
      "Epoch [173/200], Batch [50/107], D_loss: 0.0002, G_loss: 10.0777\n",
      "Epoch [173/200], Batch [60/107], D_loss: 0.0004, G_loss: 10.2650\n",
      "Epoch [173/200], Batch [70/107], D_loss: 0.0007, G_loss: 8.9031\n",
      "Epoch [173/200], Batch [80/107], D_loss: 0.0002, G_loss: 10.2867\n",
      "Epoch [173/200], Batch [90/107], D_loss: 1.0695, G_loss: 5.7628\n",
      "Epoch [173/200], Batch [100/107], D_loss: 0.6300, G_loss: 5.4033\n",
      "Epoch [173/200], D_loss: 0.2304, G_GAN_loss: 9.0309, G_L1_loss: 0.9711, G_perc_loss: 0.0315, G_total_loss: 10.0335, \n",
      "Epoch [174/200], Batch [0/107], D_loss: 0.1252, G_loss: 5.4960\n",
      "Epoch [174/200], Batch [10/107], D_loss: 0.0296, G_loss: 5.7286\n",
      "Epoch [174/200], Batch [20/107], D_loss: 0.0402, G_loss: 7.1901\n",
      "Epoch [174/200], Batch [30/107], D_loss: 0.0163, G_loss: 6.5725\n",
      "Epoch [174/200], Batch [40/107], D_loss: 0.0217, G_loss: 6.7497\n",
      "Epoch [174/200], Batch [50/107], D_loss: 0.0126, G_loss: 6.2308\n",
      "Epoch [174/200], Batch [60/107], D_loss: 0.0171, G_loss: 6.8234\n",
      "Epoch [174/200], Batch [70/107], D_loss: 0.0070, G_loss: 7.2732\n",
      "Epoch [174/200], Batch [80/107], D_loss: 0.0059, G_loss: 6.8827\n",
      "Epoch [174/200], Batch [90/107], D_loss: 0.0054, G_loss: 7.0310\n",
      "Epoch [174/200], Batch [100/107], D_loss: 0.0214, G_loss: 5.5504\n",
      "Epoch [174/200], D_loss: 0.0226, G_GAN_loss: 5.5819, G_L1_loss: 0.9685, G_perc_loss: 0.0320, G_total_loss: 6.5824, \n",
      "Epoch [175/200], Batch [0/107], D_loss: 0.0242, G_loss: 6.2377\n",
      "Epoch [175/200], Batch [10/107], D_loss: 0.0055, G_loss: 8.1335\n",
      "Epoch [175/200], Batch [20/107], D_loss: 0.0058, G_loss: 7.3373\n",
      "Epoch [175/200], Batch [30/107], D_loss: 0.0062, G_loss: 6.6986\n",
      "Epoch [175/200], Batch [40/107], D_loss: 0.0121, G_loss: 5.6062\n",
      "Epoch [175/200], Batch [50/107], D_loss: 0.0180, G_loss: 7.3789\n",
      "Epoch [175/200], Batch [60/107], D_loss: 0.0058, G_loss: 7.2515\n",
      "Epoch [175/200], Batch [70/107], D_loss: 0.0054, G_loss: 6.8237\n",
      "Epoch [175/200], Batch [80/107], D_loss: 0.0390, G_loss: 8.4412\n",
      "Epoch [175/200], Batch [90/107], D_loss: 0.0200, G_loss: 6.6631\n",
      "Epoch [175/200], Batch [100/107], D_loss: 0.0124, G_loss: 6.6317\n",
      "Epoch [175/200], D_loss: 0.0357, G_GAN_loss: 6.1201, G_L1_loss: 0.9777, G_perc_loss: 0.0318, G_total_loss: 7.1296, \n",
      "Epoch [176/200], Batch [0/107], D_loss: 0.0065, G_loss: 7.1939\n",
      "Epoch [176/200], Batch [10/107], D_loss: 0.0079, G_loss: 6.8292\n",
      "Epoch [176/200], Batch [20/107], D_loss: 0.0100, G_loss: 6.2779\n",
      "Epoch [176/200], Batch [30/107], D_loss: 0.0024, G_loss: 7.9407\n",
      "Epoch [176/200], Batch [40/107], D_loss: 0.0027, G_loss: 7.8669\n",
      "Epoch [176/200], Batch [50/107], D_loss: 0.0014, G_loss: 8.0961\n",
      "Epoch [176/200], Batch [60/107], D_loss: 0.0016, G_loss: 9.0019\n",
      "Epoch [176/200], Batch [70/107], D_loss: 0.0023, G_loss: 7.9787\n",
      "Epoch [176/200], Batch [80/107], D_loss: 0.0074, G_loss: 6.0613\n",
      "Epoch [176/200], Batch [90/107], D_loss: 0.0015, G_loss: 8.6618\n",
      "Epoch [176/200], Batch [100/107], D_loss: 0.0076, G_loss: 9.0964\n",
      "Epoch [176/200], D_loss: 0.0052, G_GAN_loss: 6.9148, G_L1_loss: 0.9707, G_perc_loss: 0.0316, G_total_loss: 7.9171, \n",
      "Epoch [177/200], Batch [0/107], D_loss: 0.0032, G_loss: 6.9348\n",
      "Epoch [177/200], Batch [10/107], D_loss: 0.0021, G_loss: 7.1949\n",
      "Epoch [177/200], Batch [20/107], D_loss: 0.0076, G_loss: 6.8385\n",
      "Epoch [177/200], Batch [30/107], D_loss: 0.0016, G_loss: 8.4590\n",
      "Epoch [177/200], Batch [40/107], D_loss: 0.0013, G_loss: 8.8633\n",
      "Epoch [177/200], Batch [50/107], D_loss: 0.0027, G_loss: 7.7230\n",
      "Epoch [177/200], Batch [60/107], D_loss: 0.0092, G_loss: 8.9206\n",
      "Epoch [177/200], Batch [70/107], D_loss: 0.0027, G_loss: 9.4875\n",
      "Epoch [177/200], Batch [80/107], D_loss: 0.0019, G_loss: 7.8111\n",
      "Epoch [177/200], Batch [90/107], D_loss: 0.0005, G_loss: 9.3207\n",
      "Epoch [177/200], Batch [100/107], D_loss: 0.0009, G_loss: 8.9790\n",
      "Epoch [177/200], D_loss: 0.0050, G_GAN_loss: 7.1231, G_L1_loss: 0.9689, G_perc_loss: 0.0315, G_total_loss: 8.1235, \n",
      "Epoch [178/200], Batch [0/107], D_loss: 0.0018, G_loss: 7.8497\n",
      "Epoch [178/200], Batch [10/107], D_loss: 0.0013, G_loss: 8.5506\n",
      "Epoch [178/200], Batch [20/107], D_loss: 0.0032, G_loss: 6.9541\n",
      "Epoch [178/200], Batch [30/107], D_loss: 0.0014, G_loss: 8.2281\n",
      "Epoch [178/200], Batch [40/107], D_loss: 0.0008, G_loss: 8.7790\n",
      "Epoch [178/200], Batch [50/107], D_loss: 0.0011, G_loss: 8.3143\n",
      "Epoch [178/200], Batch [60/107], D_loss: 0.0011, G_loss: 7.9608\n",
      "Epoch [178/200], Batch [70/107], D_loss: 0.0036, G_loss: 7.2296\n",
      "Epoch [178/200], Batch [80/107], D_loss: 0.0022, G_loss: 9.3015\n",
      "Epoch [178/200], Batch [90/107], D_loss: 0.0023, G_loss: 9.3446\n",
      "Epoch [178/200], Batch [100/107], D_loss: 0.0013, G_loss: 7.5348\n",
      "Epoch [178/200], D_loss: 0.0031, G_GAN_loss: 7.6963, G_L1_loss: 0.9668, G_perc_loss: 0.0314, G_total_loss: 8.6945, \n",
      "Epoch [179/200], Batch [0/107], D_loss: 0.0006, G_loss: 9.0028\n",
      "Epoch [179/200], Batch [10/107], D_loss: 0.0007, G_loss: 8.5808\n",
      "Epoch [179/200], Batch [20/107], D_loss: 0.0009, G_loss: 8.5405\n",
      "Epoch [179/200], Batch [30/107], D_loss: 0.0004, G_loss: 8.5336\n",
      "Epoch [179/200], Batch [40/107], D_loss: 0.0004, G_loss: 9.7402\n",
      "Epoch [179/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.6300\n",
      "Epoch [179/200], Batch [60/107], D_loss: 0.0007, G_loss: 8.5956\n",
      "Epoch [179/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.3772\n",
      "Epoch [179/200], Batch [80/107], D_loss: 0.0063, G_loss: 6.7015\n",
      "Epoch [179/200], Batch [90/107], D_loss: 0.0006, G_loss: 8.0313\n",
      "Epoch [179/200], Batch [100/107], D_loss: 0.0008, G_loss: 8.1941\n",
      "Epoch [179/200], D_loss: 0.0014, G_GAN_loss: 7.8992, G_L1_loss: 0.9699, G_perc_loss: 0.0313, G_total_loss: 8.9004, \n",
      "Epoch [180/200], Batch [0/107], D_loss: 0.0005, G_loss: 8.1209\n",
      "Epoch [180/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.0245\n",
      "Epoch [180/200], Batch [20/107], D_loss: 0.0006, G_loss: 9.7399\n",
      "Epoch [180/200], Batch [30/107], D_loss: 0.0008, G_loss: 9.1595\n",
      "Epoch [180/200], Batch [40/107], D_loss: 0.0006, G_loss: 9.9432\n",
      "Epoch [180/200], Batch [50/107], D_loss: 0.0004, G_loss: 8.3830\n",
      "Epoch [180/200], Batch [60/107], D_loss: 0.0003, G_loss: 10.2486\n",
      "Epoch [180/200], Batch [70/107], D_loss: 0.0025, G_loss: 11.0305\n",
      "Epoch [180/200], Batch [80/107], D_loss: 0.0004, G_loss: 9.8739\n",
      "Epoch [180/200], Batch [90/107], D_loss: 0.0026, G_loss: 7.4286\n",
      "Epoch [180/200], Batch [100/107], D_loss: 0.0013, G_loss: 7.9812\n",
      "Epoch [180/200], D_loss: 0.0017, G_GAN_loss: 8.3274, G_L1_loss: 0.9679, G_perc_loss: 0.0315, G_total_loss: 9.3268, \n",
      "Epoch [181/200], Batch [0/107], D_loss: 0.0004, G_loss: 8.7319\n",
      "Epoch [181/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.8825\n",
      "Epoch [181/200], Batch [20/107], D_loss: 0.0018, G_loss: 7.6530\n",
      "Epoch [181/200], Batch [30/107], D_loss: 0.0005, G_loss: 9.8673\n",
      "Epoch [181/200], Batch [40/107], D_loss: 0.0003, G_loss: 8.7963\n",
      "Epoch [181/200], Batch [50/107], D_loss: 0.0015, G_loss: 7.0586\n",
      "Epoch [181/200], Batch [60/107], D_loss: 0.0006, G_loss: 8.5773\n",
      "Epoch [181/200], Batch [70/107], D_loss: 0.0002, G_loss: 9.6308\n",
      "Epoch [181/200], Batch [80/107], D_loss: 0.0003, G_loss: 9.5451\n",
      "Epoch [181/200], Batch [90/107], D_loss: 0.0052, G_loss: 10.7112\n",
      "Epoch [181/200], Batch [100/107], D_loss: 0.0112, G_loss: 5.9514\n",
      "Epoch [181/200], D_loss: 0.0019, G_GAN_loss: 8.2927, G_L1_loss: 0.9652, G_perc_loss: 0.0314, G_total_loss: 9.2893, \n",
      "Epoch [182/200], Batch [0/107], D_loss: 0.0025, G_loss: 6.0343\n",
      "Epoch [182/200], Batch [10/107], D_loss: 0.0007, G_loss: 8.5037\n",
      "Epoch [182/200], Batch [20/107], D_loss: 0.0003, G_loss: 9.9066\n",
      "Epoch [182/200], Batch [30/107], D_loss: 0.0002, G_loss: 9.0249\n",
      "Epoch [182/200], Batch [40/107], D_loss: 0.0013, G_loss: 9.3473\n",
      "Epoch [182/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.6299\n",
      "Epoch [182/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.2558\n",
      "Epoch [182/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.4063\n",
      "Epoch [182/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.3493\n",
      "Epoch [182/200], Batch [90/107], D_loss: 0.0002, G_loss: 8.9641\n",
      "Epoch [182/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.1221\n",
      "Epoch [182/200], D_loss: 0.0009, G_GAN_loss: 8.6478, G_L1_loss: 0.9662, G_perc_loss: 0.0316, G_total_loss: 9.6455, \n",
      "Epoch [183/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.5996\n",
      "Epoch [183/200], Batch [10/107], D_loss: 0.0001, G_loss: 9.6109\n",
      "Epoch [183/200], Batch [20/107], D_loss: 0.0034, G_loss: 10.0989\n",
      "Epoch [183/200], Batch [30/107], D_loss: 0.0009, G_loss: 8.1137\n",
      "Epoch [183/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.3061\n",
      "Epoch [183/200], Batch [50/107], D_loss: 0.0004, G_loss: 8.4546\n",
      "Epoch [183/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.6114\n",
      "Epoch [183/200], Batch [70/107], D_loss: 0.0123, G_loss: 11.3884\n",
      "Epoch [183/200], Batch [80/107], D_loss: 0.0004, G_loss: 10.0251\n",
      "Epoch [183/200], Batch [90/107], D_loss: 0.0006, G_loss: 8.1503\n",
      "Epoch [183/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.8537\n",
      "Epoch [183/200], D_loss: 0.0007, G_GAN_loss: 8.6286, G_L1_loss: 0.9650, G_perc_loss: 0.0313, G_total_loss: 9.6249, \n",
      "Epoch [184/200], Batch [0/107], D_loss: 0.0022, G_loss: 7.5003\n",
      "Epoch [184/200], Batch [10/107], D_loss: 0.5890, G_loss: 6.3299\n",
      "Epoch [184/200], Batch [20/107], D_loss: 0.0017, G_loss: 8.4060\n",
      "Epoch [184/200], Batch [30/107], D_loss: 0.0026, G_loss: 7.2066\n",
      "Epoch [184/200], Batch [40/107], D_loss: 0.0010, G_loss: 9.1747\n",
      "Epoch [184/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.7951\n",
      "Epoch [184/200], Batch [60/107], D_loss: 0.0004, G_loss: 9.9347\n",
      "Epoch [184/200], Batch [70/107], D_loss: 0.0045, G_loss: 10.0036\n",
      "Epoch [184/200], Batch [80/107], D_loss: 0.0012, G_loss: 8.1373\n",
      "Epoch [184/200], Batch [90/107], D_loss: 0.0006, G_loss: 10.1289\n",
      "Epoch [184/200], Batch [100/107], D_loss: 0.0022, G_loss: 7.3862\n",
      "Epoch [184/200], D_loss: 0.0528, G_GAN_loss: 7.6751, G_L1_loss: 0.9682, G_perc_loss: 0.0313, G_total_loss: 8.6746, \n",
      "Epoch [185/200], Batch [0/107], D_loss: 0.0054, G_loss: 7.0235\n",
      "Epoch [185/200], Batch [10/107], D_loss: 0.0016, G_loss: 7.1121\n",
      "Epoch [185/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.4723\n",
      "Epoch [185/200], Batch [30/107], D_loss: 0.0006, G_loss: 8.9396\n",
      "Epoch [185/200], Batch [40/107], D_loss: 0.0009, G_loss: 9.4956\n",
      "Epoch [185/200], Batch [50/107], D_loss: 0.0037, G_loss: 10.5805\n",
      "Epoch [185/200], Batch [60/107], D_loss: 0.0010, G_loss: 9.4763\n",
      "Epoch [185/200], Batch [70/107], D_loss: 0.0008, G_loss: 8.5795\n",
      "Epoch [185/200], Batch [80/107], D_loss: 0.0006, G_loss: 7.9735\n",
      "Epoch [185/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.2573\n",
      "Epoch [185/200], Batch [100/107], D_loss: 0.0005, G_loss: 9.3072\n",
      "Epoch [185/200], D_loss: 0.0012, G_GAN_loss: 8.2042, G_L1_loss: 0.9649, G_perc_loss: 0.0315, G_total_loss: 9.2006, \n",
      "Epoch [186/200], Batch [0/107], D_loss: 0.0003, G_loss: 9.6897\n",
      "Epoch [186/200], Batch [10/107], D_loss: 0.0009, G_loss: 10.2750\n",
      "Epoch [186/200], Batch [20/107], D_loss: 0.0060, G_loss: 11.5285\n",
      "Epoch [186/200], Batch [30/107], D_loss: 0.0006, G_loss: 9.5742\n",
      "Epoch [186/200], Batch [40/107], D_loss: 0.0021, G_loss: 8.1959\n",
      "Epoch [186/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.7999\n",
      "Epoch [186/200], Batch [60/107], D_loss: 0.0016, G_loss: 11.2857\n",
      "Epoch [186/200], Batch [70/107], D_loss: 0.0008, G_loss: 9.3598\n",
      "Epoch [186/200], Batch [80/107], D_loss: 0.0003, G_loss: 8.9003\n",
      "Epoch [186/200], Batch [90/107], D_loss: 0.0006, G_loss: 9.3365\n",
      "Epoch [186/200], Batch [100/107], D_loss: 0.0012, G_loss: 8.5129\n",
      "Epoch [186/200], D_loss: 0.0149, G_GAN_loss: 8.4592, G_L1_loss: 0.9634, G_perc_loss: 0.0313, G_total_loss: 9.4539, \n",
      "Epoch [187/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.0561\n",
      "Epoch [187/200], Batch [10/107], D_loss: 0.0005, G_loss: 9.2830\n",
      "Epoch [187/200], Batch [20/107], D_loss: 0.0006, G_loss: 10.8554\n",
      "Epoch [187/200], Batch [30/107], D_loss: 0.0004, G_loss: 9.1346\n",
      "Epoch [187/200], Batch [40/107], D_loss: 0.0003, G_loss: 9.4937\n",
      "Epoch [187/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.9948\n",
      "Epoch [187/200], Batch [60/107], D_loss: 0.0054, G_loss: 9.5425\n",
      "Epoch [187/200], Batch [70/107], D_loss: 0.0006, G_loss: 8.5927\n",
      "Epoch [187/200], Batch [80/107], D_loss: 0.0022, G_loss: 6.8425\n",
      "Epoch [187/200], Batch [90/107], D_loss: 0.0002, G_loss: 9.8303\n",
      "Epoch [187/200], Batch [100/107], D_loss: 0.0002, G_loss: 10.0953\n",
      "Epoch [187/200], D_loss: 0.0016, G_GAN_loss: 8.3251, G_L1_loss: 0.9648, G_perc_loss: 0.0316, G_total_loss: 9.3214, \n",
      "Epoch [188/200], Batch [0/107], D_loss: 0.0007, G_loss: 8.3782\n",
      "Epoch [188/200], Batch [10/107], D_loss: 0.0005, G_loss: 10.8118\n",
      "Epoch [188/200], Batch [20/107], D_loss: 0.0003, G_loss: 9.8996\n",
      "Epoch [188/200], Batch [30/107], D_loss: 0.0002, G_loss: 9.2927\n",
      "Epoch [188/200], Batch [40/107], D_loss: 0.0060, G_loss: 5.7702\n",
      "Epoch [188/200], Batch [50/107], D_loss: 0.0005, G_loss: 9.1427\n",
      "Epoch [188/200], Batch [60/107], D_loss: 0.0002, G_loss: 11.2864\n",
      "Epoch [188/200], Batch [70/107], D_loss: 0.0004, G_loss: 9.2333\n",
      "Epoch [188/200], Batch [80/107], D_loss: 0.0017, G_loss: 7.7964\n",
      "Epoch [188/200], Batch [90/107], D_loss: 0.0002, G_loss: 10.0157\n",
      "Epoch [188/200], Batch [100/107], D_loss: 0.0005, G_loss: 11.0951\n",
      "Epoch [188/200], D_loss: 0.0007, G_GAN_loss: 8.7269, G_L1_loss: 0.9659, G_perc_loss: 0.0320, G_total_loss: 9.7248, \n",
      "Epoch [189/200], Batch [0/107], D_loss: 0.0002, G_loss: 10.3789\n",
      "Epoch [189/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.5136\n",
      "Epoch [189/200], Batch [20/107], D_loss: 0.0008, G_loss: 8.3473\n",
      "Epoch [189/200], Batch [30/107], D_loss: 0.0007, G_loss: 7.7277\n",
      "Epoch [189/200], Batch [40/107], D_loss: 0.0071, G_loss: 6.3914\n",
      "Epoch [189/200], Batch [50/107], D_loss: 0.0004, G_loss: 9.7814\n",
      "Epoch [189/200], Batch [60/107], D_loss: 0.0003, G_loss: 9.3603\n",
      "Epoch [189/200], Batch [70/107], D_loss: 0.0016, G_loss: 11.2605\n",
      "Epoch [189/200], Batch [80/107], D_loss: 0.0005, G_loss: 7.7000\n",
      "Epoch [189/200], Batch [90/107], D_loss: 0.0002, G_loss: 8.9985\n",
      "Epoch [189/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.6056\n",
      "Epoch [189/200], D_loss: 0.0032, G_GAN_loss: 8.2309, G_L1_loss: 0.9631, G_perc_loss: 0.0312, G_total_loss: 9.2252, \n",
      "Epoch [190/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.1880\n",
      "Epoch [190/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.4759\n",
      "Epoch [190/200], Batch [20/107], D_loss: 0.0005, G_loss: 9.1224\n",
      "Epoch [190/200], Batch [30/107], D_loss: 0.0001, G_loss: 10.2709\n",
      "Epoch [190/200], Batch [40/107], D_loss: 0.0018, G_loss: 11.9075\n",
      "Epoch [190/200], Batch [50/107], D_loss: 0.1856, G_loss: 9.5762\n",
      "Epoch [190/200], Batch [60/107], D_loss: 0.0008, G_loss: 9.2703\n",
      "Epoch [190/200], Batch [70/107], D_loss: 0.0120, G_loss: 8.9551\n",
      "Epoch [190/200], Batch [80/107], D_loss: 0.0003, G_loss: 10.5162\n",
      "Epoch [190/200], Batch [90/107], D_loss: 0.0004, G_loss: 11.0987\n",
      "Epoch [190/200], Batch [100/107], D_loss: 0.0001, G_loss: 11.4990\n",
      "Epoch [190/200], D_loss: 0.0067, G_GAN_loss: 9.2534, G_L1_loss: 0.9606, G_perc_loss: 0.0311, G_total_loss: 10.2451, \n",
      "Epoch [191/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.6381\n",
      "Epoch [191/200], Batch [10/107], D_loss: 0.0002, G_loss: 12.0475\n",
      "Epoch [191/200], Batch [20/107], D_loss: 0.0002, G_loss: 11.1612\n",
      "Epoch [191/200], Batch [30/107], D_loss: 0.0001, G_loss: 10.5480\n",
      "Epoch [191/200], Batch [40/107], D_loss: 0.0008, G_loss: 11.0729\n",
      "Epoch [191/200], Batch [50/107], D_loss: 0.0002, G_loss: 9.3289\n",
      "Epoch [191/200], Batch [60/107], D_loss: 0.0004, G_loss: 11.1273\n",
      "Epoch [191/200], Batch [70/107], D_loss: 0.0025, G_loss: 7.5961\n",
      "Epoch [191/200], Batch [80/107], D_loss: 0.0001, G_loss: 11.3427\n",
      "Epoch [191/200], Batch [90/107], D_loss: 0.0002, G_loss: 11.3672\n",
      "Epoch [191/200], Batch [100/107], D_loss: 0.0002, G_loss: 10.4017\n",
      "Epoch [191/200], D_loss: 0.0006, G_GAN_loss: 9.6401, G_L1_loss: 0.9613, G_perc_loss: 0.0311, G_total_loss: 10.6326, \n",
      "Epoch [192/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.3892\n",
      "Epoch [192/200], Batch [10/107], D_loss: 0.0002, G_loss: 11.0532\n",
      "Epoch [192/200], Batch [20/107], D_loss: 0.0001, G_loss: 11.0249\n",
      "Epoch [192/200], Batch [30/107], D_loss: 0.0051, G_loss: 6.3504\n",
      "Epoch [192/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.9861\n",
      "Epoch [192/200], Batch [50/107], D_loss: 0.0664, G_loss: 11.2459\n",
      "Epoch [192/200], Batch [60/107], D_loss: 0.0411, G_loss: 9.6039\n",
      "Epoch [192/200], Batch [70/107], D_loss: 0.0907, G_loss: 10.9524\n",
      "Epoch [192/200], Batch [80/107], D_loss: 0.0022, G_loss: 7.9160\n",
      "Epoch [192/200], Batch [90/107], D_loss: 0.0015, G_loss: 8.9457\n",
      "Epoch [192/200], Batch [100/107], D_loss: 0.0004, G_loss: 9.6473\n",
      "Epoch [192/200], D_loss: 0.0230, G_GAN_loss: 9.1752, G_L1_loss: 0.9632, G_perc_loss: 0.0319, G_total_loss: 10.1703, \n",
      "Epoch [193/200], Batch [0/107], D_loss: 0.0005, G_loss: 10.0449\n",
      "Epoch [193/200], Batch [10/107], D_loss: 0.0019, G_loss: 7.6740\n",
      "Epoch [193/200], Batch [20/107], D_loss: 0.0002, G_loss: 10.6178\n",
      "Epoch [193/200], Batch [30/107], D_loss: 0.0010, G_loss: 8.8009\n",
      "Epoch [193/200], Batch [40/107], D_loss: 0.0003, G_loss: 9.9797\n",
      "Epoch [193/200], Batch [50/107], D_loss: 0.0028, G_loss: 7.9195\n",
      "Epoch [193/200], Batch [60/107], D_loss: 0.0010, G_loss: 9.8376\n",
      "Epoch [193/200], Batch [70/107], D_loss: 0.0009, G_loss: 8.2350\n",
      "Epoch [193/200], Batch [80/107], D_loss: 0.0002, G_loss: 11.5279\n",
      "Epoch [193/200], Batch [90/107], D_loss: 0.0003, G_loss: 10.5889\n",
      "Epoch [193/200], Batch [100/107], D_loss: 0.0004, G_loss: 10.5301\n",
      "Epoch [193/200], D_loss: 0.0010, G_GAN_loss: 8.8112, G_L1_loss: 0.9614, G_perc_loss: 0.0310, G_total_loss: 9.8036, \n",
      "Epoch [194/200], Batch [0/107], D_loss: 0.0008, G_loss: 10.0830\n",
      "Epoch [194/200], Batch [10/107], D_loss: 0.0003, G_loss: 10.6403\n",
      "Epoch [194/200], Batch [20/107], D_loss: 0.0002, G_loss: 9.5418\n",
      "Epoch [194/200], Batch [30/107], D_loss: 0.0004, G_loss: 10.4920\n",
      "Epoch [194/200], Batch [40/107], D_loss: 0.0007, G_loss: 8.7412\n",
      "Epoch [194/200], Batch [50/107], D_loss: 0.0003, G_loss: 9.7561\n",
      "Epoch [194/200], Batch [60/107], D_loss: 0.0002, G_loss: 9.4423\n",
      "Epoch [194/200], Batch [70/107], D_loss: 0.0003, G_loss: 9.1239\n",
      "Epoch [194/200], Batch [80/107], D_loss: 0.0007, G_loss: 11.9002\n",
      "Epoch [194/200], Batch [90/107], D_loss: 0.0004, G_loss: 9.8544\n",
      "Epoch [194/200], Batch [100/107], D_loss: 0.0002, G_loss: 9.7719\n",
      "Epoch [194/200], D_loss: 0.0004, G_GAN_loss: 9.1828, G_L1_loss: 0.9613, G_perc_loss: 0.0310, G_total_loss: 10.1751, \n",
      "Epoch [195/200], Batch [0/107], D_loss: 0.0008, G_loss: 8.3391\n",
      "Epoch [195/200], Batch [10/107], D_loss: 0.0002, G_loss: 11.4431\n",
      "Epoch [195/200], Batch [20/107], D_loss: 0.0020, G_loss: 11.6715\n",
      "Epoch [195/200], Batch [30/107], D_loss: 0.0003, G_loss: 10.8281\n",
      "Epoch [195/200], Batch [40/107], D_loss: 0.0004, G_loss: 11.1827\n",
      "Epoch [195/200], Batch [50/107], D_loss: 0.0002, G_loss: 9.6792\n",
      "Epoch [195/200], Batch [60/107], D_loss: 0.0001, G_loss: 11.4847\n",
      "Epoch [195/200], Batch [70/107], D_loss: 0.0001, G_loss: 10.7043\n",
      "Epoch [195/200], Batch [80/107], D_loss: 0.0004, G_loss: 8.7682\n",
      "Epoch [195/200], Batch [90/107], D_loss: 0.0034, G_loss: 12.5210\n",
      "Epoch [195/200], Batch [100/107], D_loss: 0.0004, G_loss: 8.7187\n",
      "Epoch [195/200], D_loss: 0.0005, G_GAN_loss: 9.4043, G_L1_loss: 0.9586, G_perc_loss: 0.0310, G_total_loss: 10.3939, \n",
      "Epoch [196/200], Batch [0/107], D_loss: 0.0001, G_loss: 10.4277\n",
      "Epoch [196/200], Batch [10/107], D_loss: 0.0001, G_loss: 10.2704\n",
      "Epoch [196/200], Batch [20/107], D_loss: 0.0007, G_loss: 8.9518\n",
      "Epoch [196/200], Batch [30/107], D_loss: 0.0006, G_loss: 11.2383\n",
      "Epoch [196/200], Batch [40/107], D_loss: 0.0004, G_loss: 8.9881\n",
      "Epoch [196/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.0662\n",
      "Epoch [196/200], Batch [60/107], D_loss: 0.0002, G_loss: 10.2917\n",
      "Epoch [196/200], Batch [70/107], D_loss: 0.0007, G_loss: 9.7385\n",
      "Epoch [196/200], Batch [80/107], D_loss: 0.0001, G_loss: 10.8270\n",
      "Epoch [196/200], Batch [90/107], D_loss: 0.0002, G_loss: 9.2292\n",
      "Epoch [196/200], Batch [100/107], D_loss: 0.0003, G_loss: 10.7783\n",
      "Epoch [196/200], D_loss: 0.0005, G_GAN_loss: 9.5372, G_L1_loss: 0.9601, G_perc_loss: 0.0312, G_total_loss: 10.5285, \n",
      "Epoch [197/200], Batch [0/107], D_loss: 0.0003, G_loss: 11.3253\n",
      "Epoch [197/200], Batch [10/107], D_loss: 0.0003, G_loss: 9.0823\n",
      "Epoch [197/200], Batch [20/107], D_loss: 0.0004, G_loss: 8.7846\n",
      "Epoch [197/200], Batch [30/107], D_loss: 0.0002, G_loss: 11.5139\n",
      "Epoch [197/200], Batch [40/107], D_loss: 0.0001, G_loss: 11.9590\n",
      "Epoch [197/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.6598\n",
      "Epoch [197/200], Batch [60/107], D_loss: 3.1152, G_loss: 13.1037\n",
      "Epoch [197/200], Batch [70/107], D_loss: 0.0023, G_loss: 8.4202\n",
      "Epoch [197/200], Batch [80/107], D_loss: 0.0038, G_loss: 8.6824\n",
      "Epoch [197/200], Batch [90/107], D_loss: 0.1835, G_loss: 6.4240\n",
      "Epoch [197/200], Batch [100/107], D_loss: 0.0068, G_loss: 8.4343\n",
      "Epoch [197/200], D_loss: 0.0922, G_GAN_loss: 8.6858, G_L1_loss: 1.0758, G_perc_loss: 0.0350, G_total_loss: 9.7966, \n",
      "Epoch [198/200], Batch [0/107], D_loss: 0.0200, G_loss: 7.8818\n",
      "Epoch [198/200], Batch [10/107], D_loss: 0.0016, G_loss: 8.6436\n",
      "Epoch [198/200], Batch [20/107], D_loss: 0.0027, G_loss: 9.6360\n",
      "Epoch [198/200], Batch [30/107], D_loss: 0.0361, G_loss: 11.0284\n",
      "Epoch [198/200], Batch [40/107], D_loss: 0.0021, G_loss: 6.7515\n",
      "Epoch [198/200], Batch [50/107], D_loss: 0.0008, G_loss: 9.3548\n",
      "Epoch [198/200], Batch [60/107], D_loss: 0.0021, G_loss: 10.1714\n",
      "Epoch [198/200], Batch [70/107], D_loss: 0.0005, G_loss: 9.8255\n",
      "Epoch [198/200], Batch [80/107], D_loss: 0.0019, G_loss: 10.7930\n",
      "Epoch [198/200], Batch [90/107], D_loss: 0.0017, G_loss: 8.6639\n",
      "Epoch [198/200], Batch [100/107], D_loss: 0.0009, G_loss: 11.2166\n",
      "Epoch [198/200], D_loss: 0.0057, G_GAN_loss: 8.3966, G_L1_loss: 0.9997, G_perc_loss: 0.0330, G_total_loss: 9.4292, \n",
      "Epoch [199/200], Batch [0/107], D_loss: 0.0011, G_loss: 9.3997\n",
      "Epoch [199/200], Batch [10/107], D_loss: 0.0017, G_loss: 11.6606\n",
      "Epoch [199/200], Batch [20/107], D_loss: 0.0004, G_loss: 9.6455\n",
      "Epoch [199/200], Batch [30/107], D_loss: 0.0007, G_loss: 11.1799\n",
      "Epoch [199/200], Batch [40/107], D_loss: 0.0003, G_loss: 9.7218\n",
      "Epoch [199/200], Batch [50/107], D_loss: 0.0005, G_loss: 9.0813\n",
      "Epoch [199/200], Batch [60/107], D_loss: 0.0007, G_loss: 11.1313\n",
      "Epoch [199/200], Batch [70/107], D_loss: 0.0014, G_loss: 9.2109\n",
      "Epoch [199/200], Batch [80/107], D_loss: 0.0396, G_loss: 6.7168\n",
      "Epoch [199/200], Batch [90/107], D_loss: 0.0004, G_loss: 10.3352\n",
      "Epoch [199/200], Batch [100/107], D_loss: 0.0003, G_loss: 9.2598\n",
      "Epoch [199/200], D_loss: 0.0023, G_GAN_loss: 8.4948, G_L1_loss: 0.9754, G_perc_loss: 0.0315, G_total_loss: 9.5018, \n",
      "Epoch [200/200], Batch [0/107], D_loss: 0.0003, G_loss: 8.5368\n",
      "Epoch [200/200], Batch [10/107], D_loss: 0.0010, G_loss: 11.3438\n",
      "Epoch [200/200], Batch [20/107], D_loss: 0.0019, G_loss: 7.8759\n",
      "Epoch [200/200], Batch [30/107], D_loss: 0.0006, G_loss: 11.3910\n",
      "Epoch [200/200], Batch [40/107], D_loss: 0.0002, G_loss: 10.6027\n",
      "Epoch [200/200], Batch [50/107], D_loss: 0.0001, G_loss: 10.0277\n",
      "Epoch [200/200], Batch [60/107], D_loss: 0.0008, G_loss: 9.5731\n",
      "Epoch [200/200], Batch [70/107], D_loss: 0.0011, G_loss: 8.0383\n",
      "Epoch [200/200], Batch [80/107], D_loss: 0.0005, G_loss: 9.3957\n",
      "Epoch [200/200], Batch [90/107], D_loss: 0.0023, G_loss: 9.8449\n",
      "Epoch [200/200], Batch [100/107], D_loss: 0.0026, G_loss: 8.1598\n",
      "Epoch [200/200], D_loss: 0.0015, G_GAN_loss: 8.6818, G_L1_loss: 0.9655, G_perc_loss: 0.0312, G_total_loss: 9.6786, \n"
     ]
    }
   ],
   "source": [
    "seed_eveything()\n",
    "\n",
    "# read dataset\n",
    "train_dataset = PairedImageDataset(root_dir='Input DataSet path', size=512, mode='train')\n",
    "\n",
    "# load dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# Create and train model\n",
    "model = Triple_U_Net(m_in_channels=1, p_in_channels=3, bg_in_channels=3, gen_features=64, disc_features=64)\n",
    "\n",
    "# train\n",
    "train_triple_u_net(model, train_loader, num_epochs=200, save_path='Input checkpoint save path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621ccd2-f36e-488b-821e-73f2d65d7c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
